{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is its performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.717692Z",
     "start_time": "2019-08-05T20:40:53.967273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up data science environment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation, MeanShift, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from typing import Dict\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "sns.set_style('white')\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    module='scipy',\n",
    "    message='internal gelsd'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Processing, and Language Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.733250Z",
     "start_time": "2019-08-05T20:40:58.720561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lists for files and presidents.\n",
    "files = [\"1789-Washington.txt\",\n",
    "         \"1801-Jefferson.txt\",\n",
    "         \"1861-Lincoln.txt\",\n",
    "         \"1933-Roosevelt.txt\",\n",
    "         \"1953-Eisenhower.txt\",\n",
    "         \"1961-Kennedy.txt\",\n",
    "         \"1981-Reagan.txt\",\n",
    "         \"1989-Bush.txt\",\n",
    "         \"1993-Clinton.txt\",\n",
    "         \"2009-Obama.txt\"]\n",
    "\n",
    "presidents = [\"washington\",\n",
    "              \"jefferson\",\n",
    "              \"lincoln\",\n",
    "              \"fdr\",\n",
    "              \"eisenhower\",\n",
    "              \"kennedy\",\n",
    "              \"reagan\",\n",
    "              \"bush41\",\n",
    "              \"clinton\",\n",
    "              \"obama\"]\n",
    "\n",
    "# Control to make sure both lists are the same length.\n",
    "assert len(files) == len(presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.757120Z",
     "start_time": "2019-08-05T20:40:58.735774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick function to open all files needed (and close them again).\n",
    "docs = []\n",
    "for file_name, president in zip(files, presidents):\n",
    "    with open(f'./inaugural/{file_name}') as f:\n",
    "        doc = f.read()\n",
    "        docs.append((doc, president))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.771118Z",
     "start_time": "2019-08-05T20:40:58.759077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text: str) -> str:\n",
    "    \"\"\"Function to strip all characters except letters in words.\"\"\"\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.790267Z",
     "start_time": "2019-08-05T20:40:58.774257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use text_cleaner on the docs, combine them into data frame (clean_docs).\n",
    "clean_docs = []\n",
    "for doc, pres in docs:\n",
    "    clean_doc = text_cleaner(doc)\n",
    "    clean_docs.append((clean_doc, pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:40:58.805628Z",
     "start_time": "2019-08-05T20:40:58.793944Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives: Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but ov WASHINGTON\n",
      "\n",
      "Friends and Fellow Citizens: Called upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that portion of my fellow citizens which is here assembled to express my grateful thanks for the favor with which they have been pleased to look toward me, to declare a sincere consciousness that the task is above my talents, and that I approach it with those anxious and awful presentiments which the greatness of the charge and the weakness of my powers so justly inspire. A rising nation, spread over a wide and fruitful land, traversing all the seas with the rich productions of their industry, engaged in commerce with nations who feel power and forget right, advancing rapidly to destinies beyond the reach of mortal eye when I contemplate these transcendent objects, and see the honor, the happiness, and the hopes of this beloved country committed to the issue, and the auspices of this day, I shrink from the contemplation, and humble myself befo JEFFERSON\n",
      "\n",
      "Fellow-Citizens of the United States: In compliance with a custom as old as the Government itself, I appear before you to address you briefly and to take in your presence the oath prescribed by the Constitution of the United States to be taken by the President \"before he enters on the execution of this office.\" I do not consider it necessary at present for me to discuss those matters of administration about which there is no special anxiety or excitement. Apprehension seems to exist among the people of the Southern States that by the accession of a Republican Administration their property and their peace and personal security are to be endangered. There has never been any reasonable cause for such apprehension. Indeed, the most ample evidence to the contrary has all the while existed and been open to their inspection. It is found in nearly all the published speeches of him who now addresses you. I do but quote from one of those speeches when I declare that I have no purpose, directly o LINCOLN\n",
      "\n",
      "I am certain that my fellow Americans expect that on my induction into the Presidency I will address them with a candor and a decision which the present situation of our Nation impels. This is preeminently the time to speak the truth, the whole truth, frankly and boldly. Nor need we shrink from honestly facing conditions in our country today. This great Nation will endure as it has endured, will revive and will prosper. So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself nameless, unreasoning, unjustified terror which paralyzes needed efforts to convert retreat into advance. In every dark hour of our national life a leadership of frankness and vigor has met with that understanding and support of the people themselves which is essential to victory. I am convinced that you will again give that support to leadership in these critical days. In such a spirit on my part and on yours we face our common difficulties. They concern, thank God, only m FDR\n",
      "\n",
      "My friends, before I begin the expression of those thoughts that I deem appropriate to this moment, would you permit me the privilege of uttering a little private prayer of my own. And I ask that you bow your heads: Almighty God, as we stand here at this moment my future associates in the executive branch of government join me in beseeching that Thou will make full and complete our dedication to the service of the people in this throng, and their fellow citizens everywhere. Give us, we pray, the power to discern clearly right from wrong, and allow all our words and actions to be governed thereby, and by the laws of this land. Especially we pray that our concern shall be for all the people regardless of station, race, or calling. May cooperation be permitted and be the mutual aim of those who, under the concepts of our Constitution, hold to differing political faiths; so that all may work for the good of our beloved country and Thy glory. Amen. My fellow citizens: The world and we have  EISENHOWER\n",
      "\n",
      "Vice President Johnson, Mr. Speaker, Mr. Chief Justice, President Eisenhower, Vice President Nixon, President Truman, reverend clergy, fellow citizens, we observe today not a victory of party, but a celebration of freedom symbolizing an end, as well as a beginning signifying renewal, as well as change. For I have sworn I before you and Almighty God the same solemn oath our forebears l prescribed nearly a century and three quarters ago. The world is very different now. For man holds in his mortal hands the power to abolish all forms of human poverty and all forms of human life. And yet the same revolutionary beliefs for which our forebears fought are still at issue around the globe the belief that the rights of man come not from the generosity of the state, but from the hand of God. We dare not forget today that we are the heirs of that first revolution. Let the word go forth from this time and place, to friend and foe alike, that the torch has been passed to a new generation of America KENNEDY\n",
      "\n",
      "Senator Hatfield, Mr. Chief Justice, Mr. President, Vice President Bush, Vice President Mondale, Senator Baker, Speaker O'Neill, Reverend Moomaw, and my fellow citizens: To a few of us here today, this is a solemn and most momentous occasion; and yet, in the history of our Nation, it is a commonplace occurrence. The orderly transfer of authority as called for in the Constitution routinely takes place as it has for almost two centuries and few of us stop to think how unique we really are. In the eyes of many in the world, this every-4-year ceremony we accept as normal is nothing less than a miracle. Mr. President, I want our fellow citizens to know how much you did to carry on this tradition. By your gracious cooperation in the transition process, you have shown a watching world that we are a united people pledged to maintaining a political system which guarantees individual liberty to a greater degree than any other, and I thank you and your people for all your help in maintaining the  REAGAN\n",
      "\n",
      "Mr. Chief Justice, Mr. President, Vice President Quayle, Senator Mitchell, Speaker Wright, Senator Dole, Congressman Michael, and fellow citizens, neighbors, and friends: There is a man here who has earned a lasting place in our hearts and in our history. President Reagan, on behalf of our Nation, I thank you for the wonderful things that you have done for America. I have just repeated word for word the oath taken by George Washington 200 years ago, and the Bible on which I placed my hand is the Bible on which he placed his. It is right that the memory of Washington be with us today, not only because this is our Bicentennial Inauguration, but because Washington remains the Father of our Country. And he would, I think, be gladdened by this day; for today is the concrete expression of a stunning fact: our continuity these 200 years since our government began. We meet on democracy's front porch, a good place to talk as neighbors and as friends. For this is a day when our nation is made wh BUSH41\n",
      "\n",
      "My fellow citizens, today we celebrate the mystery of American renewal. This ceremony is held in the depth of winter, but by the words we speak and the faces we show the world, we force the spring. A spring reborn in the world's oldest democracy, that brings forth the vision and courage to reinvent America. When our founders boldly declared America's independence to the world, and our purposes to the Almighty, they knew that America, to endure, would have to change. Not change for change sake, but change to preserve America's ideals: life, liberty, the pursuit of happiness. Though we march to the music of our time, our mission is timeless. Each generation of American's must define what it means to be an American. On behalf of our nation, I salute my predecessor, President Bush, for his half-century of service to America and I thank the millions of men and women whose steadfastness and sacrifice triumphed over depression, fascism and communism. Today, a generation raised in the shadows  CLINTON\n",
      "\n",
      "My fellow citizens: I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents. So it has been. So it must be with this generation of Americans. That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequen OBAMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each doc and print the first 1000 characters for inspection.\n",
    "for doc, pres in clean_docs:\n",
    "    print(doc[:1000], pres.upper()) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.177374Z",
     "start_time": "2019-08-05T20:40:58.814034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to parse data.\n",
    "nlp = spacy.load('en')\n",
    "df_list = []\n",
    "\n",
    "\n",
    "def nlp_text(text_file):\n",
    "    doc = nlp(text_file)\n",
    "    return doc\n",
    "\n",
    "def sentences(doc_nlp, speaker):\n",
    "    return [[sent.text, speaker] for sent in doc_nlp.sents]\n",
    "\n",
    "  \n",
    "def sentences_to_df(sents):\n",
    "    return pd.DataFrame(sents)\n",
    "\n",
    "\n",
    "for doc, pres in clean_docs:\n",
    "    parsed = nlp_text(doc)\n",
    "    sents = sentences(parsed, pres)\n",
    "    df = sentences_to_df(sents)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.189023Z",
     "start_time": "2019-08-05T20:41:03.180078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine each sentence data frame into one master data frame.\n",
    "sent_df = pd.concat([*df_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.208923Z",
     "start_time": "2019-08-05T20:41:03.191458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bush41        145\n",
       "lincoln       139\n",
       "reagan        130\n",
       "eisenhower    121\n",
       "obama         113\n",
       "fdr            86\n",
       "clinton        82\n",
       "kennedy        53\n",
       "jefferson      42\n",
       "washington     25\n",
       "Name: President, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns.\n",
    "sent_df.columns = ['sentence', 'President']\n",
    "\n",
    "# Check the count of sents per President.\n",
    "sent_df.President.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features\n",
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.221888Z",
     "start_time": "2019-08-05T20:41:03.211251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data.\n",
    "X = sent_df.sentence\n",
    "y = sent_df.President\n",
    "X_train_eval, X_holdout, y_train_eval, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.232698Z",
     "start_time": "2019-08-05T20:41:03.225513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting into train/eval/holdout groups.\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train_eval, y_train_eval, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.246907Z",
     "start_time": "2019-08-05T20:41:03.235001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create base parameters dictionary.\n",
    "base_param_dict = {'strip_accents': 'unicode',\n",
    "                   'lowercase': True,\n",
    "                   'stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.253251Z",
     "start_time": "2019-08-05T20:41:03.249244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer.\n",
    "bow = CountVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.321706Z",
     "start_time": "2019-08-05T20:41:03.255680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into dfs of bags of words.\n",
    "_bow_train = bow.fit_transform(X_train)\n",
    "_bow_eval = bow.transform(X_eval)\n",
    "_bow_holdout = bow.transform(X_holdout)\n",
    "assert len(X_train) == _bow_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names = bow.get_feature_names()\n",
    "\n",
    "# Sparse matrix to data frame.\n",
    "X_train_bow = pd.DataFrame(_bow_train.toarray(), columns=feature_names)\n",
    "X_eval_bow = pd.DataFrame(_bow_eval.toarray(), columns=feature_names)\n",
    "X_holdout_bow = pd.DataFrame(_bow_holdout.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.330088Z",
     "start_time": "2019-08-05T20:41:03.325248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Tfidf.\n",
    "tfidf = TfidfVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.376129Z",
     "start_time": "2019-08-05T20:41:03.332731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into dfs of tfidf values.\n",
    "_tfidf_train = tfidf.fit_transform(X_train)\n",
    "_tfidf_eval = tfidf.transform(X_eval)\n",
    "_tfidf_holdout = tfidf.transform(X_holdout)\n",
    "assert len(X_train) == _tfidf_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names_tfidf = tfidf.get_feature_names()\n",
    "\n",
    "# Set up data frames.\n",
    "X_train_tfidf = pd.DataFrame(\n",
    "    _tfidf_train.toarray(), columns=feature_names_tfidf)\n",
    "X_eval_tfidf = pd.DataFrame(\n",
    "    _tfidf_eval.toarray(), columns=feature_names_tfidf)\n",
    "X_holdout_tfidf = pd.DataFrame(\n",
    "    _tfidf_holdout.toarray(), columns=feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:03.421876Z",
     "start_time": "2019-08-05T20:41:03.378139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MinMaxScaler. Create train/eval/holdout groups.\n",
    "scaler = MinMaxScaler()\n",
    "X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "print(len(X_train_tfidf_scaled))\n",
    "X_eval_tfidf_scaled = scaler.transform(X_eval_tfidf)\n",
    "X_holdout_tfidf_scaled = scaler.transform(X_holdout_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:28.513760Z",
     "start_time": "2019-08-05T20:41:03.426756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 2\t silhouette: 0.5439510271051321\n",
      "\n",
      "KMeans \n",
      " col_0  0    1\n",
      "row_0        \n",
      "0      1    0\n",
      "1      0  525 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=2,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 2\t silhouette: 0.09495640752724352\n",
      "\n",
      "MiniBatch \n",
      " col_0  0    1\n",
      "row_0        \n",
      "0      1  524\n",
      "1      0    1 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=2, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 2\t silhouette: -0.1594554749082074\n",
      "\n",
      "Spectral \n",
      " col_0    0  1\n",
      "row_0        \n",
      "0      524  0\n",
      "1        0  2 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 3\t silhouette: 0.22883520005593846\n",
      "\n",
      "KMeans \n",
      " col_0  0    1  2\n",
      "row_0           \n",
      "0      1    0  0\n",
      "1      0  524  0\n",
      "2      0    0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=3,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 3\t silhouette: 0.11890584033756584\n",
      "\n",
      "MiniBatch \n",
      " col_0    0  1  2\n",
      "row_0           \n",
      "0        1  0  0\n",
      "1      522  1  1\n",
      "2        1  0  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=3, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 3\t silhouette: -0.1575722542361606\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2\n",
      "row_0           \n",
      "0      521  0  0\n",
      "1        0  3  0\n",
      "2        0  0  2 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 4\t silhouette: 0.17334730186469036\n",
      "\n",
      "KMeans \n",
      " col_0    0  1  2  3\n",
      "row_0              \n",
      "0      523  0  0  0\n",
      "1        0  1  0  0\n",
      "2        0  0  1  0\n",
      "3        0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=4,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 4\t silhouette: -0.006711557570766297\n",
      "\n",
      "MiniBatch \n",
      " col_0  0  1    2  3\n",
      "row_0              \n",
      "0      0  0    1  0\n",
      "1      0  0   15  0\n",
      "2      1  1  506  1\n",
      "3      0  0    1  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=4, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 4\t silhouette: -0.18736452507948786\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2  3\n",
      "row_0              \n",
      "0      518  0  0  0\n",
      "1        0  3  0  0\n",
      "2        0  0  2  0\n",
      "3        0  0  0  3 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 5\t silhouette: 0.10492817014956171\n",
      "\n",
      "KMeans \n",
      " col_0  0    1  2  3  4\n",
      "row_0                 \n",
      "0      1    0  0  0  0\n",
      "1      0  522  0  0  0\n",
      "2      0    0  1  0  0\n",
      "3      0    0  0  1  0\n",
      "4      0    0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=5,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 5\t silhouette: -0.008692141226892937\n",
      "\n",
      "MiniBatch \n",
      " col_0  0  1    2  3   4\n",
      "row_0                  \n",
      "0      0  0    4  0   0\n",
      "1      1  1  497  1  18\n",
      "2      0  0    1  0   1\n",
      "3      0  0    1  0   0\n",
      "4      0  0    1  0   0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=5, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 5\t silhouette: -0.1858134988579024\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2  3  4\n",
      "row_0                 \n",
      "0        0  0  0  0  3\n",
      "1        0  2  0  0  0\n",
      "2        0  0  3  0  0\n",
      "3      516  0  0  0  0\n",
      "4        0  0  0  2  0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 6\t silhouette: 0.07646087892096094\n",
      "\n",
      "KMeans \n",
      " col_0    0  1  2  3  4  5\n",
      "row_0                    \n",
      "0      521  0  0  0  0  0\n",
      "1        0  1  0  0  0  0\n",
      "2        0  0  1  0  0  0\n",
      "3        0  0  0  1  0  0\n",
      "4        0  0  0  0  1  0\n",
      "5        0  0  0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=6,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 6\t silhouette: -0.025793745839419638\n",
      "\n",
      "MiniBatch \n",
      " col_0    0  1  2  3  4  5\n",
      "row_0                    \n",
      "0      514  2  2  1  1  1\n",
      "1        1  0  0  0  0  0\n",
      "2        1  0  0  0  0  0\n",
      "3        1  0  0  0  0  0\n",
      "4        1  0  0  0  0  0\n",
      "5        1  0  0  0  0  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=6, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 6\t silhouette: -0.18524309524721844\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2  3  4  5\n",
      "row_0                    \n",
      "0      514  0  0  0  0  3\n",
      "1        0  0  1  0  0  0\n",
      "2        0  2  0  0  0  0\n",
      "3        0  0  2  0  0  0\n",
      "4        0  0  0  2  0  0\n",
      "5        0  0  0  0  2  0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 7\t silhouette: 0.08759655635644135\n",
      "\n",
      "KMeans \n",
      " col_0  0   1  2    3  4  5  6\n",
      "row_0                        \n",
      "0      1   0  0    0  0  0  0\n",
      "1      0  49  0    0  0  0  0\n",
      "2      0   0  1    0  0  0  0\n",
      "3      0   0  0  472  0  0  0\n",
      "4      0   0  0    0  1  0  0\n",
      "5      0   0  0    0  0  1  0\n",
      "6      0   0  0    0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=7,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 7\t silhouette: -0.08453819435209144\n",
      "\n",
      "MiniBatch \n",
      " col_0  0  1  2  3  4   5    6\n",
      "row_0                        \n",
      "0      0  0  0  0  0   0    1\n",
      "1      0  0  0  0  0   0    1\n",
      "2      5  1  1  1  1  11  500\n",
      "3      0  0  0  0  0   0    1\n",
      "4      0  0  0  0  0   0    1\n",
      "5      0  0  0  0  0   0    1\n",
      "6      0  0  0  0  0   0    1 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=7, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 7\t silhouette: -0.16159504387671364\n",
      "\n",
      "Spectral \n",
      " col_0    0  1   2  3  4  5  6\n",
      "row_0                        \n",
      "0      473  0   0  0  0  0  0\n",
      "1        0  2   0  0  0  0  0\n",
      "2        0  0   0  3  0  0  0\n",
      "3        0  0  40  0  0  0  0\n",
      "4        0  0   0  0  4  0  0\n",
      "5        0  0   0  0  0  0  2\n",
      "6        0  0   0  0  0  2  0 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 8\t silhouette: -0.05594710095693154\n",
      "\n",
      "KMeans \n",
      " col_0  0  1  2  3  4    5  6  7\n",
      "row_0                          \n",
      "0      1  0  0  0  0    0  0  0\n",
      "1      0  5  0  0  0    0  0  0\n",
      "2      0  0  1  0  0    0  0  0\n",
      "3      0  0  0  3  0    0  0  0\n",
      "4      0  0  0  0  1    0  0  0\n",
      "5      0  0  0  0  0  513  0  0\n",
      "6      0  0  0  0  0    0  1  0\n",
      "7      0  0  0  0  0    0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=8,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 8\t silhouette: -0.03960830807412723\n",
      "\n",
      "MiniBatch \n",
      " col_0   0  1  2  3  4  5    6  7\n",
      "row_0                           \n",
      "0       0  0  0  0  0  0    1  0\n",
      "1       0  0  0  0  0  0    9  0\n",
      "2       1  0  0  0  0  0    2  0\n",
      "3       0  0  0  0  0  0    2  0\n",
      "4       0  0  0  0  0  0    1  0\n",
      "5       0  0  0  0  0  0    1  0\n",
      "6      14  1  1  1  1  1  488  1\n",
      "7       0  0  0  0  0  0    1  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=8, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 8\t silhouette: -0.16055093042616858\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2  3   4   5  6  7\n",
      "row_0                            \n",
      "0      442  0  0  0   0   0  0  0\n",
      "1        0  2  0  0   0   0  0  0\n",
      "2        0  0  3  0   0   0  0  0\n",
      "3        0  0  0  3   0   0  0  0\n",
      "4        2  0  0  0  30   0  0  0\n",
      "5        0  0  0  0   0   0  2  0\n",
      "6        0  0  0  0   0  40  0  0\n",
      "7        0  0  0  0   0   0  0  2 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=9, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 9\t silhouette: -0.1436679836118903\n",
      "\n",
      "KMeans \n",
      " col_0  0  1    2  3  4  5  6  7  8\n",
      "row_0                             \n",
      "0      1  0    0  0  0  0  0  0  0\n",
      "1      0  1    0  0  0  0  0  0  0\n",
      "2      0  0  518  0  0  0  0  0  0\n",
      "3      0  0    0  1  0  0  0  0  0\n",
      "4      0  0    0  0  1  0  0  0  0\n",
      "5      0  0    0  0  0  1  0  0  0\n",
      "6      0  0    0  0  0  0  1  0  0\n",
      "7      0  0    0  0  0  0  0  1  0\n",
      "8      0  0    0  0  0  0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=9,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 9\t silhouette: -0.013503221819838603\n",
      "\n",
      "MiniBatch \n",
      " col_0  0  1  2  3  4  5  6    7  8\n",
      "row_0                             \n",
      "0      0  0  0  0  0  0  0    1  0\n",
      "1      0  0  0  0  0  0  0    2  0\n",
      "2      0  0  0  0  0  0  0    1  0\n",
      "3      0  0  0  0  0  0  0    6  0\n",
      "4      0  0  0  0  0  0  0    1  0\n",
      "5      0  0  0  0  0  0  0    1  0\n",
      "6      5  1  1  1  3  1  1  497  1\n",
      "7      0  0  0  0  0  0  0    1  0\n",
      "8      0  0  0  0  0  0  0    2  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=9, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 9\t silhouette: -0.1600004693400773\n",
      "\n",
      "Spectral \n",
      " col_0   0  1  2    3   4   5  6  7  8\n",
      "row_0                                \n",
      "0       2  0  0  398   0   0  0  0  2\n",
      "1       0  0  3    0   0   0  0  0  0\n",
      "2       0  2  0    0   0   0  0  0  0\n",
      "3       0  0  0    0  22   0  0  0  0\n",
      "4       0  0  0    0   0  17  0  0  0\n",
      "5      15  0  0    0   0   0  0  0  0\n",
      "6       0  0  0    0   0   0  0  2  0\n",
      "7       0  0  0   46   0  15  0  0  0\n",
      "8       0  0  0    0   0   0  2  0  0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=42, tol=0.0001, verbose=0)\n",
      "clusters: 10\t silhouette: 0.05215944835818892\n",
      "\n",
      "KMeans \n",
      " col_0  0   1    2  3  4  5  6  7  8  9\n",
      "row_0                                 \n",
      "0      1   0    0  0  0  0  0  0  0  0\n",
      "1      0  43    0  0  0  0  0  0  0  0\n",
      "2      0   0  475  0  0  0  0  0  0  0\n",
      "3      0   0    0  1  0  0  0  0  0  0\n",
      "4      0   0    0  0  1  0  0  0  0  0\n",
      "5      0   0    0  0  0  1  0  0  0  0\n",
      "6      0   0    0  0  0  0  1  0  0  0\n",
      "7      0   0    0  0  0  0  0  1  0  0\n",
      "8      0   0    0  0  0  0  0  0  1  0\n",
      "9      0   0    0  0  0  0  0  0  0  1 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=10,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 10\t silhouette: -0.03158968614079272\n",
      "\n",
      "MiniBatch \n",
      " col_0  0  1  2  3  4  5    6   7   8  9\n",
      "row_0                                  \n",
      "0      0  0  0  0  0  0    1   0   0  0\n",
      "1      0  0  0  0  0  0    1   0   0  0\n",
      "2      0  0  0  0  0  0    1   0   0  0\n",
      "3      1  1  1  1  1  1  433  16  43  2\n",
      "4      0  0  0  0  0  0    1   0   0  0\n",
      "5      0  0  0  0  0  0   14   0   4  0\n",
      "6      0  0  0  0  0  0    1   0   0  0\n",
      "7      0  0  0  0  0  0    1   0   0  0\n",
      "8      0  0  0  0  0  0    1   0   0  0\n",
      "9      0  0  0  0  0  0    1   0   0  0 \n",
      "\n",
      "MeanShift(bandwidth=3.071450947055253, bin_seeding=True, cluster_all=True,\n",
      "     min_bin_freq=1, n_jobs=None, seeds=None)\n",
      "SpectralClustering(affinity='rbf', assign_labels='kmeans', coef0=1, degree=3,\n",
      "          eigen_solver=None, eigen_tol=0.0, gamma=1.0, kernel_params=None,\n",
      "          n_clusters=10, n_init=10, n_jobs=None, n_neighbors=10,\n",
      "          random_state=None)\n",
      "clusters: 10\t silhouette: -0.15897513303071156\n",
      "\n",
      "Spectral \n",
      " col_0    0  1  2   3  4   5   6  7  8  9\n",
      "row_0                                   \n",
      "0        2  0  0  28  0   0   0  0  0  2\n",
      "1        0  2  0   0  0   0   0  0  0  0\n",
      "2      438  0  0   0  0   0   0  0  0  2\n",
      "3        0  0  3   0  0   0   0  0  0  0\n",
      "4        0  0  0   0  0   0   0  0  2  0\n",
      "5        0  0  0   0  0   0  17  0  0  0\n",
      "6        1  0  0   0  0  19   0  0  0  0\n",
      "7        0  0  0   0  2   0   0  0  0  0\n",
      "8        0  0  0   0  0   0   0  2  0  0\n",
      "9        6  0  0   0  0   0   0  0  0  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clustering models\n",
    "models = []\n",
    "names = []\n",
    "plot_nums = []\n",
    "silhouettes = []\n",
    "clust = []\n",
    "\n",
    "for clusters in range(2,11):\n",
    "    bandwidth = estimate_bandwidth(\n",
    "        X_train_tfidf_scaled, quantile=0.2, n_samples=500)\n",
    "    models.append((0, 'KMeans', KMeans(n_clusters=clusters,\n",
    "                                   init='k-means++', random_state=42)))\n",
    "    models.append((1, 'MiniBatch', MiniBatchKMeans(\n",
    "        init='random', n_clusters=clusters, batch_size=500)))\n",
    "    models.append((2, 'MeanShift', MeanShift(\n",
    "        bandwidth=bandwidth, bin_seeding=True)))\n",
    "    models.append((3, 'Spectral', SpectralClustering(n_clusters=clusters)))\n",
    "    #models.append((4, 'Affinity', AffinityPropagation()))\n",
    "    \n",
    "for _, name, model in models:\n",
    "    names.append(name)\n",
    "    model.fit(X_train_tfidf_scaled)\n",
    "    labels = model.labels_\n",
    "    print(model)\n",
    "    if len(set(labels)) > 1:\n",
    "        ypred = model.fit_predict(X_train_tfidf_scaled)\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X_train_tfidf_scaled, labels, metric='euclidean')\n",
    "        silhouettes.append(silhouette)\n",
    "        #ax[plot_num].set_title(name)\n",
    "        #plotting(plot_num, labels, ypred)\n",
    "        print('clusters: {}\\t silhouette: {}\\n'.format(\n",
    "            model.n_clusters, silhouette))\n",
    "        print(name, '\\n', pd.crosstab(ypred, labels), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:28.788330Z",
     "start_time": "2019-08-05T20:41:28.517410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-run KMeans and extract cluster information.\n",
    "model = KMeans(n_clusters=2, random_state=15).fit(X_train_tfidf_scaled)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = model.labels_\n",
    "\n",
    "X_train_tfidf['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:28.828161Z",
     "start_time": "2019-08-05T20:41:28.792544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>100</th>\n",
       "      <th>14th</th>\n",
       "      <th>1776</th>\n",
       "      <th>1787</th>\n",
       "      <th>1917</th>\n",
       "      <th>200</th>\n",
       "      <th>21st</th>\n",
       "      <th>abdicated</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yearn</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yields</th>\n",
       "      <th>young</th>\n",
       "      <th>zeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 2408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters   100  14th  1776  1787  1917   200  21st  abdicated  abhorring  \\\n",
       "0         0 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000      0.000   \n",
       "1         1 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001      0.001   \n",
       "\n",
       "   ...  write  written  wrong  year  yearn  years   yes  yields  young  zeal  \n",
       "0  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   0.000  0.000 0.000  \n",
       "1  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   0.001  0.004 0.001  \n",
       "\n",
       "[2 rows x 2408 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_clusters = X_train_tfidf.groupby(\n",
    "    ['clusters'], as_index=False).mean()\n",
    "X_train_tfidf_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:33.845005Z",
     "start_time": "2019-08-05T20:41:28.833294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>100</th>\n",
       "      <th>14th</th>\n",
       "      <th>1776</th>\n",
       "      <th>1787</th>\n",
       "      <th>1917</th>\n",
       "      <th>200</th>\n",
       "      <th>21st</th>\n",
       "      <th>abdicated</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yearn</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yields</th>\n",
       "      <th>young</th>\n",
       "      <th>zeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clusters   100  14th  1776  1787  1917   200  21st  abdicated  \\\n",
       "count     1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000      1.000   \n",
       "mean      0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "std         nan   nan   nan   nan   nan   nan   nan   nan        nan   \n",
       "min       0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "25%       0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "50%       0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "75%       0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "max       0.000 0.000 0.000 0.000 0.000 0.000 0.008 0.000      0.000   \n",
       "\n",
       "       abhorring  ...  write  written  wrong  year  yearn  years   yes  \\\n",
       "count      1.000  ...  1.000    1.000  1.000 1.000  1.000  1.000 1.000   \n",
       "mean       0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "std          nan  ...    nan      nan    nan   nan    nan    nan   nan   \n",
       "min        0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "25%        0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "50%        0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "75%        0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "max        0.000  ...  0.000    0.032  0.000 0.000  0.010  0.018 0.000   \n",
       "\n",
       "       yields  young  zeal  \n",
       "count   1.000  1.000 1.000  \n",
       "mean    0.000  0.000 0.000  \n",
       "std       nan    nan   nan  \n",
       "min     0.000  0.000 0.000  \n",
       "25%     0.000  0.000 0.000  \n",
       "50%     0.000  0.000 0.000  \n",
       "75%     0.000  0.000 0.000  \n",
       "max     0.000  0.000 0.000  \n",
       "\n",
       "[8 rows x 2408 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster0 = X_train_tfidf_clusters[X_train_tfidf_clusters['clusters'] == 0]\n",
    "cluster0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:41.620737Z",
     "start_time": "2019-08-05T20:41:33.855359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>100</th>\n",
       "      <th>14th</th>\n",
       "      <th>1776</th>\n",
       "      <th>1787</th>\n",
       "      <th>1917</th>\n",
       "      <th>200</th>\n",
       "      <th>21st</th>\n",
       "      <th>abdicated</th>\n",
       "      <th>abhorring</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yearn</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yields</th>\n",
       "      <th>young</th>\n",
       "      <th>zeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clusters   100  14th  1776  1787  1917   200  21st  abdicated  \\\n",
       "count     1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000      1.000   \n",
       "mean      1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "std         nan   nan   nan   nan   nan   nan   nan   nan        nan   \n",
       "min       1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "25%       1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "50%       1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "75%       1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "max       1.000 0.001 0.001 0.001 0.001 0.001 0.000 0.001      0.001   \n",
       "\n",
       "       abhorring  ...  write  written  wrong  year  yearn  years   yes  \\\n",
       "count      1.000  ...  1.000    1.000  1.000 1.000  1.000  1.000 1.000   \n",
       "mean       0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "std          nan  ...    nan      nan    nan   nan    nan    nan   nan   \n",
       "min        0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "25%        0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "50%        0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "75%        0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "max        0.001  ...  0.001    0.002  0.002 0.001  0.000  0.002 0.003   \n",
       "\n",
       "       yields  young  zeal  \n",
       "count   1.000  1.000 1.000  \n",
       "mean    0.001  0.004 0.001  \n",
       "std       nan    nan   nan  \n",
       "min     0.001  0.004 0.001  \n",
       "25%     0.001  0.004 0.001  \n",
       "50%     0.001  0.004 0.001  \n",
       "75%     0.001  0.004 0.001  \n",
       "max     0.001  0.004 0.001  \n",
       "\n",
       "[8 rows x 2408 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1 = X_train_tfidf_clusters[X_train_tfidf_clusters['clusters'] == 1]\n",
    "cluster1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:41.638679Z",
     "start_time": "2019-08-05T20:41:41.625175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100              2\n",
       "14th             2\n",
       "1776             2\n",
       "1787             2\n",
       "1917             2\n",
       "200              2\n",
       "21st             2\n",
       "abdicated        2\n",
       "abhorring        2\n",
       "abide            2\n",
       "abiding          2\n",
       "ability          2\n",
       "able             2\n",
       "abraham          2\n",
       "abroad           2\n",
       "absolute         2\n",
       "abuses           2\n",
       "accept           2\n",
       "accession        2\n",
       "accidental       2\n",
       "accomplished     2\n",
       "according        2\n",
       "accordingly      2\n",
       "account          2\n",
       "achieve          2\n",
       "achieved         2\n",
       "achievement      2\n",
       "acknowledge      2\n",
       "acknowledging    2\n",
       "acquiescence     2\n",
       "                ..\n",
       "wish             2\n",
       "wishes           2\n",
       "withal           2\n",
       "withered         2\n",
       "withhold         2\n",
       "women            2\n",
       "wonder           2\n",
       "wonderful        2\n",
       "wonders          2\n",
       "word             2\n",
       "words            2\n",
       "work             2\n",
       "workers          2\n",
       "working          2\n",
       "world            2\n",
       "worldly          2\n",
       "worse            2\n",
       "worst            2\n",
       "worth            2\n",
       "worthy           2\n",
       "write            2\n",
       "written          2\n",
       "wrong            2\n",
       "year             2\n",
       "yearn            2\n",
       "years            2\n",
       "yes              2\n",
       "yields           2\n",
       "young            2\n",
       "zeal             2\n",
       "Length: 2407, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many words are in each cluster.\n",
    "X_train_tfidf_clusters.groupby('clusters').count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:20:36.652787Z",
     "start_time": "2019-08-03T20:20:36.547143Z"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:44.419567Z",
     "start_time": "2019-08-05T20:41:41.642660Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish and fit the multi-level perceptron model.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,), random_state=15)\n",
    "mlp.fit(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:44.447892Z",
     "start_time": "2019-08-05T20:41:44.422074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885931558935361"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find MLP score.\n",
    "mlp.score(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:55.467905Z",
     "start_time": "2019-08-05T20:41:44.451852Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80909091, 0.82568807, 0.88349515, 0.87254902, 0.88235294])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find cross-validation score.\n",
    "cross_val_score(mlp, X_train_tfidf, ypred, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:57.742867Z",
     "start_time": "2019-08-05T20:41:55.472193Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(10,10,), random_state=15)\n",
    "mlp1.fit(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:41:57.773276Z",
     "start_time": "2019-08-05T20:41:57.746024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752851711026616"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp1.score(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:06.597986Z",
     "start_time": "2019-08-05T20:41:57.775834Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8       , 0.80733945, 0.86407767, 0.8627451 , 0.88235294])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp1, X_train_tfidf, ypred, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:09.480601Z",
     "start_time": "2019-08-05T20:42:06.602936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(25, 15), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(25,15,), random_state=15)\n",
    "mlp2.fit(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:09.535255Z",
     "start_time": "2019-08-05T20:42:09.492650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923954372623575"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp2.score(X_train_tfidf, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.794800Z",
     "start_time": "2019-08-05T20:42:09.544347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80909091, 0.81651376, 0.88349515, 0.87254902, 0.88235294])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp2, X_train_tfidf, ypred, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.807385Z",
     "start_time": "2019-08-05T20:42:19.796943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score to beat: 0.15491452991452967\n"
     ]
    }
   ],
   "source": [
    "# Create baseline score to beat.\n",
    "# Bush41 had the most sentences, so guessing him\n",
    "# for all sentences would give this percentage.\n",
    "print('Baseline score to beat:', sum(\n",
    "    (sent_df.President == 'bush41') / len(sent_df.President)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.815557Z",
     "start_time": "2019-08-05T20:42:19.810250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline helpers.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.826315Z",
     "start_time": "2019-08-05T20:42:19.819366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the models.\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "boost = GradientBoostingClassifier()\n",
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.837159Z",
     "start_time": "2019-08-05T20:42:19.830996Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up _kwargs files for convenience.\n",
    "bow_kwargs = {'X_train': X_train_bow, 'y_train': y_train,\n",
    "              'X_eval': X_eval_bow, 'y_eval': y_eval,\n",
    "              'X_holdout': X_holdout_bow, 'y_holdout': y_holdout}\n",
    "\n",
    "tfidf_kwargs = {'X_train': X_train_tfidf_scaled,'y_train': y_train,\n",
    "                'X_eval': X_eval_tfidf_scaled,'y_eval': y_eval,\n",
    "                'X_holdout': X_eval_tfidf_scaled,'y_holdout': y_holdout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.848439Z",
     "start_time": "2019-08-05T20:42:19.840212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tune parameter grids.\n",
    "log_reg_params = {'model__C': [1]}\n",
    "tree_params = {'model__criterion': ['gini']}\n",
    "forest_params = {'model__n_estimators': [100]}\n",
    "boost_params = {'model__n_estimators': [100]}\n",
    "nb_params = {'model__alpha': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:19.874635Z",
     "start_time": "2019-08-05T20:42:19.851818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to fit and predict all working kernals.\n",
    "\n",
    "\n",
    "def fit_and_predict(model, params: Dict,\n",
    "                    X_train: pd.DataFrame,\n",
    "                    y_train: pd.DataFrame,\n",
    "                    X_eval: pd.DataFrame,\n",
    "                    y_eval: pd.DataFrame,\n",
    "                    X_holdout: pd.DataFrame,\n",
    "                    y_holdout: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Takes an instantiated sklearn model, training data (X_train, y_train), \n",
    "    and performs cross-validation and then prints the mean of the cross-\n",
    "    validation accuracies.\n",
    "    \"\"\"\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_eval) == len(y_eval)\n",
    "    assert len(X_holdout) == len(y_holdout)\n",
    "    pipe = Pipeline(steps=[('model', model)])\n",
    "    clf = GridSearchCV(pipe, cv=skf, param_grid=params, n_jobs=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('The mean cross_val accuracy on train is',\n",
    "          f'{clf.cv_results_[\"mean_test_score\"]}.')\n",
    "    print('The std of the cross_val accuracy is',\n",
    "          f'{clf.cv_results_[\"std_test_score\"]}.')\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print(classification_report(y_eval, y_pred))\n",
    "    print(confusion_matrix(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:25.649564Z",
     "start_time": "2019-08-05T20:42:19.879363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.39163498].\n",
      "The std of the cross_val accuracy is [0.03240037].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      bush41       0.25      0.68      0.37        25\n",
      "     clinton       0.25      0.19      0.21        16\n",
      "  eisenhower       0.39      0.47      0.43        19\n",
      "         fdr       0.60      0.19      0.29        16\n",
      "   jefferson       0.50      0.10      0.17        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.63      0.59      0.61        29\n",
      "       obama       0.40      0.08      0.13        25\n",
      "      reagan       0.27      0.38      0.32        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       176\n",
      "   macro avg       0.33      0.27      0.25       176\n",
      "weighted avg       0.38      0.35      0.32       176\n",
      "\n",
      "[[17  2  3  0  0  0  0  0  3  0]\n",
      " [ 8  3  0  0  0  1  0  1  3  0]\n",
      " [ 7  0  9  0  0  0  0  1  2  0]\n",
      " [ 7  0  0  3  0  0  3  0  3  0]\n",
      " [ 2  0  1  0  1  0  2  0  4  0]\n",
      " [ 3  3  0  0  1  0  1  1  0  0]\n",
      " [ 5  0  2  0  0  0 17  0  5  0]\n",
      " [13  3  1  1  0  0  1  2  4  0]\n",
      " [ 5  1  7  0  0  0  2  0  9  0]\n",
      " [ 1  0  0  1  0  0  1  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:25.996048Z",
     "start_time": "2019-08-05T20:42:25.657639Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e6b111db4afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_reg_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtfidf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c400e75c5aec>\u001b[0m in \u001b[0;36mfit_and_predict\u001b[0;34m(model, params, X_train, y_train, X_eval, y_eval, X_holdout, y_holdout)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_eval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_holdout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_holdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.115099Z",
     "start_time": "2019-08-05T20:40:54.130Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(tree, params=tree_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.121518Z",
     "start_time": "2019-08-05T20:40:54.134Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(tree, params=tree_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.127102Z",
     "start_time": "2019-08-05T20:40:54.139Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(forest, params=forest_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.129212Z",
     "start_time": "2019-08-05T20:40:54.144Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(forest, params=forest_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.132731Z",
     "start_time": "2019-08-05T20:40:54.149Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(boost, params=boost_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.134619Z",
     "start_time": "2019-08-05T20:40:54.155Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(boost, params=boost_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.138736Z",
     "start_time": "2019-08-05T20:40:54.160Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(nb, params=nb_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T20:42:26.140335Z",
     "start_time": "2019-08-05T20:40:54.164Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_and_predict(nb, params=nb_params, **tfidf_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1099px",
    "right": "20px",
    "top": "2px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
