{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is its performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.810516Z",
     "start_time": "2019-08-12T13:39:51.906732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.1.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.1.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.1.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.1.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.1.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.1.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.1.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.1.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.1.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up data science environment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.palettes import all_palettes\n",
    "from bokeh.plotting import figure, show\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize, Normalizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from typing import Dict\n",
    "output_notebook()\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "sns.set_style('white')\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    module='scipy',\n",
    "    message='internal gelsd'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Processing, and Language Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.831651Z",
     "start_time": "2019-08-12T13:39:57.820315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lists for files and presidents.\n",
    "files = [\"1789-Washington.txt\",\n",
    "         \"1801-Jefferson.txt\",\n",
    "         \"1861-Lincoln.txt\",\n",
    "         \"1933-Roosevelt.txt\",\n",
    "         \"1953-Eisenhower.txt\",\n",
    "         \"1961-Kennedy.txt\",\n",
    "         \"1981-Reagan.txt\",\n",
    "         \"1989-Bush.txt\",\n",
    "         \"1993-Clinton.txt\",\n",
    "         \"2009-Obama.txt\"]\n",
    "\n",
    "presidents = [\"washington\",\n",
    "              \"jefferson\",\n",
    "              \"lincoln\",\n",
    "              \"fdr\",\n",
    "              \"eisenhower\",\n",
    "              \"kennedy\",\n",
    "              \"reagan\",\n",
    "              \"ghwbush\",\n",
    "              \"clinton\",\n",
    "              \"obama\"]\n",
    "\n",
    "# Control to make sure both lists are the same length.\n",
    "assert len(files) == len(presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.859177Z",
     "start_time": "2019-08-12T13:39:57.841478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop to open files.\n",
    "docs = []\n",
    "for file_name, president in zip(files, presidents):\n",
    "    with open(f'./inaugural/{file_name}') as f:\n",
    "        doc = f.read()\n",
    "        docs.append((doc, president))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.874651Z",
     "start_time": "2019-08-12T13:39:57.865954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text: str) -> str:\n",
    "    \"\"\"Function to strip all characters except letters in words.\"\"\"\n",
    "    \n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.911233Z",
     "start_time": "2019-08-12T13:39:57.884695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use text_cleaner on the docs, combine them into data frame (clean_docs).\n",
    "clean_docs = []\n",
    "for doc, pres in docs:\n",
    "    clean_doc = text_cleaner(doc)\n",
    "    clean_docs.append((clean_doc, pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:39:57.936268Z",
     "start_time": "2019-08-12T13:39:57.921236Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives: Among the vicissitudes incident t WASHINGTON\n",
      "\n",
      "Friends and Fellow Citizens: Called upon to undertake the duties of the first executive office of ou JEFFERSON\n",
      "\n",
      "Fellow-Citizens of the United States: In compliance with a custom as old as the Government itself, I LINCOLN\n",
      "\n",
      "I am certain that my fellow Americans expect that on my induction into the Presidency I will address FDR\n",
      "\n",
      "My friends, before I begin the expression of those thoughts that I deem appropriate to this moment,  EISENHOWER\n",
      "\n",
      "Vice President Johnson, Mr. Speaker, Mr. Chief Justice, President Eisenhower, Vice President Nixon,  KENNEDY\n",
      "\n",
      "Senator Hatfield, Mr. Chief Justice, Mr. President, Vice President Bush, Vice President Mondale, Sen REAGAN\n",
      "\n",
      "Mr. Chief Justice, Mr. President, Vice President Quayle, Senator Mitchell, Speaker Wright, Senator D GHWBUSH\n",
      "\n",
      "My fellow citizens, today we celebrate the mystery of American renewal. This ceremony is held in the CLINTON\n",
      "\n",
      "My fellow citizens: I stand here today humbled by the task before us, grateful for the trust you hav OBAMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each doc and print the first 1000 characters for inspection.\n",
    "for doc, pres in clean_docs:\n",
    "    print(doc[:100], pres.upper()) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.304968Z",
     "start_time": "2019-08-12T13:39:57.942493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define nlp as spacy.\n",
    "nlp = spacy.load('en')\n",
    "# Create an empty list for df.\n",
    "df_list = []\n",
    "\n",
    "\n",
    "# Create a function to parse data.\n",
    "def nlp_text(text_file: str) -> doc:\n",
    "    \"\"\"Function that takes a text file and tokenizes it with spacy.\"\"\"\n",
    "    return nlp(text_file)\n",
    "\n",
    "\n",
    "# Create a function to lemmatize sentences.\n",
    "def sentences(doc_nlp: str, speaker: str) -> [str, str]:\n",
    "    \"\"\"Function that takes two strings, lemmatizes the first string and \n",
    "    returns a list with two strings.\n",
    "    \"\"\"\n",
    "    return [[sent.lemma_, speaker] for sent in doc_nlp.sents]\n",
    "\n",
    "\n",
    "# Create a function to combine groups of sentences into one data frame.\n",
    "def sentences_to_df(sents):\n",
    "    \"\"\"Function that takes a string and returns a data frame.\"\"\"\n",
    "    return pd.DataFrame(sents)\n",
    "\n",
    "\n",
    "# Calling each function.\n",
    "for doc, pres in clean_docs:\n",
    "    parsed = nlp_text(doc)\n",
    "    sents = sentences(parsed, pres)\n",
    "    df = sentences_to_df(sents)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.325712Z",
     "start_time": "2019-08-12T13:40:05.314159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine each sentence data frame into one master data frame.\n",
    "sent_df = pd.concat([*df_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.352161Z",
     "start_time": "2019-08-12T13:40:05.332216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ghwbush       145\n",
       "lincoln       139\n",
       "reagan        130\n",
       "eisenhower    121\n",
       "obama         113\n",
       "fdr            86\n",
       "clinton        82\n",
       "kennedy        53\n",
       "jefferson      42\n",
       "washington     25\n",
       "Name: President, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns.\n",
    "sent_df.columns = ['sentence', 'President']\n",
    "\n",
    "# Check the count of sents per President.\n",
    "sent_df.President.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.376311Z",
     "start_time": "2019-08-12T13:40:05.358886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out pronouns from results.\n",
    "sent_df['sentence'] = sent_df['sentence'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.389497Z",
     "start_time": "2019-08-12T13:40:05.380777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data.\n",
    "X = sent_df.sentence\n",
    "y = sent_df.President\n",
    "X_train_eval, X_holdout, y_train_eval, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.401026Z",
     "start_time": "2019-08-12T13:40:05.394417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting into train/eval/holdout groups.\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train_eval, y_train_eval, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.409348Z",
     "start_time": "2019-08-12T13:40:05.404019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create base parameters dictionary.\n",
    "base_param_dict = {'strip_accents': 'unicode',\n",
    "                   'lowercase': True,\n",
    "                   'stop_words': 'english',\n",
    "                   'ngram_range': (1, 3),\n",
    "                   'max_df': 0.5,\n",
    "                   'min_df': 5,\n",
    "                   'max_features': 1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.415387Z",
     "start_time": "2019-08-12T13:40:05.412059Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer.\n",
    "bow = CountVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.507914Z",
     "start_time": "2019-08-12T13:40:05.418665Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into dfs of bags of words.\n",
    "_bow_train = bow.fit_transform(X_train)\n",
    "_bow_eval = bow.transform(X_eval)\n",
    "_bow_holdout = bow.transform(X_holdout)\n",
    "assert len(X_train) == _bow_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names = bow.get_feature_names()\n",
    "\n",
    "# Sparse matrix to data frame.\n",
    "X_train_bow = pd.DataFrame(_bow_train.toarray(), columns=feature_names)\n",
    "X_eval_bow = pd.DataFrame(_bow_eval.toarray(), columns=feature_names)\n",
    "X_holdout_bow = pd.DataFrame(_bow_holdout.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.519034Z",
     "start_time": "2019-08-12T13:40:05.513299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Tfidf.\n",
    "tfidf = TfidfVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.659392Z",
     "start_time": "2019-08-12T13:40:05.523246Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into scipy sparse matrices of tfidf values.\n",
    "_tfidf_train = tfidf.fit_transform(X_train)\n",
    "_tfidf_eval = tfidf.transform(X_eval)\n",
    "_tfidf_holdout = tfidf.transform(X_holdout)\n",
    "assert len(X_train) == _tfidf_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names_tfidf = tfidf.get_feature_names()\n",
    "\n",
    "# Sparse matrix to data frames.\n",
    "X_train_tfidf = pd.DataFrame(\n",
    "    _tfidf_train.toarray(), columns=feature_names_tfidf)\n",
    "X_eval_tfidf = pd.DataFrame(\n",
    "    _tfidf_eval.toarray(), columns=feature_names_tfidf)\n",
    "X_holdout_tfidf = pd.DataFrame(\n",
    "    _tfidf_holdout.toarray(), columns=feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.689085Z",
     "start_time": "2019-08-12T13:40:05.663705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Weights:\n",
      "            word  avg_weight\n",
      "147      people       0.032\n",
      "227       world       0.028\n",
      "80   government       0.028\n",
      "131      nation       0.026\n",
      "206        time       0.025\n",
      "108         let       0.023\n",
      "79         good       0.023\n",
      "120        make       0.022\n",
      "111        life       0.022\n",
      "81        great       0.022\n",
      "\n",
      "Eval Weights:\n",
      "            word  avg_weight\n",
      "135         new       0.029\n",
      "226        work       0.029\n",
      "81        great       0.028\n",
      "74      freedom       0.027\n",
      "132    national       0.027\n",
      "194      states       0.025\n",
      "131      nation       0.024\n",
      "80   government       0.023\n",
      "147      people       0.023\n",
      "158   principle       0.023\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights on training data.\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame(\n",
    "    {'word': tfidf.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"\\nTrain Weights:\\n\", weights_df.sort_values(\n",
    "    by='avg_weight', ascending=False).head(10))\n",
    "\n",
    "# Calculate weights on eval data.\n",
    "weights = np.asarray(X_eval_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame(\n",
    "    {'word': tfidf.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"\\nEval Weights:\\n\", weights_df.sort_values(\n",
    "    by='avg_weight', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.794302Z",
     "start_time": "2019-08-12T13:40:05.692627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by components: 74.92015598847075\n",
      "\n",
      "Component 0:\n",
      "sentence\n",
      " government have no power except that grant  by the people .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.430\n",
      "to the people of poor nation ,  pledge to work alongside  to make  farm flourish and let clean water flow ; to nourish starve body and feed hungry mind .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.422\n",
      "americans deserve good , and in this city today there be people who want to do good , and so  say to all of  here , let  resolve to reform  politic , so that power and privilege no longer shout down the voice of the people .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.422\n",
      "by  gracious cooperation in the transition process ,  have show a watch world that  be a united people pledge to maintain a political system which guarantee individual liberty to a great degree than any other , and  thank  and  people for all  help in maintain the continuity which be the bulwark of  republic .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.415\n",
      "equal and exact justice to all man , of whatev state or persuasion , religious or political ; peace , commerce , and honest friendship with all nation , entangle alliance with none ; the support of the state government in all  right , as the most competent administration for  domestic concern and the sure bulwark against antirepublican tendency ; the preservation of the general government in  whole constitutional vigor , as the sheet anchor of  peace at home and safety abroad ; a jealous care of the right of election by the people a mild and safe corrective of abuse which be lop by the sword of revolution where peaceable remedy be unprovided ; absolute acquiescence in the decision of the majority , the vital principle of republic , from which be no appeal but to force , the vital principle and immediate parent of despotism ; a well discipline militia ,  good reliance in peace and for the first moment of war , till regular may relieve  ; the supremacy of the civil over the military authority ; economy in the public expense , that labor may be lightly burthen ; the honest payment of  debt and sacred preservation of the public faith ; encouragement of agriculture , and of commerce as  handmaid ; the diffusion of information and arraignment of all abuse at the bar of the public reason ; freedom of religion ; freedom of the press , and freedom of person under the protection of the habeas corpus , and trial by jury impartially select .   0.413\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Component 1:\n",
      "sentence\n",
      "now , so there will be no misunderstanding ,  be not  intention to do away with government .                                                                                                                                                        0.625\n",
      "the government will not assail  .                                                                                                                                                                                                                   0.625\n",
      "from time to time ,  have be tempt to believe that society have become too complex to be manage by self - rule , that government by an elite group be superior to government for , by , and of the people .                                         0.588\n",
      "while the strict legal right may exist in the government to enforce the exercise of these office , the attempt to do so would be so irritate and so nearly impracticable withal that  deem  better to forego for the time the us of such office .   0.565\n",
      " be time to check and reverse the growth of government which show sign of have grow beyond the consent of the govern .                                                                                                                              0.520\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "Component 2:\n",
      "sentence\n",
      "let  embrace  .                                                                                                                                                                 0.548\n",
      "so let  mark this day with remembrance , of who  be and how far  have travel .                                                                                                  0.510\n",
      "so , as  begin , let  take inventory .                                                                                                                                          0.505\n",
      "so let  begin anew remember on both side that civility be not a sign of weakness , and sincerity be always subject to proof .                                                   0.494\n",
      "and so ,  fellow americans , as  stand at the edge of the st century , let  begin anew , with energy and hope , with faith and discipline , and let  work until  work be do .   0.466\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "Component 3:\n",
      "sentence\n",
      "technology be almost magical , and ambition for a good life be now universal .                                                                                                        0.685\n",
      "disease diminish and life lengthen .                                                                                                                                                  0.598\n",
      " threaten to shatter the life of million of  people .                                                                                                                                 0.590\n",
      "for  ,  pack up  few worldly possession and travel across ocean in search of a new life .                                                                                             0.515\n",
      " be no coincidence that  present trouble parallel and be proportionate to the intervention and intrusion in  life that result from unnecessary and excessive growth of government .   0.485\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Component 4:\n",
      "sentence\n",
      " have no vision , and when there be no vision the people perish .                                                                        0.507\n",
      "the chief magistrate derive all  authority from the people , and  have refer none upon  to fix term for the separation of the states .   0.437\n",
      "the people of the united states have not fail .                                                                                          0.412\n",
      "especially  pray that  concern shall be for all the people regardless of station , race , or call .                                      0.391\n",
      "for the impoverishment of any single people in the world mean danger to the well - being of all other people .                           0.380\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Reduce feature space to 100 features with SVD.\n",
    "svd = TruncatedSVD(100)\n",
    "\n",
    "# Make pipeline to run SVD and normalize results.\n",
    "lsa_pipe = make_pipeline(svd, Normalizer())\n",
    "\n",
    "# Fit with training data, transform test data.\n",
    "X_train_lsa = lsa_pipe.fit_transform(X_train_tfidf)\n",
    "X_eval_lsa = lsa_pipe.transform(X_eval_tfidf)\n",
    "X_holdout_lsa = lsa_pipe.transform(X_holdout_tfidf)\n",
    "\n",
    "# Examine variance captured in reduced feature space.\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('Percent variance captured by components:', total_variance*100)\n",
    "\n",
    "sent_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "\n",
    "# Look at values from first 5 components.\n",
    "for i in range(5):\n",
    "    print('\\nComponent {}:'.format(i))\n",
    "    print(sent_by_component.loc[:, i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.802659Z",
     "start_time": "2019-08-12T13:40:05.797434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate MinMaxScaler.\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:05.815261Z",
     "start_time": "2019-08-12T13:40:05.806245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train/eval/holdout groups for LSA.\n",
    "X_train_lsa_scaled = pd.DataFrame(scaler.fit_transform(X_train_lsa))\n",
    "X_eval_lsa_scaled = pd.DataFrame(scaler.transform(X_eval_lsa))\n",
    "X_holdout_lsa_scaled = pd.DataFrame(scaler.transform(X_holdout_lsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.222909Z",
     "start_time": "2019-08-12T13:40:05.823525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 2\t silhouette: 0.02403808152620107\n",
      "\n",
      "KMeans \n",
      " col_0    0   1\n",
      "row_0         \n",
      "0      487   0\n",
      "1        0  39 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=2,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 2\t silhouette: 0.02453107421171958\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1\n",
      "row_0         \n",
      "0      433  28\n",
      "1       58   7 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 3\t silhouette: 0.024610759668237767\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2\n",
      "row_0             \n",
      "0      395   0   0\n",
      "1        0  59   0\n",
      "2        0   0  72 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=3,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 3\t silhouette: 0.0256988943152478\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2\n",
      "row_0             \n",
      "0      409  38  12\n",
      "1       14   1   1\n",
      "2       50   1   0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 4\t silhouette: 0.02862078840773946\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3\n",
      "row_0                 \n",
      "0      408   0   0   0\n",
      "1        0  38   0   0\n",
      "2        0   0  58   0\n",
      "3        0   0   0  22 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=4,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 4\t silhouette: 0.02424850133080747\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2   3\n",
      "row_0                 \n",
      "0        7   3   1   1\n",
      "1       76  26   8   3\n",
      "2      250  91  24  18\n",
      "3        9   6   3   0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 5\t silhouette: 0.026262398061972524\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4\n",
      "row_0                     \n",
      "0      312   0   0   0   0\n",
      "1        0  69   0   0   0\n",
      "2        0   0  50   0   0\n",
      "3        0   0   0  42   0\n",
      "4        0   0   0   0  53 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=5,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 5\t silhouette: 0.023436519961504143\n",
      "\n",
      "MiniBatch \n",
      " col_0    0  1   2   3  4\n",
      "row_0                   \n",
      "0        1  0   0   0  0\n",
      "1        0  0   1   0  0\n",
      "2        0  0   1   0  0\n",
      "3        8  0   1   1  0\n",
      "4      378  8  83  39  5 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 6\t silhouette: 0.029523842709622067\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5\n",
      "row_0                         \n",
      "0      305   0   0   0   0   0\n",
      "1        0  84   0   0   0   0\n",
      "2        0   0  21   0   0   0\n",
      "3        0   0   0  36   0   0\n",
      "4        0   0   0   0  27   0\n",
      "5        0   0   0   0   0  53 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=6,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 6\t silhouette: 0.02648139540856822\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2  3  4   5\n",
      "row_0                       \n",
      "0      211  86  34  9  8  22\n",
      "1        1  26   1  3  0   4\n",
      "2       15  17   1  1  0   3\n",
      "3       16   2   1  2  1   0\n",
      "4       17   2   1  2  0   0\n",
      "5       23   5   6  5  0   1 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 7\t silhouette: 0.03467250479976267\n",
      "\n",
      "KMeans \n",
      " col_0   0   1    2   3   4   5   6\n",
      "row_0                             \n",
      "0      23   0    0   0   0   0   0\n",
      "1       0  19    0   0   0   0   0\n",
      "2       0   0  428   0   0   0   0\n",
      "3       0   0    0  11   0   0   0\n",
      "4       0   0    0   0  21   0   0\n",
      "5       0   0    0   0   0  13   0\n",
      "6       0   0    0   0   0   0  11 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=7,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 7\t silhouette: 0.027491503999915453\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2   3   4   5   6\n",
      "row_0                             \n",
      "0      197  19   2  13  22  11   1\n",
      "1        7   1   1   0   2   3  16\n",
      "2       11  10   0   1   1   0   0\n",
      "3        0   1   1  22   1   1   0\n",
      "4       59  11   9   0  21   6   1\n",
      "5       36   4  11   0   3   3   0\n",
      "6        9   3   0   3   2   0   1 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 8\t silhouette: 0.032527760077513825\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7\n",
      "row_0                                 \n",
      "0      234   0   0   0   0   0   0   0\n",
      "1        0  38   0   0   0   0   0   0\n",
      "2        0   0  37   0   0   0   0   0\n",
      "3        0   0   0  52   0   0   0   0\n",
      "4        0   0   0   0  38   0   0   0\n",
      "5        0   0   0   0   0  19   0   0\n",
      "6        0   0   0   0   0   0  39   0\n",
      "7        0   0   0   0   0   0   0  69 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=8,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 8\t silhouette: 0.03298196798383018\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1  2   3   4  5  6   7\n",
      "row_0                              \n",
      "0       75  33  0   1  12  1  4  30\n",
      "1      172  12  3   2  32  3  0  13\n",
      "2        8   1  0   0   0  0  0   1\n",
      "3        8   2  0   0   1  0  0   1\n",
      "4       17   1  0   0   0  1  0   1\n",
      "5        5   0  0   0   3  0  1   1\n",
      "6       11   4  1  10   1  3  0   9\n",
      "7       27   5  0   0   2  0  0   8 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=9, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 9\t silhouette: 0.0334546650517409\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7   8\n",
      "row_0                                     \n",
      "0      228   0   0   0   0   0   0   0   0\n",
      "1        0  59   0   0   0   0   0   0   0\n",
      "2        0   0  43   0   0   0   0   0   0\n",
      "3        0   0   0  29   0   0   0   0   0\n",
      "4        0   0   0   0  14   0   0   0   0\n",
      "5        0   0   0   0   0  24   0   0   0\n",
      "6        0   0   0   0   0   0  20   0   0\n",
      "7        0   0   0   0   0   0   0  86   0\n",
      "8        0   0   0   0   0   0   0   0  23 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=9,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 9\t silhouette: 0.029531641795308695\n",
      "\n",
      "MiniBatch \n",
      " col_0   0   1   2  3   4    5   6   7   8\n",
      "row_0                                    \n",
      "0       2   1   2  1   0   21   2   2   1\n",
      "1       8   2   4  1   1    2   1   0   4\n",
      "2      15   1   2  0   0    1   2   1   1\n",
      "3       2   0   0  1   0   15   1   0   0\n",
      "4      36  13  19  2  11  164  11  13   7\n",
      "5      26   2   2  7  17   13   0   1   0\n",
      "6       0   0   0  1   0    0   0   0   0\n",
      "7       4   1   5  3   4    3   0   1  26\n",
      "8       1   0   6  0   1   19   3   6   1 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters: 10\t silhouette: 0.038866084346253535\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7   8   9\n",
      "row_0                                         \n",
      "0      251   0   0   0   0   0   0   0   0   0\n",
      "1        0  43   0   0   0   0   0   0   0   0\n",
      "2        0   0  24   0   0   0   0   0   0   0\n",
      "3        0   0   0  29   0   0   0   0   0   0\n",
      "4        0   0   0   0  21   0   0   0   0   0\n",
      "5        0   0   0   0   0  61   0   0   0   0\n",
      "6        0   0   0   0   0   0  27   0   0   0\n",
      "7        0   0   0   0   0   0   0  32   0   0\n",
      "8        0   0   0   0   0   0   0   0  18   0\n",
      "9        0   0   0   0   0   0   0   0   0  20 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=10,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 10\t silhouette: 0.03109390089034699\n",
      "\n",
      "MiniBatch \n",
      " col_0   0    1   2  3  4   5   6  7   8  9\n",
      "row_0                                     \n",
      "0       1    9   0  0  0   0   0  0   0  0\n",
      "1      63  291  50  5  6  25   1  7  17  6\n",
      "2       2    1   0  0  0   0   0  0   0  0\n",
      "3       0    0   0  0  0   1   0  0   0  0\n",
      "4       8    6   0  0  0   0   0  0   0  0\n",
      "5       0    1   0  0  0   0   0  0   0  0\n",
      "6       0    0   0  0  0   0   0  1   0  0\n",
      "7       4    2   0  0  0   0   0  0   0  0\n",
      "8       0    0   0  0  0   0  17  0   0  0\n",
      "9       0    1   0  0  0   0   0  0   1  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clustering models\n",
    "models = []\n",
    "names = []\n",
    "plot_nums = []\n",
    "silhouettes = []\n",
    "clust = []\n",
    "\n",
    "for clusters in range(2, 11):\n",
    "    models.append(\n",
    "        (0, 'KMeans', KMeans(n_clusters=clusters,\n",
    "                             init='k-means++', random_state=15)))\n",
    "    models.append(\n",
    "        (1, 'MiniBatch', MiniBatchKMeans(init='random',\n",
    "                                         n_clusters=clusters,\n",
    "                                         batch_size=500)))\n",
    "# Check for numbers.**\n",
    "for _, name, model in models:\n",
    "    names.append(name)\n",
    "    model.fit(X_train_tfidf)\n",
    "    labels = model.labels_\n",
    "    print(model)\n",
    "    if len(set(labels)) > 1:\n",
    "        ypred = model.fit_predict(X_train_tfidf)\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X_train_tfidf, labels, metric='euclidean')\n",
    "        silhouettes.append(silhouette)\n",
    "        if silhouette > 0:\n",
    "            print('clusters: {}\\t silhouette: {}\\n'.format(\n",
    "                model.n_clusters, silhouette))\n",
    "            print(name, '\\n', pd.crosstab(ypred, labels), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.497619Z",
     "start_time": "2019-08-12T13:40:10.230677Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Re-run KMeans and extract cluster information.\n",
    "model_tfidf = KMeans(n_clusters=10, random_state=15).fit(X_train_tfidf)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = model_tfidf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.532017Z",
     "start_time": "2019-08-12T13:40:10.506302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster assignment for eval, holdout groups.\n",
    "X_eval_tfidf_labels = model_tfidf.predict(X_eval_tfidf)\n",
    "X_holdout_tfidf_labels = model_tfidf.predict(X_holdout_tfidf)\n",
    "\n",
    "# Create a column for cluster labels.\n",
    "X_eval_tfidf['clusters'] = X_eval_tfidf_labels\n",
    "X_holdout_tfidf['clusters'] = X_holdout_tfidf_labels\n",
    "\n",
    "X_train_tfidf['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.598364Z",
     "start_time": "2019-08-12T13:40:10.539361Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>administration</th>\n",
       "      <th>america</th>\n",
       "      <th>american</th>\n",
       "      <th>american people</th>\n",
       "      <th>americans</th>\n",
       "      <th>ask</th>\n",
       "      <th>authority</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>willing</th>\n",
       "      <th>wish</th>\n",
       "      <th>woman</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters   act  action  administration  america  american  american people  \\\n",
       "0         0 0.015   0.005           0.003    0.019     0.002            0.000   \n",
       "1         1 0.000   0.000           0.000    0.007     0.007            0.000   \n",
       "2         2 0.000   0.000           0.000    0.017     0.000            0.000   \n",
       "3         3 0.000   0.000           0.000    0.016     0.021            0.000   \n",
       "4         4 0.000   0.009           0.018    0.000     0.023            0.000   \n",
       "5         5 0.005   0.009           0.008    0.029     0.021            0.023   \n",
       "6         6 0.011   0.036           0.000    0.015     0.013            0.014   \n",
       "7         7 0.000   0.016           0.023    0.000     0.007            0.008   \n",
       "8         8 0.027   0.000           0.000    0.021     0.000            0.000   \n",
       "9         9 0.019   0.000           0.000    0.037     0.000            0.000   \n",
       "\n",
       "   americans   ask  authority  ...   way  willing  wish  woman  word  work  \\\n",
       "0      0.011 0.010      0.011  ... 0.003    0.000 0.004  0.006 0.001 0.000   \n",
       "1      0.014 0.012      0.000  ... 0.000    0.013 0.007  0.008 0.009 0.006   \n",
       "2      0.012 0.000      0.000  ... 0.030    0.030 0.000  0.000 0.010 0.000   \n",
       "3      0.019 0.000      0.000  ... 0.037    0.018 0.000  0.000 0.000 0.000   \n",
       "4      0.043 0.000      0.000  ... 0.000    0.000 0.000  0.000 0.000 0.000   \n",
       "5      0.000 0.006      0.016  ... 0.008    0.000 0.008  0.015 0.005 0.000   \n",
       "6      0.027 0.016      0.000  ... 0.000    0.000 0.000  0.000 0.160 0.000   \n",
       "7      0.000 0.037      0.004  ... 0.022    0.000 0.010  0.000 0.000 0.000   \n",
       "8      0.036 0.017      0.000  ... 0.000    0.019 0.000  0.000 0.015 0.505   \n",
       "9      0.000 0.000      0.000  ... 0.000    0.000 0.000  0.000 0.019 0.000   \n",
       "\n",
       "   world  write  year  young  \n",
       "0  0.001  0.006 0.003  0.007  \n",
       "1  0.025  0.000 0.000  0.017  \n",
       "2  0.017  0.000 0.015  0.000  \n",
       "3  0.015  0.000 0.000  0.000  \n",
       "4  0.010  0.000 0.000  0.000  \n",
       "5  0.195  0.000 0.006  0.000  \n",
       "6  0.025  0.053 0.030  0.012  \n",
       "7  0.000  0.000 0.041  0.000  \n",
       "8  0.000  0.018 0.000  0.000  \n",
       "9  0.000  0.000 0.000  0.000  \n",
       "\n",
       "[10 rows x 232 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by cluster.\n",
    "X_train_tfidf_clusters = X_train_tfidf.groupby(\n",
    "    ['clusters'], as_index=False).mean()\n",
    "X_train_tfidf_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.773254Z",
     "start_time": "2019-08-12T13:40:10.601427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "day            0.028\n",
      "freedom        0.022\n",
      "god            0.021\n",
      "right          0.020\n",
      "union          0.019\n",
      "constitution   0.019\n",
      "america        0.019\n",
      "make           0.019\n",
      "man            0.018\n",
      "old            0.017\n",
      "dtype: float64\n",
      "\n",
      "principle    0.031\n",
      "law          0.029\n",
      "believe      0.027\n",
      "faith        0.026\n",
      "states       0.024\n",
      "way          0.023\n",
      "new          0.023\n",
      "generation   0.021\n",
      "union        0.021\n",
      "case         0.021\n",
      "dtype: float64\n",
      "\n",
      "There are 1 words that are in the top ten of         both training and testing.\n",
      "These words are: {'union'}.\n",
      "\n",
      "Cluster 1:\n",
      "time         0.264\n",
      "nation       0.228\n",
      "change       0.047\n",
      "great        0.040\n",
      "government   0.035\n",
      "live         0.032\n",
      "make         0.032\n",
      "require      0.031\n",
      "god          0.029\n",
      "believe      0.029\n",
      "dtype: float64\n",
      "\n",
      "nation    0.289\n",
      "time      0.209\n",
      "freedom   0.134\n",
      "great     0.101\n",
      "new       0.095\n",
      "right     0.094\n",
      "think     0.075\n",
      "speak     0.073\n",
      "power     0.062\n",
      "future    0.060\n",
      "dtype: float64\n",
      "\n",
      "There are 3 words that are in the top ten of         both training and testing.\n",
      "These words are: {'nation', 'time', 'great'}.\n",
      "\n",
      "Cluster 2:\n",
      "good         0.444\n",
      "make         0.088\n",
      "power        0.070\n",
      "government   0.061\n",
      "hope         0.060\n",
      "life         0.043\n",
      "discipline   0.043\n",
      "equal        0.040\n",
      "public       0.036\n",
      "peace        0.036\n",
      "dtype: float64\n",
      "\n",
      "good       0.357\n",
      "land       0.191\n",
      "national   0.175\n",
      "action     0.141\n",
      "place      0.138\n",
      "hold       0.131\n",
      "friend     0.128\n",
      "meet       0.126\n",
      "right      0.116\n",
      "use        0.101\n",
      "dtype: float64\n",
      "\n",
      "There are 1 words that are in the top ten of         both training and testing.\n",
      "These words are: {'good'}.\n",
      "\n",
      "Cluster 3:\n",
      "life      0.323\n",
      "hand      0.219\n",
      "heart     0.097\n",
      "new       0.067\n",
      "present   0.057\n",
      "faith     0.052\n",
      "know      0.043\n",
      "great     0.041\n",
      "way       0.037\n",
      "make      0.030\n",
      "dtype: float64\n",
      "\n",
      "life             0.279\n",
      "hand             0.260\n",
      "national         0.191\n",
      "value            0.134\n",
      "form             0.105\n",
      "administration   0.096\n",
      "leader           0.088\n",
      "majority         0.086\n",
      "heart            0.080\n",
      "day              0.075\n",
      "dtype: float64\n",
      "\n",
      "There are 3 words that are in the top ten of         both training and testing.\n",
      "These words are: {'hand', 'life', 'heart'}.\n",
      "\n",
      "Cluster 4:\n",
      "citizen          0.306\n",
      "fellow           0.276\n",
      "fellow citizen   0.175\n",
      "president        0.095\n",
      "country          0.064\n",
      "man              0.054\n",
      "great            0.049\n",
      "free             0.048\n",
      "duty             0.044\n",
      "americans        0.043\n",
      "dtype: float64\n",
      "\n",
      "citizen          0.458\n",
      "ask              0.247\n",
      "fellow citizen   0.203\n",
      "fellow           0.181\n",
      "america          0.154\n",
      "today            0.146\n",
      "hope             0.143\n",
      "world            0.125\n",
      "mind             0.120\n",
      "heart            0.111\n",
      "dtype: float64\n",
      "\n",
      "There are 3 words that are in the top ten of         both training and testing.\n",
      "These words are: {'fellow', 'fellow citizen', 'citizen'}.\n",
      "\n",
      "Cluster 5:\n",
      "world           0.195\n",
      "people          0.194\n",
      "free            0.054\n",
      "states          0.049\n",
      "united          0.047\n",
      "great           0.039\n",
      "shall           0.037\n",
      "united states   0.036\n",
      "economic        0.035\n",
      "peace           0.030\n",
      "dtype: float64\n",
      "\n",
      "people            0.177\n",
      "world             0.175\n",
      "peace             0.097\n",
      "america           0.095\n",
      "great             0.086\n",
      "american people   0.075\n",
      "purpose           0.069\n",
      "american          0.068\n",
      "year              0.058\n",
      "americans         0.058\n",
      "dtype: float64\n",
      "\n",
      "There are 4 words that are in the top ten of         both training and testing.\n",
      "These words are: {'peace', 'great', 'people', 'world'}.\n",
      "\n",
      "Cluster 6:\n",
      "need           0.257\n",
      "word           0.160\n",
      "deny           0.098\n",
      "write          0.053\n",
      "constitution   0.043\n",
      "speak          0.040\n",
      "factory        0.040\n",
      "great          0.039\n",
      "material       0.038\n",
      "moment         0.037\n",
      "dtype: float64\n",
      "\n",
      "need        0.501\n",
      "action      0.152\n",
      "serve       0.126\n",
      "care        0.119\n",
      "congress    0.109\n",
      "executive   0.107\n",
      "face        0.106\n",
      "country     0.100\n",
      "authority   0.097\n",
      "today       0.097\n",
      "dtype: float64\n",
      "\n",
      "There are 1 words that are in the top ten of         both training and testing.\n",
      "These words are: {'need'}.\n",
      "\n",
      "Cluster 7:\n",
      "government   0.310\n",
      "support      0.135\n",
      "shall        0.098\n",
      "people       0.065\n",
      "federal      0.057\n",
      "year         0.041\n",
      "right        0.041\n",
      "power        0.039\n",
      "law          0.039\n",
      "states       0.038\n",
      "dtype: float64\n",
      "\n",
      "government      0.356\n",
      "states          0.089\n",
      "opportunity     0.076\n",
      "continue        0.076\n",
      "support         0.069\n",
      "united states   0.067\n",
      "let             0.063\n",
      "united          0.061\n",
      "freedom         0.059\n",
      "union           0.059\n",
      "dtype: float64\n",
      "\n",
      "There are 3 words that are in the top ten of         both training and testing.\n",
      "These words are: {'states', 'government', 'support'}.\n",
      "\n",
      "Cluster 8:\n",
      "work        0.505\n",
      "sacrifice   0.063\n",
      "let         0.056\n",
      "good        0.051\n",
      "return      0.049\n",
      "god         0.046\n",
      "thing       0.046\n",
      "moral       0.043\n",
      "seek        0.042\n",
      "right       0.039\n",
      "dtype: float64\n",
      "\n",
      "work        0.638\n",
      "know        0.093\n",
      "new         0.089\n",
      "people      0.080\n",
      "task        0.080\n",
      "old         0.075\n",
      "friend      0.074\n",
      "man woman   0.062\n",
      "woman       0.061\n",
      "stand       0.060\n",
      "dtype: float64\n",
      "\n",
      "There are 1 words that are in the top ten of         both training and testing.\n",
      "These words are: {'work'}.\n",
      "\n",
      "Cluster 9:\n",
      "let          0.506\n",
      "begin        0.133\n",
      "fear         0.081\n",
      "renew        0.058\n",
      "remember     0.056\n",
      "hope         0.047\n",
      "hard         0.041\n",
      "understand   0.040\n",
      "america      0.037\n",
      "far          0.034\n",
      "dtype: float64\n",
      "\n",
      "let              0.554\n",
      "seek             0.197\n",
      "people           0.159\n",
      "child            0.140\n",
      "responsibility   0.138\n",
      "community        0.135\n",
      "country          0.120\n",
      "turn             0.070\n",
      "end              0.068\n",
      "generation       0.067\n",
      "dtype: float64\n",
      "\n",
      "There are 1 words that are in the top ten of         both training and testing.\n",
      "These words are: {'let'}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = model_tfidf.n_clusters\n",
    "for i in range(num_clusters):\n",
    "    data_train = X_train_tfidf[X_train_tfidf['clusters'] == i]\n",
    "    data_eval = X_eval_tfidf[X_eval_tfidf['clusters'] == i]\n",
    "    print(f'Cluster {i}:')\n",
    "    if i < 1:\n",
    "        train = data_train.mean().sort_values(ascending=False)[:10].copy()\n",
    "        print(train)\n",
    "        print()\n",
    "        evl = data_eval.mean().sort_values(ascending=False)[:10].copy()\n",
    "        print(evl)\n",
    "        print()\n",
    "        train_set, eval_set = {*train.index}, {*evl.index}\n",
    "        overlap = train_set.intersection(eval_set)\n",
    "        print((f'There are {len(overlap)} words that are in the top ten of \\\n",
    "        both training and testing.'))\n",
    "        if len(overlap) > 0:\n",
    "            print(f'These words are: {overlap}.')\n",
    "        print()\n",
    "    elif i >= 1:\n",
    "        train = data_train.mean().sort_values(ascending=False)[1:11].copy()\n",
    "        print(train)\n",
    "        print()\n",
    "        evl = data_eval.mean().sort_values(ascending=False)[1:11].copy()\n",
    "        print(evl)\n",
    "        print()\n",
    "        train_set, eval_set = {*train.index}, {*evl.index}\n",
    "        overlap = train_set.intersection(eval_set)\n",
    "        print(f'There are {len(overlap)} words that are in the top ten of \\\n",
    "        both training and testing.')\n",
    "        if len(overlap) > 0:\n",
    "            print(f'These words are: {overlap}.')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:10.986429Z",
     "start_time": "2019-08-12T13:40:10.781843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-run KMeans and extract cluster information.\n",
    "model_lsa = KMeans(n_clusters=10, random_state=42).fit(X_train_lsa_scaled)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = model_lsa.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.022008Z",
     "start_time": "2019-08-12T13:40:10.999916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster assignment for eval, holdout groups.\n",
    "X_eval_lsa_labels = model_lsa.predict(X_eval_lsa_scaled)\n",
    "X_holdout_lsa_labels = model_lsa.predict(X_holdout_lsa_scaled)\n",
    "\n",
    "# Create a column for cluster labels.\n",
    "X_eval_lsa_scaled['clusters'] = X_eval_lsa_labels\n",
    "X_holdout_lsa_scaled['clusters'] = X_holdout_lsa_labels\n",
    "\n",
    "X_train_lsa_scaled['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.068370Z",
     "start_time": "2019-08-12T13:40:11.025833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters     0     1     2     3     4     5     6     7     8  ...    90  \\\n",
       "0         0 0.247 0.362 0.508 0.354 0.455 0.422 0.442 0.481 0.467  ... 0.468   \n",
       "1         1 0.584 0.360 0.431 0.324 0.461 0.289 0.753 0.563 0.416  ... 0.450   \n",
       "2         2 0.301 0.458 0.520 0.372 0.491 0.513 0.484 0.451 0.367  ... 0.498   \n",
       "3         3 0.424 0.384 0.469 0.410 0.417 0.473 0.585 0.529 0.647  ... 0.464   \n",
       "4         4 0.220 0.341 0.565 0.364 0.453 0.386 0.449 0.561 0.549  ... 0.482   \n",
       "5         5 0.382 0.340 0.518 0.365 0.480 0.362 0.536 0.548 0.458  ... 0.447   \n",
       "6         6 0.270 0.419 0.427 0.410 0.587 0.374 0.459 0.496 0.409  ... 0.459   \n",
       "7         7 0.669 0.344 0.616 0.575 0.381 0.452 0.198 0.776 0.173  ... 0.468   \n",
       "8         8 0.280 0.379 0.477 0.324 0.384 0.430 0.400 0.461 0.591  ... 0.431   \n",
       "9         9 0.640 0.430 0.407 0.346 0.478 0.354 0.413 0.463 0.334  ... 0.477   \n",
       "\n",
       "     91    92    93    94    95    96    97    98    99  \n",
       "0 0.552 0.514 0.362 0.509 0.543 0.526 0.486 0.435 0.379  \n",
       "1 0.552 0.510 0.347 0.512 0.541 0.497 0.466 0.433 0.355  \n",
       "2 0.652 0.566 0.275 0.522 0.649 0.498 0.450 0.435 0.365  \n",
       "3 0.543 0.521 0.360 0.508 0.534 0.519 0.488 0.422 0.382  \n",
       "4 0.550 0.493 0.334 0.513 0.546 0.524 0.560 0.412 0.406  \n",
       "5 0.531 0.511 0.328 0.441 0.531 0.498 0.499 0.499 0.424  \n",
       "6 0.599 0.673 0.394 0.598 0.472 0.643 0.430 0.610 0.370  \n",
       "7 0.556 0.519 0.348 0.498 0.555 0.531 0.490 0.440 0.390  \n",
       "8 0.508 0.461 0.344 0.628 0.556 0.510 0.449 0.427 0.399  \n",
       "9 0.547 0.511 0.349 0.525 0.534 0.521 0.488 0.424 0.388  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by cluster.\n",
    "X_train_lsa_clusters = X_train_lsa_scaled.groupby(\n",
    "    ['clusters'], as_index=False).mean()\n",
    "X_train_lsa_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.200809Z",
     "start_time": "2019-08-12T13:40:11.074107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "91   0.552\n",
      "95   0.543\n",
      "49   0.528\n",
      "96   0.526\n",
      "89   0.523\n",
      "76   0.520\n",
      "73   0.518\n",
      "36   0.517\n",
      "47   0.516\n",
      "34   0.515\n",
      "dtype: float64\n",
      "\n",
      "91   0.571\n",
      "95   0.543\n",
      "89   0.534\n",
      "76   0.525\n",
      "34   0.524\n",
      "73   0.519\n",
      "11   0.519\n",
      "96   0.516\n",
      "36   0.514\n",
      "94   0.514\n",
      "dtype: float64\n",
      "\n",
      "There are 8 features that are in the top ten of both training and testing.\n",
      "These features are: {96, 34, 36, 73, 76, 89, 91, 95}.\n",
      "\n",
      "Cluster 1:\n",
      "6    0.753\n",
      "14   0.624\n",
      "41   0.586\n",
      "0    0.584\n",
      "19   0.567\n",
      "7    0.563\n",
      "10   0.558\n",
      "49   0.555\n",
      "34   0.552\n",
      "91   0.552\n",
      "dtype: float64\n",
      "\n",
      "6    0.743\n",
      "55   0.725\n",
      "14   0.675\n",
      "34   0.630\n",
      "0    0.625\n",
      "41   0.584\n",
      "49   0.576\n",
      "69   0.567\n",
      "7    0.561\n",
      "19   0.560\n",
      "dtype: float64\n",
      "\n",
      "There are 8 features that are in the top ten of both training and testing.\n",
      "These features are: {0, 34, 6, 7, 41, 14, 49, 19}.\n",
      "\n",
      "Cluster 2:\n",
      "61   0.801\n",
      "66   0.789\n",
      "64   0.737\n",
      "32   0.728\n",
      "41   0.677\n",
      "34   0.652\n",
      "91   0.652\n",
      "95   0.649\n",
      "60   0.647\n",
      "44   0.646\n",
      "dtype: float64\n",
      "\n",
      "17   0.850\n",
      "66   0.840\n",
      "60   0.798\n",
      "61   0.764\n",
      "64   0.716\n",
      "41   0.694\n",
      "95   0.669\n",
      "34   0.662\n",
      "79   0.649\n",
      "32   0.637\n",
      "dtype: float64\n",
      "\n",
      "There are 8 features that are in the top ten of both training and testing.\n",
      "These features are: {64, 32, 34, 66, 41, 60, 61, 95}.\n",
      "\n",
      "Cluster 3:\n",
      "21   0.743\n",
      "15   0.730\n",
      "8    0.647\n",
      "20   0.637\n",
      "34   0.635\n",
      "17   0.609\n",
      "41   0.607\n",
      "51   0.587\n",
      "49   0.585\n",
      "6    0.585\n",
      "dtype: float64\n",
      "\n",
      "21   0.808\n",
      "51   0.683\n",
      "34   0.678\n",
      "15   0.671\n",
      "20   0.653\n",
      "8    0.651\n",
      "82   0.649\n",
      "17   0.642\n",
      "73   0.622\n",
      "49   0.621\n",
      "dtype: float64\n",
      "\n",
      "There are 8 features that are in the top ten of both training and testing.\n",
      "These features are: {34, 8, 15, 17, 49, 51, 20, 21}.\n",
      "\n",
      "Cluster 4:\n",
      "41   0.715\n",
      "9    0.677\n",
      "11   0.659\n",
      "76   0.638\n",
      "46   0.613\n",
      "12   0.610\n",
      "33   0.610\n",
      "49   0.595\n",
      "61   0.594\n",
      "78   0.586\n",
      "dtype: float64\n",
      "\n",
      "1    nan\n",
      "2    nan\n",
      "3    nan\n",
      "4    nan\n",
      "5    nan\n",
      "6    nan\n",
      "7    nan\n",
      "8    nan\n",
      "9    nan\n",
      "10   nan\n",
      "dtype: float64\n",
      "\n",
      "There are 1 features that are in the top ten of both training and testing.\n",
      "These features are: {9}.\n",
      "\n",
      "Cluster 5:\n",
      "51   0.737\n",
      "23   0.644\n",
      "40   0.614\n",
      "18   0.612\n",
      "60   0.611\n",
      "32   0.586\n",
      "63   0.584\n",
      "26   0.572\n",
      "57   0.567\n",
      "64   0.561\n",
      "dtype: float64\n",
      "\n",
      "51   0.742\n",
      "40   0.691\n",
      "60   0.654\n",
      "97   0.637\n",
      "92   0.617\n",
      "19   0.614\n",
      "95   0.599\n",
      "63   0.590\n",
      "57   0.580\n",
      "47   0.580\n",
      "dtype: float64\n",
      "\n",
      "There are 5 features that are in the top ten of both training and testing.\n",
      "These features are: {40, 51, 57, 60, 63}.\n",
      "\n",
      "Cluster 6:\n",
      "37   0.822\n",
      "57   0.773\n",
      "78   0.770\n",
      "92   0.673\n",
      "55   0.646\n",
      "96   0.643\n",
      "51   0.639\n",
      "9    0.619\n",
      "44   0.613\n",
      "98   0.610\n",
      "dtype: float64\n",
      "\n",
      "1    nan\n",
      "2    nan\n",
      "3    nan\n",
      "4    nan\n",
      "5    nan\n",
      "6    nan\n",
      "7    nan\n",
      "8    nan\n",
      "9    nan\n",
      "10   nan\n",
      "dtype: float64\n",
      "\n",
      "There are 1 features that are in the top ten of both training and testing.\n",
      "These features are: {9}.\n",
      "\n",
      "Cluster 7:\n",
      "7    0.776\n",
      "0    0.669\n",
      "2    0.616\n",
      "3    0.575\n",
      "19   0.568\n",
      "91   0.556\n",
      "95   0.555\n",
      "41   0.549\n",
      "64   0.534\n",
      "96   0.531\n",
      "dtype: float64\n",
      "\n",
      "7    0.705\n",
      "91   0.673\n",
      "76   0.655\n",
      "96   0.646\n",
      "49   0.636\n",
      "73   0.630\n",
      "29   0.628\n",
      "19   0.627\n",
      "89   0.598\n",
      "50   0.596\n",
      "dtype: float64\n",
      "\n",
      "There are 4 features that are in the top ten of both training and testing.\n",
      "These features are: {96, 91, 19, 7}.\n",
      "\n",
      "Cluster 8:\n",
      "31   0.810\n",
      "38   0.742\n",
      "55   0.676\n",
      "64   0.652\n",
      "29   0.642\n",
      "94   0.628\n",
      "30   0.614\n",
      "9    0.614\n",
      "41   0.610\n",
      "76   0.601\n",
      "dtype: float64\n",
      "\n",
      "38   0.689\n",
      "30   0.654\n",
      "64   0.629\n",
      "77   0.621\n",
      "29   0.602\n",
      "86   0.601\n",
      "32   0.598\n",
      "55   0.598\n",
      "31   0.591\n",
      "70   0.590\n",
      "dtype: float64\n",
      "\n",
      "There are 6 features that are in the top ten of both training and testing.\n",
      "These features are: {64, 38, 55, 29, 30, 31}.\n",
      "\n",
      "Cluster 9:\n",
      "0    0.640\n",
      "64   0.550\n",
      "9    0.548\n",
      "91   0.547\n",
      "11   0.536\n",
      "95   0.534\n",
      "12   0.531\n",
      "36   0.529\n",
      "94   0.525\n",
      "96   0.521\n",
      "dtype: float64\n",
      "\n",
      "0    0.668\n",
      "95   0.580\n",
      "91   0.570\n",
      "11   0.551\n",
      "34   0.543\n",
      "64   0.534\n",
      "94   0.533\n",
      "76   0.530\n",
      "89   0.524\n",
      "36   0.517\n",
      "dtype: float64\n",
      "\n",
      "There are 7 features that are in the top ten of both training and testing.\n",
      "These features are: {0, 64, 36, 11, 91, 94, 95}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = model_lsa.n_clusters\n",
    "for i in range(num_clusters):\n",
    "    data_train = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == i]\n",
    "    data_eval = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == i]\n",
    "    print(f'Cluster {i}:')\n",
    "    if i < 1:\n",
    "        train = data_train.mean().sort_values(ascending=False)[:10].copy()\n",
    "        print(train)\n",
    "        print()\n",
    "        evl = data_eval.mean().sort_values(ascending=False)[:10].copy()\n",
    "        print(evl)\n",
    "        print()\n",
    "        train_set, eval_set = {*train.index}, {*evl.index}\n",
    "        overlap = train_set.intersection(eval_set)\n",
    "        print((f'There are {len(overlap)} features that are in the top ten of both training and testing.'))\n",
    "        if len(overlap) > 0:\n",
    "            print(f'These features are: {overlap}.')\n",
    "        print()\n",
    "    elif i >= 1:\n",
    "        train = data_train.mean().sort_values(ascending=False)[1:11].copy()\n",
    "        print(train)\n",
    "        print()\n",
    "        evl = data_eval.mean().sort_values(ascending=False)[1:11].copy()\n",
    "        print(evl)\n",
    "        print()\n",
    "        train_set, eval_set = {*train.index}, {*evl.index}\n",
    "        overlap = train_set.intersection(eval_set)\n",
    "        print(f'There are {len(overlap)} features that are in the top ten of both training and testing.')\n",
    "        if len(overlap) > 0:\n",
    "            print(f'These features are: {overlap}.')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up text for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.287574Z",
     "start_time": "2019-08-12T13:40:11.228814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellow - citizens of the senate and of the hou...\n",
      "0    friend and fellow citizens : call upon to unde...\n",
      "0             fellow - citizens of the united states :\n",
      "0     be certain that  fellow americans expect that...\n",
      "0     friend , before  begin the expression of thos...\n",
      "0    vice president johnson , mr. speaker , mr. chi...\n",
      "0    senator hatfield , mr. chief justice , mr. pre...\n",
      "0    mr. chief justice , mr. president , vice presi...\n",
      "0     fellow citizen , today  celebrate the mystery...\n",
      "0     fellow citizen :  stand here today humble by ...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing numerals.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence.map(\n",
    "    lambda x: re.sub(r'\\d+', '', x))\n",
    "# Lower case.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(lambda x: x.lower())\n",
    "print(sent_df['sentence_tokens'][0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.365382Z",
     "start_time": "2019-08-12T13:40:11.292969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [fellow, citizens, of, the, senate, and, of, t...\n",
      "0    [friend, and, fellow, citizens, call, upon, to...\n",
      "0          [fellow, citizens, of, the, united, states]\n",
      "0    [be, certain, that, fellow, americans, expect,...\n",
      "0    [friend, before, begin, the, expression, of, t...\n",
      "0    [vice, president, johnson, mr, speaker, mr, ch...\n",
      "0    [senator, hatfield, mr, chief, justice, mr, pr...\n",
      "0    [mr, chief, justice, mr, president, vice, pres...\n",
      "0    [fellow, citizen, today, celebrate, the, myste...\n",
      "0    [fellow, citizen, stand, here, today, humble, ...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.889995Z",
     "start_time": "2019-08-12T13:40:11.369236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [fellow, citizen, of, the, senat, and, of, the...\n",
      "0    [friend, and, fellow, citizen, call, upon, to,...\n",
      "0              [fellow, citizen, of, the, unit, state]\n",
      "0    [be, certain, that, fellow, american, expect, ...\n",
      "0    [friend, befor, begin, the, express, of, those...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, the, mysteri,...\n",
      "0    [fellow, citizen, stand, here, today, humbl, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming.\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [snowball.stem(token) for token in x])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.970484Z",
     "start_time": "2019-08-12T13:40:11.896489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [fellow, citizen, senat, hous, repres]\n",
      "0    [friend, fellow, citizen, call, upon, undertak...\n",
      "0                       [fellow, citizen, unit, state]\n",
      "0    [certain, fellow, american, expect, induct, pr...\n",
      "0    [friend, befor, begin, express, thought, deem,...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, mysteri, amer...\n",
      "0    [fellow, citizen, stand, today, humbl, task, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stop words.\n",
    "stop_en = stopwords.words('english')\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [t for t in x if t not in stop_en])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:11.989236Z",
     "start_time": "2019-08-12T13:40:11.972671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [fellow, citizen, senat, hous, repres]\n",
      "0    [friend, fellow, citizen, call, upon, undertak...\n",
      "0                       [fellow, citizen, unit, state]\n",
      "0    [certain, fellow, american, expect, induct, pr...\n",
      "0    [friend, befor, begin, express, thought, deem,...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, mysteri, amer...\n",
      "0    [fellow, citizen, stand, today, humbl, task, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Final cleaning.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [t for t in x if len(t) > 1])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:16.037110Z",
     "start_time": "2019-08-12T13:40:12.001634Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = sent_df['sentence_tokens']\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = LdaModel(corpus,\n",
    "               id2word=dictionary,\n",
    "               num_topics=10,\n",
    "               passes=5,\n",
    "               minimum_probability=0,\n",
    "               random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:16.052443Z",
     "start_time": "2019-08-12T13:40:16.039468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"america\" + 0.012*\"nation\" + 0.011*\"state\" + 0.010*\"shall\" + 0.010*\"govern\" + 0.009*\"god\" + 0.008*\"let\" + 0.008*\"world\" + 0.007*\"renew\" + 0.007*\"must\"'),\n",
       " (1,\n",
       "  '0.010*\"great\" + 0.010*\"nation\" + 0.009*\"good\" + 0.009*\"everi\" + 0.008*\"peopl\" + 0.007*\"citizen\" + 0.007*\"mean\" + 0.006*\"may\" + 0.006*\"man\" + 0.006*\"free\"'),\n",
       " (2,\n",
       "  '0.016*\"may\" + 0.013*\"constitut\" + 0.009*\"take\" + 0.008*\"state\" + 0.008*\"law\" + 0.006*\"great\" + 0.006*\"shall\" + 0.006*\"say\" + 0.006*\"union\" + 0.006*\"first\"'),\n",
       " (3,\n",
       "  '0.011*\"american\" + 0.010*\"right\" + 0.009*\"ani\" + 0.009*\"onli\" + 0.009*\"state\" + 0.008*\"must\" + 0.008*\"govern\" + 0.007*\"generat\" + 0.007*\"peopl\" + 0.006*\"old\"'),\n",
       " (4,\n",
       "  '0.014*\"peopl\" + 0.012*\"govern\" + 0.011*\"life\" + 0.009*\"countri\" + 0.008*\"strength\" + 0.007*\"ask\" + 0.007*\"free\" + 0.007*\"make\" + 0.006*\"good\" + 0.005*\"man\"'),\n",
       " (5,\n",
       "  '0.013*\"peopl\" + 0.011*\"work\" + 0.010*\"new\" + 0.009*\"nation\" + 0.008*\"make\" + 0.008*\"great\" + 0.007*\"american\" + 0.007*\"man\" + 0.007*\"freedom\" + 0.006*\"today\"'),\n",
       " (6,\n",
       "  '0.014*\"must\" + 0.013*\"let\" + 0.010*\"time\" + 0.010*\"know\" + 0.008*\"free\" + 0.008*\"good\" + 0.008*\"faith\" + 0.007*\"chang\" + 0.006*\"one\" + 0.006*\"hope\"'),\n",
       " (7,\n",
       "  '0.019*\"nation\" + 0.012*\"ani\" + 0.010*\"act\" + 0.009*\"world\" + 0.008*\"constitut\" + 0.007*\"must\" + 0.006*\"requir\" + 0.006*\"peopl\" + 0.006*\"common\" + 0.006*\"presid\"'),\n",
       " (8,\n",
       "  '0.013*\"world\" + 0.013*\"govern\" + 0.011*\"must\" + 0.010*\"man\" + 0.009*\"one\" + 0.008*\"shall\" + 0.007*\"peopl\" + 0.007*\"faith\" + 0.007*\"america\" + 0.007*\"power\"'),\n",
       " (9,\n",
       "  '0.012*\"world\" + 0.008*\"govern\" + 0.008*\"shall\" + 0.008*\"new\" + 0.008*\"becaus\" + 0.007*\"surrend\" + 0.006*\"slave\" + 0.006*\"war\" + 0.006*\"america\" + 0.005*\"man\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:16.462168Z",
     "start_time": "2019-08-12T13:40:16.055101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Refactoring results of LDA into numpy matrix.\n",
    "hm = np.array([[y for (x,y) in lda[corpus[i]]] for i in range(len(corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:23.364426Z",
     "start_time": "2019-08-12T13:40:16.464737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality using t-SNE.\n",
    "tsne = TSNE(random_state=15, perplexity=30, early_exaggeration=120)\n",
    "embedding = tsne.fit_transform(hm)\n",
    "embedding = pd.DataFrame(embedding, columns=['x','y'])\n",
    "embedding['hue'] = hm.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:23.865885Z",
     "start_time": "2019-08-12T13:40:23.366566Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected a value of type Real, got clinton of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4eba6af9fa54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m slider = Slider(\n\u001b[1;32m     52\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPresident\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     ['Washington', 'Jefferson', 'Lincoln', 'FDR', 'Eisenhower', 'Kennedy', 'Reagan', 'GHWBush', 'Clinton', 'Obama'], step=1, title=\"Inaugural Speeches\")\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mslider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjs_on_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/models/widgets/sliders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Slider 'start' and 'end' cannot be equal.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     title = String(default=\"\", help=\"\"\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mdefault_theme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/has_props.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **properties)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/has_props.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasProps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/descriptors.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, obj, value, setter)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.%s is a readonly property\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delete__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/descriptors.py\u001b[0m in \u001b[0;36m_internal_set\u001b[0;34m(self, obj, value, hint, setter)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         '''\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mprepare_value\u001b[0;34m(self, obj_or_cls, name, value)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mprepare_value\u001b[0;34m(self, obj_or_cls, name, value)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, value, detail)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mnice_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_underlying_type\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             )\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected a value of type Real, got clinton of type str"
     ]
    }
   ],
   "source": [
    "# Scatter plot using Bokeh.\n",
    "source = ColumnDataSource(\n",
    "    data=dict(x=embedding.x,\n",
    "              y=embedding.y,\n",
    "              colors=[all_palettes['Set1'][9] for i in embedding.hue],\n",
    "              sentence=sent_df.sentence,\n",
    "              President=sent_df.President,\n",
    "              alpha=[0.9] * embedding.shape[0],\n",
    "              size=[7] * embedding.shape[0]\n",
    "              )\n",
    ")\n",
    "hover_tsne = HoverTool(names=[\"sent_df\"], tooltips=\"\"\"\n",
    "    <div style=\"margin: 10\">\n",
    "        <div style=\"margin: 0 auto; width:300px;\">\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n",
    "            <span style=\"font-size: 12px\">@title</span>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n",
    "            <span style=\"font-size: 12px\">@year</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(plot_width=700, plot_height=700,\n",
    "                   tools=tools_tsne, title='Inaugural Addresses')\n",
    "plot_tsne.circle('x', 'y', size='size', fill_color='colors',alpha='alpha',\n",
    "                 line_alpha=0, line_width=0.01, source=source, name=\"sent_df\")\n",
    "\n",
    "\n",
    "callback = CustomJS(args=dict(source=source), code=\n",
    "    \"\"\"var data = source.data;\n",
    "    var f = cb_obj.value\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    colors = data['colors']\n",
    "    alpha = data['alpha']\n",
    "    title = data['title']\n",
    "    President = data['President']\n",
    "    size = data['size']\n",
    "    for (i = 0; i < x.length; i++) {\n",
    "        if (year[i] <= f) {\n",
    "            alpha[i] = 0.9\n",
    "            size[i] = 7\n",
    "        } else {\n",
    "            alpha[i] = 0.05\n",
    "            size[i] = 4\n",
    "        }\n",
    "    }\n",
    "    source.change.emit();\n",
    "    \"\"\")\n",
    "\n",
    "slider = Slider(\n",
    "    start=sent_df.President.min(), end=sent_df.sentence.max(),values=\n",
    "    ['Washington', 'Jefferson', 'Lincoln', 'FDR', 'Eisenhower', 'Kennedy', 'Reagan', 'GHWBush', 'Clinton', 'Obama'], step=1, title=\"Inaugural Speeches\")\n",
    "slider.js_on_change('value', callback)\n",
    "\n",
    "layout = column(plot_tsne)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:09.018471Z",
     "start_time": "2019-08-12T13:41:09.007029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score to beat: 0.15491452991452967\n"
     ]
    }
   ],
   "source": [
    "'''Create baseline score to beat. GHWBush had the most sentences, so guessing \n",
    "him for all sentences would give this percentage.\n",
    "'''\n",
    "\n",
    "print('Baseline score to beat:', sum(\n",
    "    (sent_df.President == 'ghwbush') / len(sent_df.President)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:11.438582Z",
     "start_time": "2019-08-12T13:41:11.432466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline helpers.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:17.119187Z",
     "start_time": "2019-08-12T13:41:17.113716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the models.\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=15)\n",
    "tree = DecisionTreeClassifier(random_state=15)\n",
    "forest = RandomForestClassifier(max_depth=10, random_state=15)\n",
    "boost = GradientBoostingClassifier(random_state=15)\n",
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:22.382303Z",
     "start_time": "2019-08-12T13:41:22.378038Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up _kwargs files for convenience.\n",
    "tfidf_kwargs = {'X_train': X_train_tfidf,'y_train': y_train,\n",
    "                'X_eval': X_eval_tfidf,'y_eval': y_eval}\n",
    "                #'X_holdout': X_holdout_tfidf, 'y_holdout': y_holdout}\n",
    "\n",
    "lsa_kwargs = {'X_train': X_train_lsa_scaled, 'y_train': y_train,\n",
    "              'X_eval': X_eval_lsa_scaled, 'y_eval': y_eval}\n",
    "              #'X_holdout': X_holdout_tfidf_scaled, 'y_holdout': y_holdout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:28.720919Z",
     "start_time": "2019-08-12T13:41:28.715267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tune parameter grids.\n",
    "log_reg_params = {'model__C': [1, 10, 100, 1000]}\n",
    "tree_params = {'model__criterion': ['gini']}\n",
    "forest_params = {'model__n_estimators': [100, 200, 300,400],\n",
    "                 'model__max_depth': [None, 5, 10]}\n",
    "boost_params = {'model__n_estimators': [100]}\n",
    "nb_params = {'model__alpha': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:34.958206Z",
     "start_time": "2019-08-12T13:41:34.951887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to fit and predict all working kernals.\n",
    "\n",
    "\n",
    "def fit_and_predict(model, params: Dict,\n",
    "                    X_train: pd.DataFrame,\n",
    "                    y_train: pd.DataFrame,\n",
    "                    X_eval: pd.DataFrame,\n",
    "                    y_eval: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Takes an instantiated sklearn model, training data (X_train, y_train), \n",
    "    and performs cross-validation and then prints the mean of the cross-\n",
    "    validation accuracies.\n",
    "    \"\"\"\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_eval) == len(y_eval)\n",
    "    # assert len(X_holdout) == len(y_holdout)\n",
    "    pipe = Pipeline(steps=[('model', model)])\n",
    "    clf = GridSearchCV(pipe, cv=skf, param_grid=params, n_jobs=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('The mean cross_val accuracy on train is',\n",
    "          f'{clf.cv_results_[\"mean_test_score\"]}.')\n",
    "    print('The std of the cross_val accuracy is',\n",
    "          f'{clf.cv_results_[\"std_test_score\"]}.')\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print(classification_report(y_eval, y_pred))\n",
    "    print(confusion_matrix(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:41:53.257740Z",
     "start_time": "2019-08-12T13:41:41.283959Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.34410646 0.34220532 0.31749049 0.28136882].\n",
      "The std of the cross_val accuracy is [0.0365393  0.01761933 0.0400676  0.05275828].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.33      0.12      0.18        16\n",
      "  eisenhower       0.35      0.42      0.38        19\n",
      "         fdr       0.25      0.06      0.10        16\n",
      "     ghwbush       0.15      0.44      0.22        25\n",
      "   jefferson       1.00      0.10      0.18        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.58      0.62      0.60        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.24      0.33      0.28        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.28      0.28      0.28       176\n",
      "   macro avg       0.29      0.21      0.19       176\n",
      "weighted avg       0.30      0.28      0.25       176\n",
      "\n",
      "[[ 2  1  0  9  0  1  0  0  3  0]\n",
      " [ 0  8  0  7  0  0  1  2  1  0]\n",
      " [ 0  1  1  8  0  0  1  0  5  0]\n",
      " [ 0  5  2 11  0  0  2  0  5  0]\n",
      " [ 0  2  0  2  1  0  2  0  3  0]\n",
      " [ 1  1  0  4  0  0  1  1  1  0]\n",
      " [ 0  1  0  7  0  0 18  0  3  0]\n",
      " [ 1  1  1 15  0  0  3  0  4  0]\n",
      " [ 2  3  0  8  0  0  2  1  8  0]\n",
      " [ 0  0  0  2  0  0  1  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (Latent Semantic Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:42:14.291561Z",
     "start_time": "2019-08-12T13:42:00.773853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.32509506 0.31368821 0.29467681 0.28897338].\n",
      "The std of the cross_val accuracy is [0.03170107 0.04516318 0.04269594 0.05058671].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.23      0.19      0.21        16\n",
      "  eisenhower       0.33      0.47      0.39        19\n",
      "         fdr       0.14      0.06      0.09        16\n",
      "     ghwbush       0.14      0.32      0.20        25\n",
      "   jefferson       0.50      0.20      0.29        10\n",
      "     kennedy       0.50      0.11      0.18         9\n",
      "     lincoln       0.58      0.62      0.60        29\n",
      "       obama       0.12      0.04      0.06        25\n",
      "      reagan       0.32      0.33      0.33        24\n",
      "  washington       0.33      0.33      0.33         3\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       176\n",
      "   macro avg       0.32      0.27      0.27       176\n",
      "weighted avg       0.31      0.30      0.28       176\n",
      "\n",
      "[[ 3  2  1  8  0  1  0  0  1  0]\n",
      " [ 0  9  0  7  0  0  1  2  0  0]\n",
      " [ 0  4  1  5  0  0  1  1  4  0]\n",
      " [ 3  5  1  8  0  0  1  1  5  1]\n",
      " [ 0  1  0  1  2  0  4  0  2  0]\n",
      " [ 2  1  0  2  1  1  1  1  0  0]\n",
      " [ 0  1  0  7  1  0 18  0  1  1]\n",
      " [ 3  1  2 11  0  0  3  1  4  0]\n",
      " [ 2  2  2  6  0  0  2  2  8  0]\n",
      " [ 0  1  0  1  0  0  0  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:42:21.265854Z",
     "start_time": "2019-08-12T13:42:20.973065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.26996198].\n",
      "The std of the cross_val accuracy is [0.0617106].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.12      0.12      0.12        16\n",
      "  eisenhower       0.21      0.26      0.23        19\n",
      "         fdr       0.08      0.06      0.07        16\n",
      "     ghwbush       0.11      0.28      0.16        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.40      0.22      0.29         9\n",
      "     lincoln       0.55      0.41      0.47        29\n",
      "       obama       0.18      0.08      0.11        25\n",
      "      reagan       0.33      0.25      0.29        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.21      0.21      0.21       176\n",
      "   macro avg       0.20      0.17      0.17       176\n",
      "weighted avg       0.24      0.21      0.21       176\n",
      "\n",
      "[[ 2  1  0  9  0  0  1  1  2  0]\n",
      " [ 1  5  2  9  0  0  1  0  1  0]\n",
      " [ 1  3  1  7  1  0  0  1  2  0]\n",
      " [ 4  4  2  7  0  0  3  2  2  1]\n",
      " [ 0  0  0  2  0  1  3  2  2  0]\n",
      " [ 3  2  1  1  0  2  0  0  0  0]\n",
      " [ 0  3  2  8  0  1 12  0  2  1]\n",
      " [ 4  2  3 12  0  1  0  2  1  0]\n",
      " [ 1  3  1 10  0  0  0  3  6  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(tree, params=tree_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:42:28.639666Z",
     "start_time": "2019-08-12T13:42:28.205478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.2243346].\n",
      "The std of the cross_val accuracy is [0.02520304].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.14      0.06      0.09        16\n",
      "  eisenhower       0.14      0.26      0.19        19\n",
      "         fdr       0.13      0.12      0.13        16\n",
      "     ghwbush       0.20      0.32      0.25        25\n",
      "   jefferson       0.25      0.10      0.14        10\n",
      "     kennedy       0.11      0.11      0.11         9\n",
      "     lincoln       0.34      0.34      0.34        29\n",
      "       obama       0.10      0.04      0.06        25\n",
      "      reagan       0.13      0.12      0.13        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.18      0.18      0.18       176\n",
      "   macro avg       0.16      0.15      0.14       176\n",
      "weighted avg       0.18      0.18      0.17       176\n",
      "\n",
      "[[ 1  3  1  4  0  0  2  2  3  0]\n",
      " [ 0  5  0  6  2  1  1  2  2  0]\n",
      " [ 0  4  2  1  0  2  3  1  2  1]\n",
      " [ 1  6  2  8  0  1  2  2  3  0]\n",
      " [ 1  0  1  0  1  1  3  0  2  1]\n",
      " [ 0  4  2  1  0  1  1  0  0  0]\n",
      " [ 2  7  0  7  0  0 10  0  2  1]\n",
      " [ 2  1  2  8  0  2  3  1  5  1]\n",
      " [ 0  4  5  5  1  1  3  2  3  0]\n",
      " [ 0  1  0  0  0  0  1  0  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(tree, params=tree_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:42:55.860328Z",
     "start_time": "2019-08-12T13:42:35.126725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.30798479 0.31749049 0.32509506 0.32889734 0.28326996 0.29277567\n",
      " 0.28707224 0.28326996 0.28707224 0.29657795 0.30228137 0.30228137].\n",
      "The std of the cross_val accuracy is [0.0383545  0.03435724 0.03282714 0.03598003 0.05548412 0.05193235\n",
      " 0.04677472 0.04852622 0.04649525 0.04687379 0.04612755 0.04741648].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.27      0.25      0.26        16\n",
      "  eisenhower       0.21      0.32      0.25        19\n",
      "         fdr       0.12      0.06      0.08        16\n",
      "     ghwbush       0.15      0.32      0.21        25\n",
      "   jefferson       0.50      0.30      0.37        10\n",
      "     kennedy       1.00      0.11      0.20         9\n",
      "     lincoln       0.56      0.52      0.54        29\n",
      "       obama       0.07      0.04      0.05        25\n",
      "      reagan       0.27      0.25      0.26        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.26      0.26      0.26       176\n",
      "   macro avg       0.31      0.22      0.22       176\n",
      "weighted avg       0.30      0.26      0.25       176\n",
      "\n",
      "[[ 4  2  0  7  0  0  0  1  2  0]\n",
      " [ 0  6  1  6  1  0  1  3  1  0]\n",
      " [ 0  4  1  6  0  0  1  2  2  0]\n",
      " [ 3  6  3  8  0  0  3  1  1  0]\n",
      " [ 0  0  0  0  3  0  4  1  2  0]\n",
      " [ 1  1  0  2  1  1  1  0  2  0]\n",
      " [ 0  2  0  6  1  0 15  0  4  1]\n",
      " [ 5  3  3 11  0  0  0  1  2  0]\n",
      " [ 2  4  0  6  0  0  0  6  6  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(forest, params=forest_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:43:47.092460Z",
     "start_time": "2019-08-12T13:43:02.070033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.31178707 0.32889734 0.3269962  0.32319392 0.30988593 0.30798479\n",
      " 0.31939163 0.31939163 0.32129278 0.33079848 0.3365019  0.3365019 ].\n",
      "The std of the cross_val accuracy is [0.02704477 0.04949333 0.05538108 0.04361437 0.04872195 0.04950851\n",
      " 0.04421362 0.04167774 0.05076097 0.04369075 0.0503528  0.03839635].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.40      0.12      0.19        16\n",
      "  eisenhower       0.28      0.42      0.33        19\n",
      "         fdr       0.33      0.06      0.11        16\n",
      "     ghwbush       0.10      0.28      0.15        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.54      0.69      0.61        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.24      0.29      0.26        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.26      0.26      0.26       176\n",
      "   macro avg       0.19      0.19      0.16       176\n",
      "weighted avg       0.23      0.26      0.22       176\n",
      "\n",
      "[[ 2  2  0 11  0  0  0  0  1  0]\n",
      " [ 0  8  0 10  0  0  1  0  0  0]\n",
      " [ 0  5  1  5  0  0  2  1  2  0]\n",
      " [ 2  5  1  7  0  0  3  0  7  0]\n",
      " [ 0  1  0  1  0  0  5  0  3  0]\n",
      " [ 0  3  0  4  0  0  0  0  2  0]\n",
      " [ 0  1  0  6  0  0 20  0  2  0]\n",
      " [ 1  1  1 16  0  0  2  0  4  0]\n",
      " [ 0  3  0 11  0  0  2  1  7  0]\n",
      " [ 0  0  0  0  0  0  2  0  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(forest, params=forest_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:02.657714Z",
     "start_time": "2019-08-12T13:43:53.354424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.28326996].\n",
      "The std of the cross_val accuracy is [0.03678817].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.33      0.25      0.29        16\n",
      "  eisenhower       0.19      0.26      0.22        19\n",
      "         fdr       0.25      0.12      0.17        16\n",
      "     ghwbush       0.14      0.36      0.20        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.46      0.38      0.42        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.33      0.33      0.33        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.22      0.22      0.22       176\n",
      "   macro avg       0.17      0.17      0.16       176\n",
      "weighted avg       0.21      0.22      0.21       176\n",
      "\n",
      "[[ 4  3  0  6  0  0  1  0  2  0]\n",
      " [ 0  5  1  7  0  1  1  3  1  0]\n",
      " [ 0  0  2  8  0  0  1  1  4  0]\n",
      " [ 1  6  2  9  0  1  2  2  2  0]\n",
      " [ 0  2  0  2  0  0  3  1  2  0]\n",
      " [ 1  2  0  2  1  0  1  1  1  0]\n",
      " [ 1  3  0 10  1  0 11  0  2  1]\n",
      " [ 4  2  3 12  0  0  2  0  2  0]\n",
      " [ 1  2  0  8  1  1  1  2  8  0]\n",
      " [ 0  1  0  1  0  0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(boost, params=boost_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:30.001592Z",
     "start_time": "2019-08-12T13:44:09.094771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.28897338].\n",
      "The std of the cross_val accuracy is [0.02820887].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.00      0.00      0.00        16\n",
      "  eisenhower       0.21      0.26      0.23        19\n",
      "         fdr       0.22      0.12      0.16        16\n",
      "     ghwbush       0.11      0.24      0.15        25\n",
      "   jefferson       0.17      0.10      0.12        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.52      0.59      0.55        29\n",
      "       obama       0.08      0.04      0.05        25\n",
      "      reagan       0.22      0.25      0.24        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.22      0.22      0.22       176\n",
      "   macro avg       0.15      0.16      0.15       176\n",
      "weighted avg       0.19      0.22      0.20       176\n",
      "\n",
      "[[ 0  2  2  9  0  0  1  1  1  0]\n",
      " [ 1  5  0  8  0  0  1  2  2  0]\n",
      " [ 0  2  2  6  1  0  3  0  2  0]\n",
      " [ 2  4  0  6  1  0  5  0  7  0]\n",
      " [ 0  1  0  1  1  1  2  1  3  0]\n",
      " [ 0  2  0  0  1  0  1  4  1  0]\n",
      " [ 0  2  1  6  1  0 17  1  1  0]\n",
      " [ 2  3  2 12  0  0  1  1  4  0]\n",
      " [ 3  2  2  6  1  1  1  2  6  0]\n",
      " [ 0  1  0  0  0  0  1  1  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(boost, params=boost_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:36.375710Z",
     "start_time": "2019-08-12T13:44:36.229367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.36311787].\n",
      "The std of the cross_val accuracy is [0.05182334].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.00      0.00      0.00        16\n",
      "  eisenhower       0.50      0.37      0.42        19\n",
      "         fdr       0.40      0.12      0.19        16\n",
      "     ghwbush       0.21      0.64      0.31        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.63      0.59      0.61        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.22      0.42      0.29        24\n",
      "  washington       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       176\n",
      "   macro avg       0.30      0.25      0.23       176\n",
      "weighted avg       0.27      0.30      0.26       176\n",
      "\n",
      "[[ 0  1  0 11  0  0  0  0  4  0]\n",
      " [ 0  7  0  8  0  0  2  0  2  0]\n",
      " [ 0  1  2  6  0  0  0  0  7  0]\n",
      " [ 0  1  0 16  0  0  1  0  7  0]\n",
      " [ 0  1  0  2  0  1  2  1  3  0]\n",
      " [ 3  0  0  3  0  0  1  0  2  0]\n",
      " [ 0  0  1  8  0  0 17  1  2  0]\n",
      " [ 1  1  1 13  0  0  1  0  8  0]\n",
      " [ 0  2  1  9  0  0  2  0 10  0]\n",
      " [ 0  0  0  1  0  0  1  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(nb, params=nb_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:43.025696Z",
     "start_time": "2019-08-12T13:44:42.910560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.15969582].\n",
      "The std of the cross_val accuracy is [0.02462363].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.00      0.00      0.00        16\n",
      "  eisenhower       0.20      0.05      0.08        19\n",
      "         fdr       0.00      0.00      0.00        16\n",
      "     ghwbush       0.13      0.84      0.23        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.00      0.00      0.00        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.27      0.12      0.17        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.14      0.14      0.14       176\n",
      "   macro avg       0.06      0.10      0.05       176\n",
      "weighted avg       0.08      0.14      0.06       176\n",
      "\n",
      "[[ 0  0  0 15  0  0  0  0  1  0]\n",
      " [ 0  1  0 18  0  0  0  0  0  0]\n",
      " [ 0  1  0 15  0  0  0  0  0  0]\n",
      " [ 0  3  0 21  0  0  0  0  1  0]\n",
      " [ 0  0  0  7  0  0  0  0  3  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0]\n",
      " [ 0  0  0 28  0  0  0  0  1  0]\n",
      " [ 0  0  0 22  0  0  1  0  2  0]\n",
      " [ 0  0  0 20  0  0  0  1  3  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(nb, params=nb_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:20:36.652787Z",
     "start_time": "2019-08-03T20:20:36.547143Z"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Note: there are not enough data to effectively run a neural network on this project. Section 5 is merely going through the process for the sake of the capstone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:52.147710Z",
     "start_time": "2019-08-12T13:44:49.171509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish and fit the multi-level perceptron model.\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(3,), random_state=15, max_iter=5000, alpha=0.05)\n",
    "mlp.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:44:58.571935Z",
     "start_time": "2019-08-12T13:44:58.562389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7718631178707225"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find MLP score.\n",
    "mlp.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:45:19.795075Z",
     "start_time": "2019-08-12T13:45:05.404866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22727273, 0.22222222, 0.24528302, 0.2745098 , 0.24      ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find cross-validation score.\n",
    "cross_val_score(mlp, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:45:32.190530Z",
     "start_time": "2019-08-12T13:45:26.023264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp1 = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,2,), random_state=15, max_iter=5000, alpha=0.01)\n",
    "mlp1.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:45:38.148741Z",
     "start_time": "2019-08-12T13:45:38.140727Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8897338403041825"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp1.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:46:04.794274Z",
     "start_time": "2019-08-12T13:45:44.061270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20909091, 0.19444444, 0.19811321, 0.25490196, 0.15      ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp1, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:46:16.633564Z",
     "start_time": "2019-08-12T13:46:10.982749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp2 = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,2,), random_state=15, max_iter=5000, alpha=0.05)\n",
    "mlp2.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:46:23.021662Z",
     "start_time": "2019-08-12T13:46:23.012281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8878326996197718"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp2.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:47:03.037212Z",
     "start_time": "2019-08-12T13:46:29.776015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22727273, 0.19444444, 0.19811321, 0.2745098 , 0.15      ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp2, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:24.014026Z",
     "start_time": "2019-08-12T13:39:52.126Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(log_reg, params=log_reg_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:24.017831Z",
     "start_time": "2019-08-12T13:39:52.130Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(tree, params=tree_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:24.022277Z",
     "start_time": "2019-08-12T13:39:52.133Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(forest, params=forest_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:24.026498Z",
     "start_time": "2019-08-12T13:39:52.136Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(boost, params=boost_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T13:40:24.028946Z",
     "start_time": "2019-08-12T13:39:52.140Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(nb, params=nb_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 277,
   "position": {
    "height": "299px",
    "left": "892px",
    "right": "20px",
    "top": "111px",
    "width": "508px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
