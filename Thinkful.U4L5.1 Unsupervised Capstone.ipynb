{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is its performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.096030Z",
     "start_time": "2019-08-11T12:36:55.222296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up data science environment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\n",
    "from bokeh.palettes import all_palettes\n",
    "from bokeh.plotting import figure, show\n",
    "from collections import Counter\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize, Normalizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from typing import Dict\n",
    "output_notebook()\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "sns.set_style('white')\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    module='scipy',\n",
    "    message='internal gelsd'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Processing, and Language Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.107811Z",
     "start_time": "2019-08-11T12:37:00.098668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lists for files and presidents.\n",
    "files = [\"1789-Washington.txt\",\n",
    "         \"1801-Jefferson.txt\",\n",
    "         \"1861-Lincoln.txt\",\n",
    "         \"1933-Roosevelt.txt\",\n",
    "         \"1953-Eisenhower.txt\",\n",
    "         \"1961-Kennedy.txt\",\n",
    "         \"1981-Reagan.txt\",\n",
    "         \"1989-Bush.txt\",\n",
    "         \"1993-Clinton.txt\",\n",
    "         \"2009-Obama.txt\"]\n",
    "\n",
    "presidents = [\"washington\",\n",
    "              \"jefferson\",\n",
    "              \"lincoln\",\n",
    "              \"fdr\",\n",
    "              \"eisenhower\",\n",
    "              \"kennedy\",\n",
    "              \"reagan\",\n",
    "              \"ghwbush\",\n",
    "              \"clinton\",\n",
    "              \"obama\"]\n",
    "\n",
    "# Control to make sure both lists are the same length.\n",
    "assert len(files) == len(presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.128360Z",
     "start_time": "2019-08-11T12:37:00.110222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick function to open all files needed (and close them again).\n",
    "docs = []\n",
    "for file_name, president in zip(files, presidents):\n",
    "    with open(f'./inaugural/{file_name}') as f:\n",
    "        doc = f.read()\n",
    "        docs.append((doc, president))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.135403Z",
     "start_time": "2019-08-11T12:37:00.129912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text: str) -> str:\n",
    "    \"\"\"Function to strip all characters except letters in words.\"\"\"\n",
    "    \n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.148349Z",
     "start_time": "2019-08-11T12:37:00.137924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use text_cleaner on the docs, combine them into data frame (clean_docs).\n",
    "clean_docs = []\n",
    "for doc, pres in docs:\n",
    "    clean_doc = text_cleaner(doc)\n",
    "    clean_docs.append((clean_doc, pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:00.161493Z",
     "start_time": "2019-08-11T12:37:00.152669Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives: Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but over WASHINGTON\n",
      "\n",
      "Friends and Fellow Citizens: Called upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that portion of my fellow citizens which is here assembled to express my grateful thanks for the favor with which they have been pleased to look toward me, to declare a sincere consciousness that the task is above my talents, and that I approach it with those anxious and awful presentiments which the greatness of the charge and the weakness of my powers so justly inspire. A rising nation, spread over a wide and fruitful land, traversing all the seas with the rich productions of their industry, engaged in commerce with nations who feel power and forget right, advancing rapidly to destinies beyond the reach of mortal eye when I contemplate these transcendent objects, and see the honor, the happiness, and the hopes of this beloved country committed to the issue, and the auspices of this day, I shrink from the contemplation, and humble myself befo JEFFERSON\n",
      "\n",
      "Fellow-Citizens of the United States: In compliance with a custom as old as the Government itself, I appear before you to address you briefly and to take in your presence the oath prescribed by the Constitution of the United States to be taken by the President \"before he enters on the execution of this office.\" I do not consider it necessary at present for me to discuss those matters of administration about which there is no special anxiety or excitement. Apprehension seems to exist among the people of the Southern States that by the accession of a Republican Administration their property and their peace and personal security are to be endangered. There has never been any reasonable cause for such apprehension. Indeed, the most ample evidence to the contrary has all the while existed and been open to their inspection. It is found in nearly all the published speeches of him who now addresses you. I do but quote from one of those speeches when I declare that I have no purpose, directly o LINCOLN\n",
      "\n",
      "I am certain that my fellow Americans expect that on my induction into the Presidency I will address them with a candor and a decision which the present situation of our Nation impels. This is preeminently the time to speak the truth, the whole truth, frankly and boldly. Nor need we shrink from honestly facing conditions in our country today. This great Nation will endure as it has endured, will revive and will prosper. So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself nameless, unreasoning, unjustified terror which paralyzes needed efforts to convert retreat into advance. In every dark hour of our national life a leadership of frankness and vigor has met with that understanding and support of the people themselves which is essential to victory. I am convinced that you will again give that support to leadership in these critical days. In such a spirit on my part and on yours we face our common difficulties. They concern, thank God, only m FDR\n",
      "\n",
      "My friends, before I begin the expression of those thoughts that I deem appropriate to this moment, would you permit me the privilege of uttering a little private prayer of my own. And I ask that you bow your heads: Almighty God, as we stand here at this moment my future associates in the executive branch of government join me in beseeching that Thou will make full and complete our dedication to the service of the people in this throng, and their fellow citizens everywhere. Give us, we pray, the power to discern clearly right from wrong, and allow all our words and actions to be governed thereby, and by the laws of this land. Especially we pray that our concern shall be for all the people regardless of station, race, or calling. May cooperation be permitted and be the mutual aim of those who, under the concepts of our Constitution, hold to differing political faiths; so that all may work for the good of our beloved country and Thy glory. Amen. My fellow citizens: The world and we have  EISENHOWER\n",
      "\n",
      "Vice President Johnson, Mr. Speaker, Mr. Chief Justice, President Eisenhower, Vice President Nixon, President Truman, reverend clergy, fellow citizens, we observe today not a victory of party, but a celebration of freedom symbolizing an end, as well as a beginning signifying renewal, as well as change. For I have sworn I before you and Almighty God the same solemn oath our forebears l prescribed nearly a century and three quarters ago. The world is very different now. For man holds in his mortal hands the power to abolish all forms of human poverty and all forms of human life. And yet the same revolutionary beliefs for which our forebears fought are still at issue around the globe the belief that the rights of man come not from the generosity of the state, but from the hand of God. We dare not forget today that we are the heirs of that first revolution. Let the word go forth from this time and place, to friend and foe alike, that the torch has been passed to a new generation of America KENNEDY\n",
      "\n",
      "Senator Hatfield, Mr. Chief Justice, Mr. President, Vice President Bush, Vice President Mondale, Senator Baker, Speaker O'Neill, Reverend Moomaw, and my fellow citizens: To a few of us here today, this is a solemn and most momentous occasion; and yet, in the history of our Nation, it is a commonplace occurrence. The orderly transfer of authority as called for in the Constitution routinely takes place as it has for almost two centuries and few of us stop to think how unique we really are. In the eyes of many in the world, this every--year ceremony we accept as normal is nothing less than a miracle. Mr. President, I want our fellow citizens to know how much you did to carry on this tradition. By your gracious cooperation in the transition process, you have shown a watching world that we are a united people pledged to maintaining a political system which guarantees individual liberty to a greater degree than any other, and I thank you and your people for all your help in maintaining the c REAGAN\n",
      "\n",
      "Mr. Chief Justice, Mr. President, Vice President Quayle, Senator Mitchell, Speaker Wright, Senator Dole, Congressman Michael, and fellow citizens, neighbors, and friends: There is a man here who has earned a lasting place in our hearts and in our history. President Reagan, on behalf of our Nation, I thank you for the wonderful things that you have done for America. I have just repeated word for word the oath taken by George Washington years ago, and the Bible on which I placed my hand is the Bible on which he placed his. It is right that the memory of Washington be with us today, not only because this is our Bicentennial Inauguration, but because Washington remains the Father of our Country. And he would, I think, be gladdened by this day; for today is the concrete expression of a stunning fact: our continuity these years since our government began. We meet on democracy's front porch, a good place to talk as neighbors and as friends. For this is a day when our nation is made whole, whe GHWBUSH\n",
      "\n",
      "My fellow citizens, today we celebrate the mystery of American renewal. This ceremony is held in the depth of winter, but by the words we speak and the faces we show the world, we force the spring. A spring reborn in the world's oldest democracy, that brings forth the vision and courage to reinvent America. When our founders boldly declared America's independence to the world, and our purposes to the Almighty, they knew that America, to endure, would have to change. Not change for change sake, but change to preserve America's ideals: life, liberty, the pursuit of happiness. Though we march to the music of our time, our mission is timeless. Each generation of American's must define what it means to be an American. On behalf of our nation, I salute my predecessor, President Bush, for his half-century of service to America and I thank the millions of men and women whose steadfastness and sacrifice triumphed over depression, fascism and communism. Today, a generation raised in the shadows  CLINTON\n",
      "\n",
      "My fellow citizens: I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents. So it has been. So it must be with this generation of Americans. That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequen OBAMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each doc and print the first 1000 characters for inspection.\n",
    "for doc, pres in clean_docs:\n",
    "    print(doc[:1000], pres.upper()) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.708642Z",
     "start_time": "2019-08-11T12:37:00.163619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define nlp as spacy.\n",
    "nlp = spacy.load('en')\n",
    "# Create an empty list for df.\n",
    "df_list = []\n",
    "\n",
    "\n",
    "# Create a function to parse data.\n",
    "def nlp_text(text_file: str) -> doc:\n",
    "    \"\"\"Function that takes a text file and tokenizes it with spacy.\"\"\"\n",
    "    \n",
    "    \n",
    "    doc = nlp(text_file)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Create a function to lemmatize sentences.\n",
    "def sentences(doc_nlp: str, speaker: str) -> [str, str]:\n",
    "    \"\"\"Function that takes two strings, lemmatizes the first string and \n",
    "    returns a list with two strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return [[sent.lemma_, speaker] for sent in doc_nlp.sents]\n",
    "\n",
    "\n",
    "# Create a function to combine groups of sentences into one data frame.\n",
    "def sentences_to_df(sents):\n",
    "    \"\"\"Function that takes a string and returns a data frame.\"\"\"\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(sents)\n",
    "\n",
    "\n",
    "# Calling each function.\n",
    "for doc, pres in clean_docs:\n",
    "    parsed = nlp_text(doc)\n",
    "    sents = sentences(parsed, pres)\n",
    "    df = sentences_to_df(sents)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.716521Z",
     "start_time": "2019-08-11T12:37:04.710328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine each sentence data frame into one master data frame.\n",
    "sent_df = pd.concat([*df_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.739077Z",
     "start_time": "2019-08-11T12:37:04.720554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ghwbush       145\n",
       "lincoln       139\n",
       "reagan        130\n",
       "eisenhower    121\n",
       "obama         113\n",
       "fdr            86\n",
       "clinton        82\n",
       "kennedy        53\n",
       "jefferson      42\n",
       "washington     25\n",
       "Name: President, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns.\n",
    "sent_df.columns = ['sentence', 'President']\n",
    "\n",
    "# Check the count of sents per President.\n",
    "sent_df.President.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.753944Z",
     "start_time": "2019-08-11T12:37:04.744881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out pronouns from results.\n",
    "sent_df['sentence'] = sent_df['sentence'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.765912Z",
     "start_time": "2019-08-11T12:37:04.757875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data.\n",
    "X = sent_df.sentence\n",
    "y = sent_df.President\n",
    "X_train_eval, X_holdout, y_train_eval, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.776251Z",
     "start_time": "2019-08-11T12:37:04.768857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting into train/eval/holdout groups.\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train_eval, y_train_eval, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.784103Z",
     "start_time": "2019-08-11T12:37:04.779332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create base parameters dictionary.\n",
    "base_param_dict = {'strip_accents': 'unicode',\n",
    "                   'lowercase': True,\n",
    "                   'stop_words': 'english',\n",
    "                   'ngram_range': (1, 3),\n",
    "                   'max_df': 0.5,\n",
    "                   'min_df': 5,\n",
    "                   'max_features': 1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.793924Z",
     "start_time": "2019-08-11T12:37:04.788130Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer.\n",
    "bow = CountVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.892849Z",
     "start_time": "2019-08-11T12:37:04.797786Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into dfs of bags of words.\n",
    "_bow_train = bow.fit_transform(X_train)\n",
    "_bow_eval = bow.transform(X_eval)\n",
    "_bow_holdout = bow.transform(X_holdout)\n",
    "assert len(X_train) == _bow_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names = bow.get_feature_names()\n",
    "\n",
    "# Sparse matrix to data frame.\n",
    "X_train_bow = pd.DataFrame(_bow_train.toarray(), columns=feature_names)\n",
    "X_eval_bow = pd.DataFrame(_bow_eval.toarray(), columns=feature_names)\n",
    "X_holdout_bow = pd.DataFrame(_bow_holdout.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:04.903185Z",
     "start_time": "2019-08-11T12:37:04.895859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Tfidf.\n",
    "tfidf = TfidfVectorizer(**base_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:05.034061Z",
     "start_time": "2019-08-11T12:37:04.906482Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert X_train, X_test into scipy sparse matrices of tfidf values.\n",
    "_tfidf_train = tfidf.fit_transform(X_train)\n",
    "_tfidf_eval = tfidf.transform(X_eval)\n",
    "_tfidf_holdout = tfidf.transform(X_holdout)\n",
    "assert len(X_train) == _tfidf_train.shape[0]  # df and sparse-matrix\n",
    "\n",
    "# Find feature names.\n",
    "feature_names_tfidf = tfidf.get_feature_names()\n",
    "\n",
    "# Sparse matrix to data frames.\n",
    "X_train_tfidf = pd.DataFrame(\n",
    "    _tfidf_train.toarray(), columns=feature_names_tfidf)\n",
    "X_eval_tfidf = pd.DataFrame(\n",
    "    _tfidf_eval.toarray(), columns=feature_names_tfidf)\n",
    "X_holdout_tfidf = pd.DataFrame(\n",
    "    _tfidf_holdout.toarray(), columns=feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:05.077155Z",
     "start_time": "2019-08-11T12:37:05.044991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Weights:\n",
      "            word  avg_weight\n",
      "147      people       0.032\n",
      "227       world       0.028\n",
      "80   government       0.028\n",
      "131      nation       0.026\n",
      "206        time       0.025\n",
      "108         let       0.023\n",
      "79         good       0.023\n",
      "120        make       0.022\n",
      "111        life       0.022\n",
      "81        great       0.022\n",
      "\n",
      "Eval Weights:\n",
      "            word  avg_weight\n",
      "135         new       0.029\n",
      "226        work       0.029\n",
      "81        great       0.028\n",
      "74      freedom       0.027\n",
      "132    national       0.027\n",
      "194      states       0.025\n",
      "131      nation       0.024\n",
      "80   government       0.023\n",
      "147      people       0.023\n",
      "158   principle       0.023\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights on training data.\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame(\n",
    "    {'word': tfidf.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"\\nTrain Weights:\\n\", weights_df.sort_values(\n",
    "    by='avg_weight', ascending=False).head(10))\n",
    "\n",
    "# Calculate weights on eval data.\n",
    "weights = np.asarray(X_eval_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame(\n",
    "    {'word': tfidf.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"\\nEval Weights:\\n\", weights_df.sort_values(\n",
    "    by='avg_weight', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:05.091277Z",
     "start_time": "2019-08-11T12:37:05.084797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate MinMaxScaler. Create train/eval/holdout groups for Tfidf.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Tfidf\n",
    "#X_train_tfidf_scaled = pd.DataFrame(\n",
    "#    scaler.fit_transform(X_train_tfidf), columns=feature_names_tfidf)\n",
    "#X_eval_tfidf_scaled = pd.DataFrame(\n",
    "#    scaler.transform(X_eval_tfidf), columns=feature_names_tfidf)\n",
    "#X_holdout_tfidf_scaled = pd.DataFrame(\n",
    "#    scaler.transform(X_holdout_tfidf), columns=feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:05.208931Z",
     "start_time": "2019-08-11T12:37:05.096960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by components: 74.96045256340335\n",
      "Component 0:\n",
      "sentence\n",
      " government have no power except that grant  by the people .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.432\n",
      "americans deserve good , and in this city today there be people who want to do good , and so  say to all of  here , let  resolve to reform  politic , so that power and privilege no longer shout down the voice of the people .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.422\n",
      "equal and exact justice to all man , of whatev state or persuasion , religious or political ; peace , commerce , and honest friendship with all nation , entangle alliance with none ; the support of the state government in all  right , as the most competent administration for  domestic concern and the sure bulwark against antirepublican tendency ; the preservation of the general government in  whole constitutional vigor , as the sheet anchor of  peace at home and safety abroad ; a jealous care of the right of election by the people a mild and safe corrective of abuse which be lop by the sword of revolution where peaceable remedy be unprovided ; absolute acquiescence in the decision of the majority , the vital principle of republic , from which be no appeal but to force , the vital principle and immediate parent of despotism ; a well discipline militia ,  good reliance in peace and for the first moment of war , till regular may relieve  ; the supremacy of the civil over the military authority ; economy in the public expense , that labor may be lightly burthen ; the honest payment of  debt and sacred preservation of the public faith ; encouragement of agriculture , and of commerce as  handmaid ; the diffusion of information and arraignment of all abuse at the bar of the public reason ; freedom of religion ; freedom of the press , and freedom of person under the protection of the habeas corpus , and trial by jury impartially select .   0.421\n",
      "to the people of poor nation ,  pledge to work alongside  to make  farm flourish and let clean water flow ; to nourish starve body and feed hungry mind .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.420\n",
      "( ) know that only a united states that be strong and immensely productive can help defend freedom in  world ,  view  nation 's strength and security as a trust upon which rest the hope of free man everywhere .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.413\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "sentence\n",
      "now , so there will be no misunderstanding ,  be not  intention to do away with government .                                                                                                                                                        0.627\n",
      "the government will not assail  .                                                                                                                                                                                                                   0.627\n",
      "from time to time ,  have be tempt to believe that society have become too complex to be manage by self - rule , that government by an elite group be superior to government for , by , and of the people .                                         0.587\n",
      "while the strict legal right may exist in the government to enforce the exercise of these office , the attempt to do so would be so irritate and so nearly impracticable withal that  deem  better to forego for the time the us of such office .   0.566\n",
      " be time to check and reverse the growth of government which show sign of have grow beyond the consent of the govern .                                                                                                                              0.530\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "sentence\n",
      "let  embrace  .                                                                                                                                                                 0.552\n",
      "so let  mark this day with remembrance , of who  be and how far  have travel .                                                                                                  0.516\n",
      "so , as  begin , let  take inventory .                                                                                                                                          0.505\n",
      "so let  begin anew remember on both side that civility be not a sign of weakness , and sincerity be always subject to proof .                                                   0.489\n",
      "and so ,  fellow americans , as  stand at the edge of the st century , let  begin anew , with energy and hope , with faith and discipline , and let  work until  work be do .   0.466\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "sentence\n",
      "technology be almost magical , and ambition for a good life be now universal .                                                                                                        0.684\n",
      "disease diminish and life lengthen .                                                                                                                                                  0.598\n",
      " threaten to shatter the life of million of  people .                                                                                                                                 0.589\n",
      "for  ,  pack up  few worldly possession and travel across ocean in search of a new life .                                                                                             0.515\n",
      " be no coincidence that  present trouble parallel and be proportionate to the intervention and intrusion in  life that result from unnecessary and excessive growth of government .   0.487\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "sentence\n",
      " have no vision , and when there be no vision the people perish .                                                                        0.508\n",
      "the chief magistrate derive all  authority from the people , and  have refer none upon  to fix term for the separation of the states .   0.430\n",
      "the people of the united states have not fail .                                                                                          0.413\n",
      "especially  pray that  concern shall be for all the people regardless of station , race , or call .                                      0.391\n",
      "for the impoverishment of any single people in the world mean danger to the well - being of all other people .                           0.381\n",
      "Name: 4, dtype: float64\n",
      "Component 5:\n",
      "sentence\n",
      "all this will not be finish in the first day .                                                                          0.618\n",
      "in the day ahead  will propose remove the roadblock that have slow  economy and reduced productivity .                  0.618\n",
      " will not go away in day , week , or month , but  will go away .                                                        0.618\n",
      "may  guide  in the day to come .                                                                                        0.576\n",
      " would be fitting and good ,  think , if on each inauguration day in future year  should be declare a day of prayer .   0.511\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Reduce feature space to 100 features.\n",
    "svd = TruncatedSVD(100)\n",
    "\n",
    "# Make pipeline to run SVD and normalize results.\n",
    "lsa_pipe = make_pipeline(svd, Normalizer())\n",
    "\n",
    "# Fit with training data, transform test data.\n",
    "X_train_lsa = lsa_pipe.fit_transform(X_train_tfidf)\n",
    "X_eval_lsa = lsa_pipe.transform(X_eval_tfidf)\n",
    "X_holdout_lsa = lsa_pipe.transform(X_holdout_tfidf)\n",
    "\n",
    "# Examine variance captured in reduced feature space.\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('Percent variance captured by components:', total_variance*100)\n",
    "\n",
    "sent_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "\n",
    "# Look at values from first 5 components.\n",
    "for i in range(6):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sent_by_component.loc[:, i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:05.220276Z",
     "start_time": "2019-08-11T12:37:05.211396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train/eval/holdout groups for LSA.\n",
    "X_train_lsa_scaled = pd.DataFrame(scaler.fit_transform(X_train_lsa))\n",
    "X_eval_lsa_scaled = pd.DataFrame(scaler.transform(X_eval_lsa))\n",
    "X_holdout_lsa_scaled = pd.DataFrame(scaler.transform(X_holdout_lsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:09.659119Z",
     "start_time": "2019-08-11T12:37:05.222632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 2\t silhouette: 0.02403808152620107\n",
      "\n",
      "KMeans \n",
      " col_0    0   1\n",
      "row_0         \n",
      "0      487   0\n",
      "1        0  39 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=2,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 2\t silhouette: 0.020863830775012474\n",
      "\n",
      "MiniBatch \n",
      " col_0    0    1\n",
      "row_0          \n",
      "0      369  143\n",
      "1       10    4 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 3\t silhouette: 0.024610759668237767\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2\n",
      "row_0             \n",
      "0      395   0   0\n",
      "1        0  59   0\n",
      "2        0   0  72 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=3,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 3\t silhouette: 0.020515731334123103\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2\n",
      "row_0             \n",
      "0      342  18  51\n",
      "1       48   7   2\n",
      "2       16   1  41 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 4\t silhouette: 0.02862078840773946\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3\n",
      "row_0                 \n",
      "0      408   0   0   0\n",
      "1        0  38   0   0\n",
      "2        0   0  58   0\n",
      "3        0   0   0  22 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=4,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 4\t silhouette: 0.025345837634727194\n",
      "\n",
      "MiniBatch \n",
      " col_0   0    1   2  3\n",
      "row_0                \n",
      "0      20    9   1  0\n",
      "1      25  361  23  8\n",
      "2      15   14   1  0\n",
      "3       0   36   7  6 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 5\t silhouette: 0.026262398061972524\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4\n",
      "row_0                     \n",
      "0      312   0   0   0   0\n",
      "1        0  69   0   0   0\n",
      "2        0   0  50   0   0\n",
      "3        0   0   0  42   0\n",
      "4        0   0   0   0  53 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=5,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 5\t silhouette: 0.028096085431426082\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2   3   4\n",
      "row_0                     \n",
      "0      265  22   0   5  25\n",
      "1        0   1  17   0   0\n",
      "2       19   1   1   0   6\n",
      "3       15   1   0   0   3\n",
      "4      102   8   1  20  14 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 6\t silhouette: 0.029523842709622067\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5\n",
      "row_0                         \n",
      "0      305   0   0   0   0   0\n",
      "1        0  84   0   0   0   0\n",
      "2        0   0  21   0   0   0\n",
      "3        0   0   0  36   0   0\n",
      "4        0   0   0   0  27   0\n",
      "5        0   0   0   0   0  53 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=6,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 6\t silhouette: 0.029375860799137867\n",
      "\n",
      "MiniBatch \n",
      " col_0   0    1   2   3   4  5\n",
      "row_0                        \n",
      "0      34  306   0   2  17  9\n",
      "1       5   23   0  21   2  0\n",
      "2       0    8  21   0   0  0\n",
      "3       2   26   0   0   0  0\n",
      "4       2   19   0   0   0  1\n",
      "5       1   25   0   2   0  0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 7\t silhouette: 0.03467250479976267\n",
      "\n",
      "KMeans \n",
      " col_0   0   1    2   3   4   5   6\n",
      "row_0                             \n",
      "0      23   0    0   0   0   0   0\n",
      "1       0  19    0   0   0   0   0\n",
      "2       0   0  428   0   0   0   0\n",
      "3       0   0    0  11   0   0   0\n",
      "4       0   0    0   0  21   0   0\n",
      "5       0   0    0   0   0  13   0\n",
      "6       0   0    0   0   0   0  11 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=7,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 7\t silhouette: 0.030234639684473354\n",
      "\n",
      "MiniBatch \n",
      " col_0   0   1    2   3   4   5   6\n",
      "row_0                             \n",
      "0      46  27  158  20   1  37  31\n",
      "1       1   0    3   0   0   0   1\n",
      "2       1   0    2   0   0   0   8\n",
      "3       0   0    0   0   0   0  10\n",
      "4       0   0    1   1   0   2   0\n",
      "5       4   4  130   3   1   1   5\n",
      "6       0   1    0   0  25   0   2 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=8, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 8\t silhouette: 0.032527760077513825\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7\n",
      "row_0                                 \n",
      "0      234   0   0   0   0   0   0   0\n",
      "1        0  38   0   0   0   0   0   0\n",
      "2        0   0  37   0   0   0   0   0\n",
      "3        0   0   0  52   0   0   0   0\n",
      "4        0   0   0   0  38   0   0   0\n",
      "5        0   0   0   0   0  19   0   0\n",
      "6        0   0   0   0   0   0  39   0\n",
      "7        0   0   0   0   0   0   0  69 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=8,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 8\t silhouette: 0.0302228333809363\n",
      "\n",
      "MiniBatch \n",
      " col_0   0  1   2    3   4   5  6  7\n",
      "row_0                              \n",
      "0       3  5  22   20   5   3  1  0\n",
      "1       1  0   1    3   0   0  0  0\n",
      "2       3  6   5   47   0   2  2  0\n",
      "3      45  6  21  197   3  18  0  7\n",
      "4       1  1   3   27   0   1  0  0\n",
      "5       1  0   0    7   0   0  0  0\n",
      "6       4  1   1   32   0   0  0  1\n",
      "7       0  0   0    0  17   0  3  0 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=9, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n",
      "clusters: 9\t silhouette: 0.0334546650517409\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7   8\n",
      "row_0                                     \n",
      "0      228   0   0   0   0   0   0   0   0\n",
      "1        0  59   0   0   0   0   0   0   0\n",
      "2        0   0  43   0   0   0   0   0   0\n",
      "3        0   0   0  29   0   0   0   0   0\n",
      "4        0   0   0   0  14   0   0   0   0\n",
      "5        0   0   0   0   0  24   0   0   0\n",
      "6        0   0   0   0   0   0  20   0   0\n",
      "7        0   0   0   0   0   0   0  86   0\n",
      "8        0   0   0   0   0   0   0   0  23 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=9,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 9\t silhouette: 0.034711747598830014\n",
      "\n",
      "MiniBatch \n",
      " col_0    0   1   2   3  4   5   6  7   8\n",
      "row_0                                   \n",
      "0       63   3   2   2  3   0   0  2   0\n",
      "1        0  26   3   0  0   0   0  2   0\n",
      "2       10   6  12   7  0   1  19  1   0\n",
      "3       12   4   2   0  1   0   2  0   0\n",
      "4        5   0   0   0  0   0   1  0   0\n",
      "5        5   0   0   0  0   1   0  0   0\n",
      "6       30   2   0   0  1   0   0  0   0\n",
      "7       16   4   0   3  0   0   1  1  12\n",
      "8      188  16  12  22  4  15   2  1   1 \n",
      "\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=15, tol=0.0001, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters: 10\t silhouette: 0.038866084346253535\n",
      "\n",
      "KMeans \n",
      " col_0    0   1   2   3   4   5   6   7   8   9\n",
      "row_0                                         \n",
      "0      251   0   0   0   0   0   0   0   0   0\n",
      "1        0  43   0   0   0   0   0   0   0   0\n",
      "2        0   0  24   0   0   0   0   0   0   0\n",
      "3        0   0   0  29   0   0   0   0   0   0\n",
      "4        0   0   0   0  21   0   0   0   0   0\n",
      "5        0   0   0   0   0  61   0   0   0   0\n",
      "6        0   0   0   0   0   0  27   0   0   0\n",
      "7        0   0   0   0   0   0   0  32   0   0\n",
      "8        0   0   0   0   0   0   0   0  18   0\n",
      "9        0   0   0   0   0   0   0   0   0  20 \n",
      "\n",
      "MiniBatchKMeans(batch_size=500, compute_labels=True, init='random',\n",
      "        init_size=None, max_iter=100, max_no_improvement=10, n_clusters=10,\n",
      "        n_init=3, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=0)\n",
      "clusters: 10\t silhouette: 0.02822580157981936\n",
      "\n",
      "MiniBatch \n",
      " col_0    0  1  2   3   4   5  6   7   8  9\n",
      "row_0                                     \n",
      "0        9  0  0  12   0   4  0  10   1  1\n",
      "1       12  0  0   1   0   0  0   1   0  1\n",
      "2        8  0  0   0   2   3  0   8   0  0\n",
      "3       15  0  0   1   0   0  2   3   0  0\n",
      "4        4  0  0   2   2   2  0  27   0  0\n",
      "5       25  0  0   1   4   1  1  14   3  1\n",
      "6      170  3  6  17  10  12  0  40  10  6\n",
      "7       21  0  1   0   3   3  6  14   1  1\n",
      "8       12  0  0   0   0   1  0   1   1  1\n",
      "9        4  0  0   0   0   0  0   1   0  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clustering models\n",
    "models = []\n",
    "names = []\n",
    "plot_nums = []\n",
    "silhouettes = []\n",
    "clust = []\n",
    "\n",
    "for clusters in range(2, 11):\n",
    "    models.append(\n",
    "        (0, 'KMeans', KMeans(n_clusters=clusters,\n",
    "                             init='k-means++', random_state=15)))\n",
    "    models.append(\n",
    "        (1, 'MiniBatch', MiniBatchKMeans(init='random',\n",
    "                                         n_clusters=clusters,\n",
    "                                         batch_size=500)))\n",
    "    #models.append((2, 'Spectral', SpectralClustering(n_clusters=clusters)))\n",
    "\n",
    "for plot_num, name, model in models:\n",
    "    names.append(name)\n",
    "    model.fit(X_train_tfidf)\n",
    "    labels = model.labels_\n",
    "    print(model)\n",
    "    if len(set(labels)) > 1:\n",
    "        ypred = model.fit_predict(X_train_tfidf)\n",
    "        silhouette = metrics.silhouette_score(\n",
    "            X_train_tfidf, labels, metric='euclidean')\n",
    "        silhouettes.append(silhouette)\n",
    "        #ax[plot_num].set_title(name)\n",
    "        #plotting(plot_num, labels, ypred)\n",
    "        if silhouette > 0:\n",
    "            print('clusters: {}\\t silhouette: {}\\n'.format(\n",
    "                model.n_clusters, silhouette))\n",
    "            print(name, '\\n', pd.crosstab(ypred, labels), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.017645Z",
     "start_time": "2019-08-11T12:37:09.674488Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Re-run KMeans and extract cluster information.\n",
    "model_tfidf = KMeans(n_clusters=10, random_state=15).fit(X_train_tfidf)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = model_tfidf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.036994Z",
     "start_time": "2019-08-11T12:37:10.020918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster assignment for eval, holdout groups.\n",
    "X_eval_tfidf_labels = model_tfidf.predict(X_eval_tfidf)\n",
    "X_holdout_tfidf_labels = model_tfidf.predict(X_holdout_tfidf)\n",
    "\n",
    "# Create a column for cluster labels.\n",
    "X_eval_tfidf['clusters'] = X_eval_tfidf_labels\n",
    "X_holdout_tfidf['clusters'] = X_holdout_tfidf_labels\n",
    "\n",
    "X_train_tfidf['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.105185Z",
     "start_time": "2019-08-11T12:37:10.048525Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>administration</th>\n",
       "      <th>america</th>\n",
       "      <th>american</th>\n",
       "      <th>american people</th>\n",
       "      <th>americans</th>\n",
       "      <th>ask</th>\n",
       "      <th>authority</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>willing</th>\n",
       "      <th>wish</th>\n",
       "      <th>woman</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters   act  action  administration  america  american  american people  \\\n",
       "0         0 0.015   0.005           0.003    0.019     0.002            0.000   \n",
       "1         1 0.000   0.000           0.000    0.007     0.007            0.000   \n",
       "2         2 0.000   0.000           0.000    0.017     0.000            0.000   \n",
       "3         3 0.000   0.000           0.000    0.016     0.021            0.000   \n",
       "4         4 0.000   0.009           0.018    0.000     0.023            0.000   \n",
       "5         5 0.005   0.009           0.008    0.029     0.021            0.023   \n",
       "6         6 0.011   0.036           0.000    0.015     0.013            0.014   \n",
       "7         7 0.000   0.016           0.023    0.000     0.007            0.008   \n",
       "8         8 0.027   0.000           0.000    0.021     0.000            0.000   \n",
       "9         9 0.019   0.000           0.000    0.037     0.000            0.000   \n",
       "\n",
       "   americans   ask  authority  ...   way  willing  wish  woman  word  work  \\\n",
       "0      0.011 0.010      0.011  ... 0.003    0.000 0.004  0.006 0.001 0.000   \n",
       "1      0.014 0.012      0.000  ... 0.000    0.013 0.007  0.008 0.009 0.006   \n",
       "2      0.012 0.000      0.000  ... 0.030    0.030 0.000  0.000 0.010 0.000   \n",
       "3      0.019 0.000      0.000  ... 0.037    0.018 0.000  0.000 0.000 0.000   \n",
       "4      0.043 0.000      0.000  ... 0.000    0.000 0.000  0.000 0.000 0.000   \n",
       "5      0.000 0.006      0.016  ... 0.008    0.000 0.008  0.015 0.005 0.000   \n",
       "6      0.027 0.016      0.000  ... 0.000    0.000 0.000  0.000 0.160 0.000   \n",
       "7      0.000 0.037      0.004  ... 0.022    0.000 0.010  0.000 0.000 0.000   \n",
       "8      0.036 0.017      0.000  ... 0.000    0.019 0.000  0.000 0.015 0.505   \n",
       "9      0.000 0.000      0.000  ... 0.000    0.000 0.000  0.000 0.019 0.000   \n",
       "\n",
       "   world  write  year  young  \n",
       "0  0.001  0.006 0.003  0.007  \n",
       "1  0.025  0.000 0.000  0.017  \n",
       "2  0.017  0.000 0.015  0.000  \n",
       "3  0.015  0.000 0.000  0.000  \n",
       "4  0.010  0.000 0.000  0.000  \n",
       "5  0.195  0.000 0.006  0.000  \n",
       "6  0.025  0.053 0.030  0.012  \n",
       "7  0.000  0.000 0.041  0.000  \n",
       "8  0.000  0.018 0.000  0.000  \n",
       "9  0.000  0.000 0.000  0.000  \n",
       "\n",
       "[10 rows x 232 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by cluster.\n",
    "X_train_tfidf_clusters = X_train_tfidf.groupby(\n",
    "    ['clusters'], as_index=False).mean()\n",
    "X_train_tfidf_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.128104Z",
     "start_time": "2019-08-11T12:37:10.109881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day            0.028\n",
       "freedom        0.022\n",
       "god            0.021\n",
       "right          0.020\n",
       "union          0.019\n",
       "constitution   0.019\n",
       "america        0.019\n",
       "make           0.019\n",
       "man            0.018\n",
       "old            0.017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the ten most common terms in each cluster.\n",
    "cluster0_train_tfidf = X_train_tfidf[X_train_tfidf['clusters'] == 0]\n",
    "cluster0_train_tfidf.mean().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.154861Z",
     "start_time": "2019-08-11T12:37:10.134849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "principle    0.031\n",
       "law          0.029\n",
       "believe      0.027\n",
       "faith        0.026\n",
       "states       0.024\n",
       "way          0.023\n",
       "new          0.023\n",
       "generation   0.021\n",
       "union        0.021\n",
       "case         0.021\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster0_eval_tfidf = X_eval_tfidf[X_eval_tfidf['clusters'] == 0]\n",
    "cluster0_eval_tfidf.mean().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.182559Z",
     "start_time": "2019-08-11T12:37:10.165364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time         0.264\n",
       "nation       0.228\n",
       "change       0.047\n",
       "great        0.040\n",
       "government   0.035\n",
       "live         0.032\n",
       "make         0.032\n",
       "require      0.031\n",
       "god          0.029\n",
       "believe      0.029\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1_train_tfidf = X_train_tfidf[X_train_tfidf['clusters'] == 1]\n",
    "cluster1_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.208570Z",
     "start_time": "2019-08-11T12:37:10.187266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nation    0.289\n",
       "time      0.209\n",
       "freedom   0.134\n",
       "great     0.101\n",
       "new       0.095\n",
       "right     0.094\n",
       "think     0.075\n",
       "speak     0.073\n",
       "power     0.062\n",
       "future    0.060\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1_eval_tfidf = X_eval_tfidf[X_eval_tfidf['clusters'] == 1]\n",
    "cluster1_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.226697Z",
     "start_time": "2019-08-11T12:37:10.211428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good         0.444\n",
       "make         0.088\n",
       "power        0.070\n",
       "government   0.061\n",
       "hope         0.060\n",
       "life         0.043\n",
       "discipline   0.043\n",
       "equal        0.040\n",
       "public       0.036\n",
       "peace        0.036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster2_train_tfidf = X_train_tfidf[X_train_tfidf['clusters'] == 2]\n",
    "cluster2_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.246343Z",
     "start_time": "2019-08-11T12:37:10.229451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good       0.357\n",
       "land       0.191\n",
       "national   0.175\n",
       "action     0.141\n",
       "place      0.138\n",
       "hold       0.131\n",
       "friend     0.128\n",
       "meet       0.126\n",
       "right      0.116\n",
       "use        0.101\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster2_eval_tfidf = X_eval_tfidf[X_eval_tfidf['clusters'] == 2]\n",
    "cluster2_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.263189Z",
     "start_time": "2019-08-11T12:37:10.249376Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "life      0.323\n",
       "hand      0.219\n",
       "heart     0.097\n",
       "new       0.067\n",
       "present   0.057\n",
       "faith     0.052\n",
       "know      0.043\n",
       "great     0.041\n",
       "way       0.037\n",
       "make      0.030\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster3_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 3]\n",
    "cluster3_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.280582Z",
     "start_time": "2019-08-11T12:37:10.266593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "life             0.279\n",
       "hand             0.260\n",
       "national         0.191\n",
       "value            0.134\n",
       "form             0.105\n",
       "administration   0.096\n",
       "leader           0.088\n",
       "majority         0.086\n",
       "heart            0.080\n",
       "day              0.075\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster3_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 3]\n",
    "cluster3_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.302830Z",
     "start_time": "2019-08-11T12:37:10.285467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citizen          0.306\n",
       "fellow           0.276\n",
       "fellow citizen   0.175\n",
       "president        0.095\n",
       "country          0.064\n",
       "man              0.054\n",
       "great            0.049\n",
       "free             0.048\n",
       "duty             0.044\n",
       "americans        0.043\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster4_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 4]\n",
    "cluster4_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.319994Z",
     "start_time": "2019-08-11T12:37:10.307029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citizen          0.458\n",
       "ask              0.247\n",
       "fellow citizen   0.203\n",
       "fellow           0.181\n",
       "america          0.154\n",
       "today            0.146\n",
       "hope             0.143\n",
       "world            0.125\n",
       "mind             0.120\n",
       "heart            0.111\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster4_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 4]\n",
    "cluster4_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.341256Z",
     "start_time": "2019-08-11T12:37:10.322512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world           0.195\n",
       "people          0.194\n",
       "free            0.054\n",
       "states          0.049\n",
       "united          0.047\n",
       "great           0.039\n",
       "shall           0.037\n",
       "united states   0.036\n",
       "economic        0.035\n",
       "peace           0.030\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster5_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 5]\n",
    "cluster5_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.365558Z",
     "start_time": "2019-08-11T12:37:10.344125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people            0.177\n",
       "world             0.175\n",
       "peace             0.097\n",
       "america           0.095\n",
       "great             0.086\n",
       "american people   0.075\n",
       "purpose           0.069\n",
       "american          0.068\n",
       "year              0.058\n",
       "americans         0.058\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster5_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 5]\n",
    "cluster5_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.383626Z",
     "start_time": "2019-08-11T12:37:10.368173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "need           0.257\n",
       "word           0.160\n",
       "deny           0.098\n",
       "write          0.053\n",
       "constitution   0.043\n",
       "speak          0.040\n",
       "factory        0.040\n",
       "great          0.039\n",
       "material       0.038\n",
       "moment         0.037\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster6_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 6]\n",
    "cluster6_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.403638Z",
     "start_time": "2019-08-11T12:37:10.387994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "need        0.501\n",
       "action      0.152\n",
       "serve       0.126\n",
       "care        0.119\n",
       "congress    0.109\n",
       "executive   0.107\n",
       "face        0.106\n",
       "country     0.100\n",
       "authority   0.097\n",
       "today       0.097\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster6_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 6]\n",
    "cluster6_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.419720Z",
     "start_time": "2019-08-11T12:37:10.408191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government   0.310\n",
       "support      0.135\n",
       "shall        0.098\n",
       "people       0.065\n",
       "federal      0.057\n",
       "year         0.041\n",
       "right        0.041\n",
       "power        0.039\n",
       "law          0.039\n",
       "states       0.038\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster7_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 7]\n",
    "cluster7_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.434214Z",
     "start_time": "2019-08-11T12:37:10.422645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government      0.356\n",
       "states          0.089\n",
       "opportunity     0.076\n",
       "continue        0.076\n",
       "support         0.069\n",
       "united states   0.067\n",
       "let             0.063\n",
       "united          0.061\n",
       "freedom         0.059\n",
       "union           0.059\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster7_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 7]\n",
    "cluster7_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.449079Z",
     "start_time": "2019-08-11T12:37:10.436749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work        0.505\n",
       "sacrifice   0.063\n",
       "let         0.056\n",
       "good        0.051\n",
       "return      0.049\n",
       "god         0.046\n",
       "thing       0.046\n",
       "moral       0.043\n",
       "seek        0.042\n",
       "right       0.039\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster8_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 8]\n",
    "cluster8_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.463472Z",
     "start_time": "2019-08-11T12:37:10.451501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work        0.638\n",
       "know        0.093\n",
       "new         0.089\n",
       "people      0.080\n",
       "task        0.080\n",
       "old         0.075\n",
       "friend      0.074\n",
       "man woman   0.062\n",
       "woman       0.061\n",
       "stand       0.060\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster8_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 8]\n",
    "cluster8_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.477351Z",
     "start_time": "2019-08-11T12:37:10.466454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let          0.506\n",
       "begin        0.133\n",
       "fear         0.081\n",
       "renew        0.058\n",
       "remember     0.056\n",
       "hope         0.047\n",
       "hard         0.041\n",
       "understand   0.040\n",
       "america      0.037\n",
       "far          0.034\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster9_train_tfidf = X_train_tfidf[\n",
    "    X_train_tfidf['clusters'] == 9]\n",
    "cluster9_train_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.493154Z",
     "start_time": "2019-08-11T12:37:10.479821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let              0.554\n",
       "seek             0.197\n",
       "people           0.159\n",
       "child            0.140\n",
       "responsibility   0.138\n",
       "community        0.135\n",
       "country          0.120\n",
       "turn             0.070\n",
       "end              0.068\n",
       "generation       0.067\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster9_eval_tfidf = X_eval_tfidf[\n",
    "    X_eval_tfidf['clusters'] == 9]\n",
    "cluster9_eval_tfidf.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.630557Z",
     "start_time": "2019-08-11T12:37:10.495519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-run KMeans and extract cluster information.\n",
    "model_lsa = KMeans(n_clusters=10, random_state=42).fit(X_train_lsa_scaled)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = model_lsa.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.644437Z",
     "start_time": "2019-08-11T12:37:10.632357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster assignment for eval, holdout groups.\n",
    "X_eval_lsa_labels = model_lsa.predict(X_eval_lsa_scaled)\n",
    "X_holdout_lsa_labels = model_lsa.predict(X_holdout_lsa_scaled)\n",
    "\n",
    "# Create a column for cluster labels.\n",
    "X_eval_lsa_scaled['clusters'] = X_eval_lsa_labels\n",
    "X_holdout_lsa_scaled['clusters'] = X_holdout_lsa_labels\n",
    "\n",
    "X_train_lsa_scaled['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.676111Z",
     "start_time": "2019-08-11T12:37:10.647069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clusters     0     1     2     3     4     5     6     7     8  ...    90  \\\n",
       "0         0 0.253 0.363 0.510 0.355 0.450 0.421 0.442 0.488 0.485  ... 0.425   \n",
       "1         1 0.592 0.360 0.426 0.333 0.484 0.291 0.751 0.573 0.407  ... 0.418   \n",
       "2         2 0.249 0.470 0.515 0.375 0.464 0.511 0.502 0.447 0.363  ... 0.372   \n",
       "3         3 0.132 0.371 0.499 0.393 0.461 0.414 0.419 0.571 0.369  ... 0.407   \n",
       "4         4 0.639 0.430 0.399 0.351 0.480 0.358 0.417 0.458 0.331  ... 0.407   \n",
       "5         5 0.400 0.404 0.546 0.353 0.501 0.322 0.485 0.600 0.310  ... 0.461   \n",
       "6         6 0.267 0.419 0.426 0.409 0.586 0.375 0.456 0.499 0.412  ... 0.344   \n",
       "7         7 0.663 0.345 0.614 0.574 0.380 0.453 0.196 0.777 0.178  ... 0.423   \n",
       "8         8 0.410 0.349 0.434 0.284 0.469 0.532 0.398 0.487 0.505  ... 0.431   \n",
       "9         9 0.144 0.320 0.454 0.485 0.347 0.355 0.438 0.420 0.551  ... 0.461   \n",
       "\n",
       "     91    92    93    94    95    96    97    98    99  \n",
       "0 0.442 0.437 0.629 0.495 0.480 0.510 0.390 0.429 0.390  \n",
       "1 0.431 0.435 0.637 0.492 0.478 0.517 0.396 0.424 0.368  \n",
       "2 0.312 0.499 0.593 0.452 0.397 0.617 0.507 0.608 0.316  \n",
       "3 0.466 0.497 0.593 0.408 0.412 0.548 0.389 0.415 0.447  \n",
       "4 0.426 0.445 0.626 0.512 0.479 0.501 0.393 0.429 0.386  \n",
       "5 0.460 0.409 0.667 0.524 0.484 0.488 0.366 0.320 0.418  \n",
       "6 0.353 0.655 0.710 0.456 0.507 0.615 0.316 0.459 0.476  \n",
       "7 0.433 0.441 0.628 0.477 0.471 0.497 0.387 0.417 0.375  \n",
       "8 0.413 0.412 0.621 0.485 0.462 0.504 0.398 0.418 0.390  \n",
       "9 0.430 0.475 0.619 0.522 0.408 0.454 0.355 0.426 0.347  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by cluster.\n",
    "X_train_lsa_clusters = X_train_lsa_scaled.groupby(\n",
    "    ['clusters'], as_index=False).mean()\n",
    "X_train_lsa_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.691311Z",
     "start_time": "2019-08-11T12:37:10.680115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93   0.629\n",
       "89   0.541\n",
       "49   0.523\n",
       "75   0.514\n",
       "12   0.514\n",
       "9    0.512\n",
       "2    0.510\n",
       "96   0.510\n",
       "58   0.507\n",
       "72   0.507\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the ten most common terms in each cluster.\n",
    "cluster0_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 0]\n",
    "cluster0_train_lsa.mean().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.705294Z",
     "start_time": "2019-08-11T12:37:10.693780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93   0.623\n",
       "89   0.554\n",
       "75   0.529\n",
       "34   0.524\n",
       "96   0.514\n",
       "87   0.514\n",
       "11   0.513\n",
       "44   0.511\n",
       "50   0.511\n",
       "60   0.509\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster0_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 0]\n",
    "cluster0_eval_lsa.mean().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that cluster0 is a cluster that deals primarily with **people** of one sort or another. Common words in the top ten for each include \"man\", \"woman\", \"man woman\", and \"say\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.718664Z",
     "start_time": "2019-08-11T12:37:10.707679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.751\n",
       "93   0.637\n",
       "14   0.618\n",
       "0    0.592\n",
       "41   0.577\n",
       "34   0.576\n",
       "7    0.573\n",
       "10   0.557\n",
       "19   0.548\n",
       "49   0.527\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 1]\n",
    "cluster1_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.733481Z",
     "start_time": "2019-08-11T12:37:10.720974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.721\n",
       "14   0.672\n",
       "55   0.634\n",
       "34   0.633\n",
       "93   0.620\n",
       "0    0.618\n",
       "94   0.588\n",
       "41   0.572\n",
       "78   0.571\n",
       "49   0.571\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster1_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 1]\n",
    "cluster1_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster1 has only one word in common for the top ten of both groups: \"today\". However, it is apparent that this cluster has a lot to do with **ideals**, with words like \"good\"/\"great\", \"world\", \"peace\", \"principle\", \"nation\", \"state\", and \"constitution\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.747533Z",
     "start_time": "2019-08-11T12:37:10.735977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61   0.798\n",
       "32   0.782\n",
       "63   0.776\n",
       "66   0.753\n",
       "64   0.746\n",
       "87   0.706\n",
       "41   0.677\n",
       "34   0.670\n",
       "44   0.662\n",
       "89   0.649\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster2_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 2]\n",
    "cluster2_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.770203Z",
     "start_time": "2019-08-11T12:37:10.759946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87   0.846\n",
       "17   0.838\n",
       "63   0.831\n",
       "66   0.779\n",
       "61   0.747\n",
       "64   0.705\n",
       "41   0.699\n",
       "69   0.685\n",
       "96   0.671\n",
       "34   0.661\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster2_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 2]\n",
    "cluster2_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster2 has a less obvious theme that I would like to call **values**. Two common words are \"hand\" and \"material\", but other important words include \"strong\", \"life\", \"power\", \"liberty\", \"trust\", and \"value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.787838Z",
     "start_time": "2019-08-11T12:37:10.776228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44   0.972\n",
       "48   0.918\n",
       "33   0.740\n",
       "78   0.681\n",
       "89   0.634\n",
       "31   0.610\n",
       "47   0.599\n",
       "79   0.595\n",
       "64   0.594\n",
       "93   0.593\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster3_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 3]\n",
    "cluster3_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.803516Z",
     "start_time": "2019-08-11T12:37:10.790195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    nan\n",
       "2    nan\n",
       "3    nan\n",
       "4    nan\n",
       "5    nan\n",
       "6    nan\n",
       "7    nan\n",
       "8    nan\n",
       "9    nan\n",
       "10   nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster3_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 3]\n",
    "cluster3_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.817314Z",
     "start_time": "2019-08-11T12:37:10.805915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.639\n",
       "93   0.626\n",
       "11   0.539\n",
       "36   0.537\n",
       "89   0.533\n",
       "12   0.533\n",
       "9    0.519\n",
       "94   0.512\n",
       "64   0.510\n",
       "49   0.510\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster4_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 4]\n",
    "cluster4_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.831555Z",
     "start_time": "2019-08-11T12:37:10.819555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.668\n",
       "93   0.610\n",
       "89   0.577\n",
       "11   0.570\n",
       "34   0.544\n",
       "67   0.542\n",
       "96   0.533\n",
       "69   0.530\n",
       "72   0.528\n",
       "49   0.519\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster4_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 4]\n",
    "cluster4_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.845340Z",
     "start_time": "2019-08-11T12:37:10.833732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32   0.830\n",
       "35   0.760\n",
       "30   0.727\n",
       "34   0.724\n",
       "38   0.673\n",
       "93   0.667\n",
       "9    0.648\n",
       "44   0.621\n",
       "11   0.614\n",
       "7    0.600\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster5_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 5]\n",
    "cluster5_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.859494Z",
     "start_time": "2019-08-11T12:37:10.847561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32   0.880\n",
       "35   0.825\n",
       "30   0.772\n",
       "34   0.770\n",
       "38   0.759\n",
       "75   0.723\n",
       "93   0.687\n",
       "83   0.644\n",
       "44   0.642\n",
       "12   0.628\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster5_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 5]\n",
    "cluster5_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.873576Z",
     "start_time": "2019-08-11T12:37:10.861728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37   0.824\n",
       "72   0.787\n",
       "57   0.767\n",
       "93   0.710\n",
       "75   0.706\n",
       "63   0.682\n",
       "92   0.655\n",
       "51   0.643\n",
       "69   0.638\n",
       "9    0.619\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster6_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 6]\n",
    "cluster6_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.888875Z",
     "start_time": "2019-08-11T12:37:10.875767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    nan\n",
       "2    nan\n",
       "3    nan\n",
       "4    nan\n",
       "5    nan\n",
       "6    nan\n",
       "7    nan\n",
       "8    nan\n",
       "9    nan\n",
       "10   nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster6_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 6]\n",
    "cluster6_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.903148Z",
     "start_time": "2019-08-11T12:37:10.891102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.777\n",
       "0    0.663\n",
       "93   0.628\n",
       "2    0.614\n",
       "3    0.574\n",
       "19   0.568\n",
       "41   0.546\n",
       "87   0.527\n",
       "33   0.526\n",
       "34   0.523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster7_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 7]\n",
    "cluster7_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.918173Z",
     "start_time": "2019-08-11T12:37:10.905447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.708\n",
       "76   0.636\n",
       "29   0.632\n",
       "19   0.628\n",
       "49   0.625\n",
       "63   0.618\n",
       "50   0.592\n",
       "73   0.586\n",
       "0    0.585\n",
       "96   0.577\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster7_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 7]\n",
    "cluster7_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.931636Z",
     "start_time": "2019-08-11T12:37:10.920734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17   0.770\n",
       "18   0.653\n",
       "93   0.621\n",
       "36   0.613\n",
       "9    0.606\n",
       "33   0.566\n",
       "41   0.551\n",
       "22   0.546\n",
       "89   0.533\n",
       "5    0.532\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster8_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 8]\n",
    "cluster8_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.947953Z",
     "start_time": "2019-08-11T12:37:10.934090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17   0.847\n",
       "93   0.635\n",
       "36   0.634\n",
       "33   0.607\n",
       "34   0.601\n",
       "9    0.582\n",
       "18   0.559\n",
       "94   0.550\n",
       "7    0.547\n",
       "14   0.545\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster8_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 8]\n",
    "cluster8_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.962385Z",
     "start_time": "2019-08-11T12:37:10.950420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33   0.934\n",
       "34   0.901\n",
       "44   0.880\n",
       "28   0.857\n",
       "38   0.747\n",
       "20   0.733\n",
       "26   0.684\n",
       "49   0.655\n",
       "9    0.652\n",
       "24   0.651\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster9_train_lsa = X_train_lsa_scaled[X_train_lsa_scaled['clusters'] == 9]\n",
    "cluster9_train_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:10.977905Z",
     "start_time": "2019-08-11T12:37:10.966494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    nan\n",
       "2    nan\n",
       "3    nan\n",
       "4    nan\n",
       "5    nan\n",
       "6    nan\n",
       "7    nan\n",
       "8    nan\n",
       "9    nan\n",
       "10   nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster9_eval_lsa = X_eval_lsa_scaled[X_eval_lsa_scaled['clusters'] == 9]\n",
    "cluster9_eval_lsa.mean().sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, cluster3 is what I will call **now**. Common words include \"time\" and \"government\", and other relevant words include \"change\", \"history\", \"future\", \"courage\", \"generation\", and \"god\" (lemmatized, so we'll say \"God\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up text for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:11.000636Z",
     "start_time": "2019-08-11T12:37:10.982427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellow - citizens of the senate and of the hou...\n",
      "0    friend and fellow citizens : call upon to unde...\n",
      "0             fellow - citizens of the united states :\n",
      "0     be certain that  fellow americans expect that...\n",
      "0     friend , before  begin the expression of thos...\n",
      "0    vice president johnson , mr. speaker , mr. chi...\n",
      "0    senator hatfield , mr. chief justice , mr. pre...\n",
      "0    mr. chief justice , mr. president , vice presi...\n",
      "0     fellow citizen , today  celebrate the mystery...\n",
      "0     fellow citizen :  stand here today humble by ...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing numerals.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence.map(\n",
    "    lambda x: re.sub(r'\\d+', '', x))\n",
    "# Lower case.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(lambda x: x.lower())\n",
    "print(sent_df['sentence_tokens'][0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:11.031769Z",
     "start_time": "2019-08-11T12:37:11.003745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [fellow, citizens, of, the, senate, and, of, t...\n",
      "0    [friend, and, fellow, citizens, call, upon, to...\n",
      "0          [fellow, citizens, of, the, united, states]\n",
      "0    [be, certain, that, fellow, americans, expect,...\n",
      "0    [friend, before, begin, the, expression, of, t...\n",
      "0    [vice, president, johnson, mr, speaker, mr, ch...\n",
      "0    [senator, hatfield, mr, chief, justice, mr, pr...\n",
      "0    [mr, chief, justice, mr, president, vice, pres...\n",
      "0    [fellow, citizen, today, celebrate, the, myste...\n",
      "0    [fellow, citizen, stand, here, today, humble, ...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:11.377604Z",
     "start_time": "2019-08-11T12:37:11.034128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [fellow, citizen, of, the, senat, and, of, the...\n",
      "0    [friend, and, fellow, citizen, call, upon, to,...\n",
      "0              [fellow, citizen, of, the, unit, state]\n",
      "0    [be, certain, that, fellow, american, expect, ...\n",
      "0    [friend, befor, begin, the, express, of, those...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, the, mysteri,...\n",
      "0    [fellow, citizen, stand, here, today, humbl, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming.\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [snowball.stem(token) for token in x])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:11.437492Z",
     "start_time": "2019-08-11T12:37:11.379900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [fellow, citizen, senat, hous, repres]\n",
      "0    [friend, fellow, citizen, call, upon, undertak...\n",
      "0                       [fellow, citizen, unit, state]\n",
      "0    [certain, fellow, american, expect, induct, pr...\n",
      "0    [friend, befor, begin, express, thought, deem,...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, mysteri, amer...\n",
      "0    [fellow, citizen, stand, today, humbl, task, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stop words.\n",
    "stop_en = stopwords.words('english')\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [t for t in x if t not in stop_en])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:11.454423Z",
     "start_time": "2019-08-11T12:37:11.439656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [fellow, citizen, senat, hous, repres]\n",
      "0    [friend, fellow, citizen, call, upon, undertak...\n",
      "0                       [fellow, citizen, unit, state]\n",
      "0    [certain, fellow, american, expect, induct, pr...\n",
      "0    [friend, befor, begin, express, thought, deem,...\n",
      "0    [vice, presid, johnson, mr, speaker, mr, chief...\n",
      "0    [senat, hatfield, mr, chief, justic, mr, presi...\n",
      "0    [mr, chief, justic, mr, presid, vice, presid, ...\n",
      "0    [fellow, citizen, today, celebr, mysteri, amer...\n",
      "0    [fellow, citizen, stand, today, humbl, task, b...\n",
      "Name: sentence_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Final cleaning.\n",
    "sent_df['sentence_tokens'] = sent_df.sentence_tokens.map(\n",
    "    lambda x: [t for t in x if len(t) > 1])\n",
    "print(sent_df['sentence_tokens'][0][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:15.601882Z",
     "start_time": "2019-08-11T12:37:11.456585Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = sent_df['sentence_tokens'].values\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = LdaModel(corpus,\n",
    "               id2word=dictionary,\n",
    "               num_topics=10,\n",
    "               passes=5,\n",
    "               minimum_probability=0,\n",
    "               random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:15.630296Z",
     "start_time": "2019-08-11T12:37:15.610034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"america\" + 0.012*\"nation\" + 0.011*\"state\" + 0.010*\"shall\" + 0.010*\"govern\" + 0.009*\"god\" + 0.008*\"let\" + 0.008*\"world\" + 0.007*\"renew\" + 0.007*\"must\"'),\n",
       " (1,\n",
       "  '0.010*\"great\" + 0.010*\"nation\" + 0.009*\"good\" + 0.009*\"everi\" + 0.008*\"peopl\" + 0.007*\"citizen\" + 0.007*\"mean\" + 0.006*\"may\" + 0.006*\"man\" + 0.006*\"free\"'),\n",
       " (2,\n",
       "  '0.016*\"may\" + 0.013*\"constitut\" + 0.009*\"take\" + 0.008*\"state\" + 0.008*\"law\" + 0.006*\"great\" + 0.006*\"shall\" + 0.006*\"say\" + 0.006*\"union\" + 0.006*\"first\"'),\n",
       " (3,\n",
       "  '0.011*\"american\" + 0.010*\"right\" + 0.009*\"ani\" + 0.009*\"onli\" + 0.009*\"state\" + 0.008*\"must\" + 0.008*\"govern\" + 0.007*\"generat\" + 0.007*\"peopl\" + 0.006*\"old\"'),\n",
       " (4,\n",
       "  '0.014*\"peopl\" + 0.012*\"govern\" + 0.011*\"life\" + 0.009*\"countri\" + 0.008*\"strength\" + 0.007*\"ask\" + 0.007*\"free\" + 0.007*\"make\" + 0.006*\"good\" + 0.005*\"man\"'),\n",
       " (5,\n",
       "  '0.013*\"peopl\" + 0.011*\"work\" + 0.010*\"new\" + 0.009*\"nation\" + 0.008*\"make\" + 0.008*\"great\" + 0.007*\"american\" + 0.007*\"man\" + 0.007*\"freedom\" + 0.006*\"today\"'),\n",
       " (6,\n",
       "  '0.014*\"must\" + 0.013*\"let\" + 0.010*\"time\" + 0.010*\"know\" + 0.008*\"free\" + 0.008*\"good\" + 0.008*\"faith\" + 0.007*\"chang\" + 0.006*\"one\" + 0.006*\"hope\"'),\n",
       " (7,\n",
       "  '0.019*\"nation\" + 0.012*\"ani\" + 0.010*\"act\" + 0.009*\"world\" + 0.008*\"constitut\" + 0.007*\"must\" + 0.006*\"requir\" + 0.006*\"peopl\" + 0.006*\"common\" + 0.006*\"presid\"'),\n",
       " (8,\n",
       "  '0.013*\"world\" + 0.013*\"govern\" + 0.011*\"must\" + 0.010*\"man\" + 0.009*\"one\" + 0.008*\"shall\" + 0.007*\"peopl\" + 0.007*\"faith\" + 0.007*\"america\" + 0.007*\"power\"'),\n",
       " (9,\n",
       "  '0.012*\"world\" + 0.008*\"govern\" + 0.008*\"shall\" + 0.008*\"new\" + 0.008*\"becaus\" + 0.007*\"surrend\" + 0.006*\"slave\" + 0.006*\"war\" + 0.006*\"america\" + 0.005*\"man\"')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:15.937392Z",
     "start_time": "2019-08-11T12:37:15.648246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Refactoring results of LDA into numpy matrix.\n",
    "hm = np.array([[y for (x,y) in lda[corpus[i]]] for i in range(len(corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:23.867761Z",
     "start_time": "2019-08-11T12:37:15.939683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality using t-SNE.\n",
    "tsne = TSNE(random_state=15, perplexity=30, early_exaggeration=120)\n",
    "embedding = tsne.fit_transform(hm)\n",
    "embedding = pd.DataFrame(embedding, columns=['x','y'])\n",
    "embedding['hue'] = hm.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.423942Z",
     "start_time": "2019-08-11T12:37:23.869688Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected a value of type Real, got clinton of type str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-52ad4e5e9dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPresident\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     ['Washington', 'Jefferson', 'Lincoln', 'FDR', 'Eisenhower', 'Kennedy',\n\u001b[0;32m---> 53\u001b[0;31m      'Reagan', 'GHWBush', 'Clinton', 'Obama'], step=1, title=\"Inaugural Speeches\")\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mslider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjs_on_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/models/widgets/sliders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Slider 'start' and 'end' cannot be equal.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWidget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     title = String(default=\"\", help=\"\"\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mdefault_theme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/has_props.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **properties)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/has_props.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHasProps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"similar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/descriptors.py\u001b[0m in \u001b[0;36m__set__\u001b[0;34m(self, obj, value, setter)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.%s is a readonly property\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delete__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/descriptors.py\u001b[0m in \u001b[0;36m_internal_set\u001b[0;34m(self, obj, value, hint, setter)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         '''\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mprepare_value\u001b[0;34m(self, obj_or_cls, name, value)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mprepare_value\u001b[0;34m(self, obj_or_cls, name, value)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidation_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternatives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/bokeh/core/property/bases.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, value, detail)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0mnice_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_underlying_type\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             )\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected a value of type Real, got clinton of type str"
     ]
    }
   ],
   "source": [
    "# Scatter plot using Bokeh.\n",
    "source = ColumnDataSource(\n",
    "    data=dict(x=embedding.x,\n",
    "              y=embedding.y,\n",
    "              colors=[all_palettes['Set1'][8] for i in embedding.hue],\n",
    "              sentence=sent_df.sentence,\n",
    "              President=sent_df.President,\n",
    "              alpha=[0.9] * embedding.shape[0],\n",
    "              size=[7] * embedding.shape[0]\n",
    "              )\n",
    ")\n",
    "hover_tsne = HoverTool(names=[\"sent_df\"], tooltips=\"\"\"\n",
    "    <div style=\"margin: 10\">\n",
    "        <div style=\"margin: 0 auto; width:300px;\">\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Title:</span>\n",
    "            <span style=\"font-size: 12px\">@title</span>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">Year:</span>\n",
    "            <span style=\"font-size: 12px\">@year</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
    "plot_tsne = figure(plot_width=700, plot_height=700,\n",
    "                   tools=tools_tsne, title='Inaugural Addresses')\n",
    "plot_tsne.circle('x', 'y', size='size', fill_color='colors',alpha='alpha',\n",
    "                 line_alpha=0, line_width=0.01, source=source, name=\"sent_df\")\n",
    "\n",
    "callback = CustomJS(args=dict(source=source), code=\n",
    "    \"\"\"var data = source.data;\n",
    "    var f = cb_obj.value\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    colors = data['colors']\n",
    "    alpha = data['alpha']\n",
    "    title = data['title']\n",
    "    President = data['President']\n",
    "    size = data['size']\n",
    "    for (i = 0; i < x.length; i++) {\n",
    "        if (year[i] <= f) {\n",
    "            alpha[i] = 0.9\n",
    "            size[i] = 7\n",
    "        } else {\n",
    "            alpha[i] = 0.05\n",
    "            size[i] = 4\n",
    "        }\n",
    "    }\n",
    "    source.change.emit();\n",
    "    \"\"\")\n",
    "\n",
    "slider = Slider(\n",
    "    start=sent_df.President.min(), end=sent_df.sentence.max(),values=\n",
    "    ['Washington', 'Jefferson', 'Lincoln', 'FDR', 'Eisenhower', 'Kennedy',\n",
    "     'Reagan', 'GHWBush', 'Clinton', 'Obama'], step=1, title=\"Inaugural Speeches\")\n",
    "slider.js_on_change('value', callback)\n",
    "\n",
    "layout = column(plot_tsne)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:38:26.574094Z",
     "start_time": "2019-08-11T12:38:26.560125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score to beat: 0.15491452991452967\n"
     ]
    }
   ],
   "source": [
    "'''Create baseline score to beat. GHWBush had the most sentences, so guessing \n",
    "him for all sentences would give this percentage.\n",
    "'''\n",
    "\n",
    "print('Baseline score to beat:', sum(\n",
    "    (sent_df.President == 'ghwbush') / len(sent_df.President)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:38:30.749530Z",
     "start_time": "2019-08-11T12:38:30.744044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline helpers.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:38:56.052343Z",
     "start_time": "2019-08-11T12:38:56.047466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the models.\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=15)\n",
    "tree = DecisionTreeClassifier(random_state=15)\n",
    "forest = RandomForestClassifier(max_depth=10, random_state=15)\n",
    "boost = GradientBoostingClassifier(random_state=15)\n",
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:39:21.539837Z",
     "start_time": "2019-08-11T12:39:21.535965Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Set up _kwargs files for convenience.\n",
    "tfidf_kwargs = {'X_train': X_train_tfidf,'y_train': y_train,\n",
    "                'X_eval': X_eval_tfidf,'y_eval': y_eval}\n",
    "                #'X_holdout': X_holdout_tfidf, 'y_holdout': y_holdout}\n",
    "\n",
    "lsa_kwargs = {'X_train': X_train_lsa_scaled, 'y_train': y_train,\n",
    "              'X_eval': X_eval_lsa_scaled, 'y_eval': y_eval}\n",
    "              #'X_holdout': X_holdout_tfidf_scaled, 'y_holdout': y_holdout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:39:48.821183Z",
     "start_time": "2019-08-11T12:39:48.815676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tune parameter grids.\n",
    "log_reg_params = {'model__C': [1, 10, 100, 1000]}\n",
    "tree_params = {'model__criterion': ['gini']}\n",
    "forest_params = {'model__n_estimators': [100, 200, 300,400],\n",
    "                 'model__max_depth': [None, 5, 10]}\n",
    "boost_params = {'model__n_estimators': [100]}\n",
    "nb_params = {'model__alpha': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:40:15.916447Z",
     "start_time": "2019-08-11T12:40:15.907953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to fit and predict all working kernals.\n",
    "\n",
    "\n",
    "def fit_and_predict(model, params: Dict,\n",
    "                    X_train: pd.DataFrame,\n",
    "                    y_train: pd.DataFrame,\n",
    "                    X_eval: pd.DataFrame,\n",
    "                    y_eval: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Takes an instantiated sklearn model, training data (X_train, y_train), \n",
    "    and performs cross-validation and then prints the mean of the cross-\n",
    "    validation accuracies.\n",
    "    \"\"\"\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_eval) == len(y_eval)\n",
    "    # assert len(X_holdout) == len(y_holdout)\n",
    "    pipe = Pipeline(steps=[('model', model)])\n",
    "    clf = GridSearchCV(pipe, cv=skf, param_grid=params, n_jobs=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('The mean cross_val accuracy on train is',\n",
    "          f'{clf.cv_results_[\"mean_test_score\"]}.')\n",
    "    print('The std of the cross_val accuracy is',\n",
    "          f'{clf.cv_results_[\"std_test_score\"]}.')\n",
    "    y_pred = clf.predict(X_eval)\n",
    "    print(classification_report(y_eval, y_pred))\n",
    "    print(confusion_matrix(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:40:55.646090Z",
     "start_time": "2019-08-11T12:40:45.954610Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.34410646 0.34220532 0.31749049 0.28136882].\n",
      "The std of the cross_val accuracy is [0.0365393  0.01761933 0.0400676  0.05275828].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.33      0.12      0.18        16\n",
      "  eisenhower       0.35      0.42      0.38        19\n",
      "         fdr       0.25      0.06      0.10        16\n",
      "     ghwbush       0.15      0.44      0.22        25\n",
      "   jefferson       1.00      0.10      0.18        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.58      0.62      0.60        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.24      0.33      0.28        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.28      0.28      0.28       176\n",
      "   macro avg       0.29      0.21      0.19       176\n",
      "weighted avg       0.30      0.28      0.25       176\n",
      "\n",
      "[[ 2  1  0  9  0  1  0  0  3  0]\n",
      " [ 0  8  0  7  0  0  1  2  1  0]\n",
      " [ 0  1  1  8  0  0  1  0  5  0]\n",
      " [ 0  5  2 11  0  0  2  0  5  0]\n",
      " [ 0  2  0  2  1  0  2  0  3  0]\n",
      " [ 1  1  0  4  0  0  1  1  1  0]\n",
      " [ 0  1  0  7  0  0 18  0  3  0]\n",
      " [ 1  1  1 15  0  0  3  0  4  0]\n",
      " [ 2  3  0  8  0  0  2  1  8  0]\n",
      " [ 0  0  0  2  0  0  1  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (Latent Semantic Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:41:34.406942Z",
     "start_time": "2019-08-11T12:41:22.422196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.33460076 0.31178707 0.31178707 0.28136882].\n",
      "The std of the cross_val accuracy is [0.03091255 0.02803846 0.04177239 0.04674929].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.18      0.12      0.15        16\n",
      "  eisenhower       0.33      0.47      0.39        19\n",
      "         fdr       0.22      0.12      0.16        16\n",
      "     ghwbush       0.17      0.40      0.24        25\n",
      "   jefferson       0.33      0.10      0.15        10\n",
      "     kennedy       0.50      0.11      0.18         9\n",
      "     lincoln       0.50      0.55      0.52        29\n",
      "       obama       0.29      0.08      0.12        25\n",
      "      reagan       0.26      0.25      0.26        24\n",
      "  washington       0.50      0.33      0.40         3\n",
      "\n",
      "   micro avg       0.28      0.28      0.28       176\n",
      "   macro avg       0.33      0.25      0.26       176\n",
      "weighted avg       0.31      0.28      0.27       176\n",
      "\n",
      "[[ 2  1  1  9  0  1  0  0  2  0]\n",
      " [ 0  9  0  7  0  0  2  1  0  0]\n",
      " [ 0  4  2  5  0  0  0  1  4  0]\n",
      " [ 1  5  2 10  0  0  2  1  4  0]\n",
      " [ 0  1  0  1  1  0  5  0  2  0]\n",
      " [ 2  1  0  1  1  1  1  1  1  0]\n",
      " [ 0  1  0  8  1  0 16  0  2  1]\n",
      " [ 3  2  2 11  0  0  3  2  2  0]\n",
      " [ 3  3  2  7  0  0  2  1  6  0]\n",
      " [ 0  0  0  1  0  0  1  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(log_reg, params=log_reg_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:42:00.655540Z",
     "start_time": "2019-08-11T12:42:00.317528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.26996198].\n",
      "The std of the cross_val accuracy is [0.0617106].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.12      0.12      0.12        16\n",
      "  eisenhower       0.21      0.26      0.23        19\n",
      "         fdr       0.08      0.06      0.07        16\n",
      "     ghwbush       0.11      0.28      0.16        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.40      0.22      0.29         9\n",
      "     lincoln       0.55      0.41      0.47        29\n",
      "       obama       0.18      0.08      0.11        25\n",
      "      reagan       0.33      0.25      0.29        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.21      0.21      0.21       176\n",
      "   macro avg       0.20      0.17      0.17       176\n",
      "weighted avg       0.24      0.21      0.21       176\n",
      "\n",
      "[[ 2  1  0  9  0  0  1  1  2  0]\n",
      " [ 1  5  2  9  0  0  1  0  1  0]\n",
      " [ 1  3  1  7  1  0  0  1  2  0]\n",
      " [ 4  4  2  7  0  0  3  2  2  1]\n",
      " [ 0  0  0  2  0  1  3  2  2  0]\n",
      " [ 3  2  1  1  0  2  0  0  0  0]\n",
      " [ 0  3  2  8  0  1 12  0  2  1]\n",
      " [ 4  2  3 12  0  1  0  2  1  0]\n",
      " [ 1  3  1 10  0  0  0  3  6  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(tree, params=tree_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:42:28.647778Z",
     "start_time": "2019-08-11T12:42:28.115594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.2148289].\n",
      "The std of the cross_val accuracy is [0.03451126].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.09      0.06      0.07        16\n",
      "  eisenhower       0.14      0.21      0.17        19\n",
      "         fdr       0.07      0.06      0.06        16\n",
      "     ghwbush       0.11      0.20      0.14        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.44      0.28      0.34        29\n",
      "       obama       0.09      0.04      0.06        25\n",
      "      reagan       0.13      0.17      0.15        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.14      0.14      0.14       176\n",
      "   macro avg       0.11      0.10      0.10       176\n",
      "weighted avg       0.15      0.14      0.13       176\n",
      "\n",
      "[[1 2 1 6 0 1 0 1 4 0]\n",
      " [1 4 2 9 0 1 1 1 0 0]\n",
      " [1 3 1 3 1 1 0 1 2 3]\n",
      " [2 5 2 5 0 2 3 1 5 0]\n",
      " [1 2 1 0 0 1 2 0 2 1]\n",
      " [0 1 1 3 0 0 0 0 3 1]\n",
      " [0 5 3 4 0 0 8 2 5 2]\n",
      " [5 0 3 8 0 2 1 1 5 0]\n",
      " [0 6 1 6 0 0 2 4 4 1]\n",
      " [0 1 0 0 0 0 1 0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(tree, params=tree_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:43:16.330377Z",
     "start_time": "2019-08-11T12:42:55.796628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.30798479 0.31749049 0.32509506 0.32889734 0.28326996 0.29277567\n",
      " 0.28707224 0.28326996 0.28707224 0.29657795 0.30228137 0.30228137].\n",
      "The std of the cross_val accuracy is [0.0383545  0.03435724 0.03282714 0.03598003 0.05548412 0.05193235\n",
      " 0.04677472 0.04852622 0.04649525 0.04687379 0.04612755 0.04741648].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.27      0.25      0.26        16\n",
      "  eisenhower       0.21      0.32      0.25        19\n",
      "         fdr       0.12      0.06      0.08        16\n",
      "     ghwbush       0.15      0.32      0.21        25\n",
      "   jefferson       0.50      0.30      0.37        10\n",
      "     kennedy       1.00      0.11      0.20         9\n",
      "     lincoln       0.56      0.52      0.54        29\n",
      "       obama       0.07      0.04      0.05        25\n",
      "      reagan       0.27      0.25      0.26        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.26      0.26      0.26       176\n",
      "   macro avg       0.31      0.22      0.22       176\n",
      "weighted avg       0.30      0.26      0.25       176\n",
      "\n",
      "[[ 4  2  0  7  0  0  0  1  2  0]\n",
      " [ 0  6  1  6  1  0  1  3  1  0]\n",
      " [ 0  4  1  6  0  0  1  2  2  0]\n",
      " [ 3  6  3  8  0  0  3  1  1  0]\n",
      " [ 0  0  0  0  3  0  4  1  2  0]\n",
      " [ 1  1  0  2  1  1  1  0  2  0]\n",
      " [ 0  2  0  6  1  0 15  0  4  1]\n",
      " [ 5  3  3 11  0  0  0  1  2  0]\n",
      " [ 2  4  0  6  0  0  0  6  6  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(forest, params=forest_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:44:26.025440Z",
     "start_time": "2019-08-11T12:43:42.128643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.32129278 0.31178707 0.31558935 0.32319392 0.30798479 0.29847909\n",
      " 0.30988593 0.30798479 0.31178707 0.33840304 0.33079848 0.33460076].\n",
      "The std of the cross_val accuracy is [0.0440672  0.04180222 0.03493958 0.03524775 0.03959405 0.04954156\n",
      " 0.0501353  0.0576028  0.03656241 0.04614652 0.03676983 0.0385607 ].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.50      0.12      0.20        16\n",
      "  eisenhower       0.28      0.37      0.32        19\n",
      "         fdr       0.33      0.06      0.11        16\n",
      "     ghwbush       0.15      0.44      0.23        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.47      0.66      0.55        29\n",
      "       obama       0.17      0.04      0.06        25\n",
      "      reagan       0.19      0.21      0.20        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.26      0.26      0.26       176\n",
      "   macro avg       0.21      0.19      0.17       176\n",
      "weighted avg       0.26      0.26      0.22       176\n",
      "\n",
      "[[ 2  2  0  8  0  0  0  1  3  0]\n",
      " [ 0  7  0  8  0  0  2  0  2  0]\n",
      " [ 0  1  1  8  0  0  2  0  4  0]\n",
      " [ 1  3  1 11  0  0  6  0  3  0]\n",
      " [ 0  1  0  2  0  0  4  0  3  0]\n",
      " [ 0  4  0  3  0  0  1  1  0  0]\n",
      " [ 0  2  0  6  0  0 19  0  2  0]\n",
      " [ 1  1  1 16  0  0  2  1  3  0]\n",
      " [ 0  4  0 10  0  0  2  3  5  0]\n",
      " [ 0  0  0  0  0  0  2  0  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(forest, params=forest_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:45:02.002271Z",
     "start_time": "2019-08-11T12:44:52.799049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.28326996].\n",
      "The std of the cross_val accuracy is [0.03678817].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.33      0.25      0.29        16\n",
      "  eisenhower       0.19      0.26      0.22        19\n",
      "         fdr       0.25      0.12      0.17        16\n",
      "     ghwbush       0.14      0.36      0.20        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.46      0.38      0.42        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.33      0.33      0.33        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.22      0.22      0.22       176\n",
      "   macro avg       0.17      0.17      0.16       176\n",
      "weighted avg       0.21      0.22      0.21       176\n",
      "\n",
      "[[ 4  3  0  6  0  0  1  0  2  0]\n",
      " [ 0  5  1  7  0  1  1  3  1  0]\n",
      " [ 0  0  2  8  0  0  1  1  4  0]\n",
      " [ 1  6  2  9  0  1  2  2  2  0]\n",
      " [ 0  2  0  2  0  0  3  1  2  0]\n",
      " [ 1  2  0  2  1  0  1  1  1  0]\n",
      " [ 1  3  0 10  1  0 11  0  2  1]\n",
      " [ 4  2  3 12  0  0  2  0  2  0]\n",
      " [ 1  2  0  8  1  1  1  2  8  0]\n",
      " [ 0  1  0  1  0  0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(boost, params=boost_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:45:53.744216Z",
     "start_time": "2019-08-11T12:45:29.781505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.27376426].\n",
      "The std of the cross_val accuracy is [0.03294693].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.30      0.19      0.23        16\n",
      "  eisenhower       0.28      0.37      0.32        19\n",
      "         fdr       0.14      0.06      0.09        16\n",
      "     ghwbush       0.15      0.32      0.20        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.47      0.55      0.51        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.25      0.33      0.29        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.24      0.24      0.24       176\n",
      "   macro avg       0.16      0.18      0.16       176\n",
      "weighted avg       0.20      0.24      0.21       176\n",
      "\n",
      "[[ 3  1  1  7  0  1  0  1  2  0]\n",
      " [ 0  7  0  8  0  0  1  2  1  0]\n",
      " [ 0  2  1  7  0  0  4  1  1  0]\n",
      " [ 3  2  1  8  0  0  4  1  6  0]\n",
      " [ 0  1  1  3  0  0  2  0  3  0]\n",
      " [ 0  3  0  1  0  0  2  1  2  0]\n",
      " [ 0  4  0  4  1  0 16  1  3  0]\n",
      " [ 1  2  3 11  1  0  2  0  5  0]\n",
      " [ 3  3  0  6  0  0  1  3  8  0]\n",
      " [ 0  0  0  0  0  0  2  0  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(boost, params=boost_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:46:21.911762Z",
     "start_time": "2019-08-11T12:46:21.757248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.36311787].\n",
      "The std of the cross_val accuracy is [0.05182334].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.00      0.00      0.00        16\n",
      "  eisenhower       0.50      0.37      0.42        19\n",
      "         fdr       0.40      0.12      0.19        16\n",
      "     ghwbush       0.21      0.64      0.31        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.63      0.59      0.61        29\n",
      "       obama       0.00      0.00      0.00        25\n",
      "      reagan       0.22      0.42      0.29        24\n",
      "  washington       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       176\n",
      "   macro avg       0.30      0.25      0.23       176\n",
      "weighted avg       0.27      0.30      0.26       176\n",
      "\n",
      "[[ 0  1  0 11  0  0  0  0  4  0]\n",
      " [ 0  7  0  8  0  0  2  0  2  0]\n",
      " [ 0  1  2  6  0  0  0  0  7  0]\n",
      " [ 0  1  0 16  0  0  1  0  7  0]\n",
      " [ 0  1  0  2  0  1  2  1  3  0]\n",
      " [ 3  0  0  3  0  0  1  0  2  0]\n",
      " [ 0  0  1  8  0  0 17  1  2  0]\n",
      " [ 1  1  1 13  0  0  1  0  8  0]\n",
      " [ 0  2  1  9  0  0  2  0 10  0]\n",
      " [ 0  0  0  1  0  0  1  0  0  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(nb, params=nb_params, **tfidf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:46:49.854726Z",
     "start_time": "2019-08-11T12:46:49.706315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross_val accuracy on train is [0.16539924].\n",
      "The std of the cross_val accuracy is [0.01397377].\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     clinton       0.00      0.00      0.00        16\n",
      "  eisenhower       0.50      0.05      0.10        19\n",
      "         fdr       0.00      0.00      0.00        16\n",
      "     ghwbush       0.14      0.92      0.25        25\n",
      "   jefferson       0.00      0.00      0.00        10\n",
      "     kennedy       0.00      0.00      0.00         9\n",
      "     lincoln       0.00      0.00      0.00        29\n",
      "       obama       0.25      0.04      0.07        25\n",
      "      reagan       0.00      0.00      0.00        24\n",
      "  washington       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.14      0.14      0.14       176\n",
      "   macro avg       0.09      0.10      0.04       176\n",
      "weighted avg       0.11      0.14      0.06       176\n",
      "\n",
      "[[ 0  0  0 15  0  0  1  0  0  0]\n",
      " [ 0  1  0 18  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  0  0  0  0  0]\n",
      " [ 0  0  0 23  0  0  1  1  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  2  0]\n",
      " [ 0  0  0  8  0  0  1  0  0  0]\n",
      " [ 0  0  0 28  0  0  0  1  0  0]\n",
      " [ 0  0  0 22  0  0  1  1  1  0]\n",
      " [ 0  1  0 21  0  0  1  1  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fit_and_predict(nb, params=nb_params, **lsa_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:20:36.652787Z",
     "start_time": "2019-08-03T20:20:36.547143Z"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Note: there are not enough data to effectively run a neural network on this project. Section 5 is merely going through the process for the sake of the capstone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:47:20.563057Z",
     "start_time": "2019-08-11T12:47:17.471630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish and fit the multi-level perceptron model.\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(3,), random_state=15, max_iter=5000, alpha=0.05)\n",
    "mlp.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:47:47.246486Z",
     "start_time": "2019-08-11T12:47:47.237118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7718631178707225"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find MLP score.\n",
    "mlp.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:48:30.550782Z",
     "start_time": "2019-08-11T12:48:15.128821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22727273, 0.22222222, 0.24528302, 0.2745098 , 0.24      ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find cross-validation score.\n",
    "cross_val_score(mlp, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:49:03.451686Z",
     "start_time": "2019-08-11T12:48:57.199485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp1 = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,2,), random_state=15, max_iter=5000, alpha=0.01)\n",
    "mlp1.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:49:31.569896Z",
     "start_time": "2019-08-11T12:49:31.561207Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8897338403041825"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp1.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:50:19.486342Z",
     "start_time": "2019-08-11T12:49:58.352548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20909091, 0.19444444, 0.19811321, 0.25490196, 0.15      ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp1, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:50:52.800376Z",
     "start_time": "2019-08-11T12:50:47.201642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=15, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust hidden layer parameters.\n",
    "mlp2 = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,2,), random_state=15, max_iter=5000, alpha=0.05)\n",
    "mlp2.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:51:20.418953Z",
     "start_time": "2019-08-11T12:51:20.410860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8878326996197718"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score.\n",
    "mlp2.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:52:10.340702Z",
     "start_time": "2019-08-11T12:51:48.336097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22727273, 0.19444444, 0.19811321, 0.2745098 , 0.15      ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation.\n",
    "cross_val_score(mlp2, X_train_tfidf, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.810389Z",
     "start_time": "2019-08-11T12:36:56.463Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(log_reg, params=log_reg_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.812267Z",
     "start_time": "2019-08-11T12:36:56.475Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(tree, params=tree_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.820331Z",
     "start_time": "2019-08-11T12:36:56.485Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(forest, params=forest_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.822996Z",
     "start_time": "2019-08-11T12:36:56.495Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(boost, params=boost_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:37:24.825545Z",
     "start_time": "2019-08-11T12:36:56.505Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit_and_predict(nb, params=nb_params, **bow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 277,
   "position": {
    "height": "299px",
    "left": "893px",
    "right": "20px",
    "top": "111px",
    "width": "508px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
